<?xml version="1.0" encoding="utf-8"?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
	<channel>
		<title>Eric Winnington</title>
		<link>http://ewinnington.github.io/</link>
		<description>A collection of thoughts, code and snippets.</description>
		<copyright>2025</copyright>
		<pubDate>Mon, 13 Jan 2025 19:58:14 GMT</pubDate>
		<lastBuildDate>Mon, 13 Jan 2025 19:58:14 GMT</lastBuildDate>
		<item>
			<title>Network with Tailscale</title>
			<link>http://ewinnington.github.io/posts/network-tailscale</link>
			<description>&lt;p&gt;I updated my OpenVPN based network to use Tailscale instead in 2023 and it is game changing. I have used Tailscale ever since. I simply did not update my blog and network diagram.&lt;/p&gt;</description>
			<guid>http://ewinnington.github.io/posts/network-tailscale</guid>
			<pubDate>Mon, 13 Jan 2025 10:40:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;I updated my OpenVPN based network to use Tailscale instead in 2023 and it is game changing. I have used Tailscale ever since. I simply did not update my blog and network diagram.&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/network/network-update.png" class="img-fluid" width="100%" alt="Network" /&gt;&lt;/p&gt;
&lt;p&gt;With &lt;a href="https://tailscale.com/"&gt;Tailscale&lt;/a&gt;, all my machines appear seamlessly on a single control pane and I can reach any of them from any device.&lt;/p&gt;
&lt;h2 id="zone-z"&gt;Zone Z&lt;/h2&gt;
&lt;p&gt;Z has a single fiber connection via Swisscom to internet.&lt;/p&gt;
&lt;h3 id="inventory"&gt;Inventory&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;DeepThread is an AMD Threadripper 1920x running Windows 10.&lt;/li&gt;
&lt;li&gt;Minis4 is the Beelink MiniS12 N95s running Ubuntu Server 24.10.&lt;/li&gt;
&lt;li&gt;NAS is an an older QNAP TS-269L&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="zone-n"&gt;Zone N&lt;/h2&gt;
&lt;p&gt;N has two connections, a Starlink (v1 round) with only the powerbrick router and Sosh as a backup DSL provider (with an ADSL Router) both connected to a Ubiquity UDM-PRO-SE in Failover mode.&lt;/p&gt;
&lt;h3 id="inventory-1"&gt;Inventory&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Minis1 is the Beelink MiniS12 N95s running Windows 11, enjoying it VESA mounted behind a screen in the office currently. I originally thought I would also put Ubuntu, but a windows machine is useful.&lt;/li&gt;
&lt;li&gt;Minis2 and Minis3 are  the Beelink MiniS12 N95s running Ubuntu Server  24.10. Currently rackmounted with the UDM-PRO.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="vpn"&gt;VPN&lt;/h3&gt;
&lt;p&gt;On the UDM-PRO, a VPN is configured with Ubiquity and I can use the iOS application WifiMan to access the network. It's really a backup of a backup solution to have Wifiman.&lt;/p&gt;
&lt;p&gt;On Minis2 and minis4, a &lt;a href="https://github.com/cloudflare/cloudflared"&gt;cloudflared docker&lt;/a&gt; is running, reaching up to Cloudflare and providing an Zero trust tunnel to expose several dockerized websites hosted on it.&lt;/p&gt;
&lt;p&gt;I made a &lt;a href="https://suno.com/song/fb47c594-b22d-4504-83a1-75d8df705194"&gt;Suno song on how awesome&lt;/a&gt; it is.&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>Disposable software</title>
			<link>http://ewinnington.github.io/posts/Disposable-Software</link>
			<description>&lt;p&gt;With the advent of LLMs and their capability to create quick programs ("create me a flashcard app", "I need a typing exercise software", "make a dashboard to track my investments"), we might see a lot more software being written, used and then discarded since it's trivial for a LLM to re-write it next time it is needed.&lt;/p&gt;</description>
			<guid>http://ewinnington.github.io/posts/Disposable-Software</guid>
			<pubDate>Mon, 09 Dec 2024 00:00:00 GMT</pubDate>
			<content:encoded>&lt;h1 id="the-era-of-throw-away-software-is-upon-us"&gt;The era of throw away software is upon us&lt;/h1&gt;
&lt;p&gt;With the advent of LLMs and their capability to create quick programs (&amp;quot;create me a flashcard app&amp;quot;, &amp;quot;I need a typing exercise software&amp;quot;, &amp;quot;make a dashboard to track my investments&amp;quot;), we might see a lot more software being written, used and then discarded since it's trivial for a LLM to re-write it next time it is needed.&lt;/p&gt;
&lt;p&gt;Where perfection is not required, just good enough, there will be a whole slew of applications, websites and programs that are used and put into production that are never even reviewed by a human programmer, just tested for their outputs and/or visually checked by a human. Even, we might see lots of code that is only ever read by a machine for bugs and issues, to then be corrected by a machine.&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>Using an audit trail table on Oracle</title>
			<link>http://ewinnington.github.io/posts/Audit-Trail-Oracle</link>
			<description>&lt;p&gt;In modern applications, maintaining an audit trail of changes to data is crucial for compliance, debugging, and data integrity. This blog post explores a straightforward approach to implementing auditable updates in a relational database system, specifically focusing on a project management scenario with hierarchical data.&lt;/p&gt;</description>
			<guid>http://ewinnington.github.io/posts/Audit-Trail-Oracle</guid>
			<pubDate>Sat, 05 Oct 2024 08:00:00 GMT</pubDate>
			<content:encoded>&lt;h1 id="implementing-auditable-updates-in-a-relational-database"&gt;Implementing Auditable Updates in a Relational Database&lt;/h1&gt;
&lt;p&gt;In modern applications, maintaining an audit trail of changes to data is crucial for compliance, debugging, and data integrity. This blog post explores a straightforward approach to implementing auditable updates in a relational database system, specifically focusing on a project management scenario with hierarchical data.&lt;/p&gt;
&lt;h2 id="problem-description"&gt;Problem Description&lt;/h2&gt;
&lt;p&gt;We have a relational database containing &lt;code&gt;Projects&lt;/code&gt;, each of which includes &lt;code&gt;Instruments&lt;/code&gt;, &lt;code&gt;Markets&lt;/code&gt;, and &lt;code&gt;Valuations&lt;/code&gt;. These entities form a tree structure, adhering to the third normal form (3NF). Previously, any update to a project involved downloading the entire project tree, making changes, and uploading a new project under a new ID to ensure complete auditability.&lt;/p&gt;
&lt;p&gt;This approach is inefficient for small updates and doesn't allow for granular tracking of changes. The goal is to enable small, precise updates to projects while maintaining a comprehensive audit trail of all changes.&lt;/p&gt;
&lt;h2 id="solution-overview"&gt;Solution Overview&lt;/h2&gt;
&lt;p&gt;We introduce an audit table that records every change made to the database. The audit table will store serialized JSON representations of operations like &lt;code&gt;update&lt;/code&gt;, &lt;code&gt;insert&lt;/code&gt;, and &lt;code&gt;delete&lt;/code&gt;. We'll also provide C# code to apply and revert these changes, effectively creating an undo stack.&lt;/p&gt;
&lt;p&gt;Let's use the following DB Schema for illustration:&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/audit-trail/TableStructureBlog.png" class="img-fluid" width="60%" alt="TableSchema" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Primary Keys: Each table has a primary key (e.g., &lt;code&gt;ProjectID&lt;/code&gt;, &lt;code&gt;InstrumentID&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Foreign Keys: Child tables reference their parent via foreign keys (e.g., &lt;code&gt;instruments.ProjectID&lt;/code&gt; references &lt;code&gt;projects.ProjectID&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Audit Table: The &lt;code&gt;change_audit&lt;/code&gt; table records changes with fields like &lt;code&gt;ChangeAuditID&lt;/code&gt;, &lt;code&gt;TimeApplied&lt;/code&gt;, and &lt;code&gt;ImpactJson&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- 
```d2
projects: {
  shape: sql_table
  ProjectID: int {constraint: primary_key}
  Name: varchar(100)
  Description: text
  LastUpdated: timestamp with time zone
  VersionNumber: int
}

instruments: {
  shape: sql_table
  InstrumentID: int {constraint: primary_key}
  ProjectID: int {constraint: foreign_key}
  Name: varchar(100)
  Type: varchar(50)
  LastUpdated: timestamp with time zone
}

markets: {
  shape: sql_table
  MarketID: int {constraint: primary_key}
  ProjectID: int {constraint: foreign_key}
  Region: varchar(50)
  MarketType: varchar(50)
  LastUpdated: timestamp with time zone
}

valuations: {
  shape: sql_table
  ValuationID: int {constraint: primary_key}
  ProjectID: int {constraint: foreign_key}
  Value: decimal(10, 2)
  Currency: varchar(10)
  LastUpdated: timestamp with time zone
}

change_audit: {
  shape: sql_table
  ChangeAuditID: int {constraint: primary_key}
  TimeApplied: timestamp with time zone
  UserID: varchar(100)
  ImpactJson: jsonb
}

instruments.ProjectID -&gt; projects.ProjectID
markets.ProjectID -&gt; projects.ProjectID
valuations.ProjectID -&gt; projects.ProjectID

```
--&gt;
&lt;h2 id="implementing-change-auditing"&gt;Implementing Change Auditing&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;change_audit&lt;/code&gt; table is designed to store all changes in a JSON format for flexibility and ease of storage.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;CREATE TABLE change_audit (
  ChangeAuditID   NUMBER PRIMARY KEY,
  TimeApplied     TIMESTAMP,
  UserID          VARCHAR2(100),
  ImpactJson      CLOB
);
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="json-structure-for-changes"&gt;JSON Structure for Changes&lt;/h2&gt;
&lt;p&gt;Each change is recorded as a JSON object:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
  &amp;quot;Operation&amp;quot;: &amp;quot;update&amp;quot;,
  &amp;quot;impact&amp;quot;: [
    {
      &amp;quot;Table&amp;quot;: &amp;quot;Instruments&amp;quot;,
      &amp;quot;PrimaryKey&amp;quot;: {&amp;quot;ProjectID&amp;quot;: 4, &amp;quot;InstrumentID&amp;quot;: 2},
      &amp;quot;Column&amp;quot;: &amp;quot;Name&amp;quot;,
      &amp;quot;OldValue&amp;quot;: &amp;quot;Old Instrument Name&amp;quot;,
      &amp;quot;NewValue&amp;quot;: &amp;quot;Updated Instrument Name&amp;quot;
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="csharp-to-apply-changes-given-an-operation"&gt;CSharp to apply changes given an operation&lt;/h2&gt;
&lt;p&gt;To apply changes recorded in the JSON, we'll use C# code that parses the JSON and executes the corresponding SQL commands.&lt;/p&gt;
&lt;p&gt;I assume you have the &lt;code&gt;_connectionString&lt;/code&gt; available somewhere as a constant in the code.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;using Oracle.ManagedDataAccess.Client;
using Newtonsoft.Json.Linq;
using System;
using System.Collections.Generic;

public class ChangeApplier
{

    public void ApplyChanges(string jsonInput)
    {
        // Parse the JSON input
        var operation = JObject.Parse(jsonInput);
        string opType = operation[&amp;quot;Operation&amp;quot;].ToString();
        var impactList = (JArray)operation[&amp;quot;impact&amp;quot;];

        using (var conn = new OracleConnection(_connectionString))
        {
            conn.Open();
            using (var transaction = conn.BeginTransaction())
            {
                try
                {
                    foreach (var impact in impactList)
                    {
                        string table = impact[&amp;quot;Table&amp;quot;].ToString();
                        var primaryKey = (JObject)impact[&amp;quot;PrimaryKey&amp;quot;];
                        string column = impact[&amp;quot;Column&amp;quot;]?.ToString();
                        string newValue = impact[&amp;quot;NewValue&amp;quot;]?.ToString();

                        switch (opType)
                        {
                            case &amp;quot;update&amp;quot;:
                                ApplyUpdate(conn, table, primaryKey, column, newValue);
                                break;
                            case &amp;quot;insert&amp;quot;:
                                ApplyInsert(conn, table, impact);
                                break;
                            case &amp;quot;delete&amp;quot;:
                                ApplyDelete(conn, table, primaryKey);
                                break;
                        }
                    }

                    transaction.Commit();
                }
                catch (Exception ex)
                {
                    transaction.Rollback();
                    Console.WriteLine($&amp;quot;Error applying changes: {ex.Message}&amp;quot;);
                }
            }
        }
    }

    private void ApplyUpdate(OracleConnection conn, string table, JObject primaryKey, string column, string newValue)
    {
        var pkConditions = BuildPrimaryKeyCondition(primaryKey);
        var query = $&amp;quot;UPDATE {table} SET {column} = :newValue WHERE {pkConditions}&amp;quot;;

        using (var cmd = new OracleCommand(query, conn))
        {
            cmd.Parameters.Add(new OracleParameter(&amp;quot;newValue&amp;quot;, newValue));
            cmd.ExecuteNonQuery();
        }
    }

    private void ApplyInsert(OracleConnection conn, string table, JToken impact)
    {
        var primaryKey = (JObject)impact[&amp;quot;PrimaryKey&amp;quot;];
        var newValues = (JObject)impact[&amp;quot;NewValues&amp;quot;];
        var columns = new List&amp;lt;string&amp;gt;();
        var values = new List&amp;lt;string&amp;gt;();

        foreach (var property in primaryKey.Properties())
        {
            columns.Add(property.Name);
            values.Add($&amp;quot;:{property.Name}&amp;quot;);
        }

        foreach (var property in newValues.Properties())
        {
            columns.Add(property.Name);
            values.Add($&amp;quot;:{property.Name}&amp;quot;);
        }

        var query = $&amp;quot;INSERT INTO {table} ({string.Join(&amp;quot;, &amp;quot;, columns)}) VALUES ({string.Join(&amp;quot;, &amp;quot;, values)})&amp;quot;;

        using (var cmd = new OracleCommand(query, conn))
        {
            foreach (var property in primaryKey.Properties())
            {
                cmd.Parameters.Add(new OracleParameter(property.Name, property.Value.ToString()));
            }

            foreach (var property in newValues.Properties())
            {
                cmd.Parameters.Add(new OracleParameter(property.Name, property.Value.ToString()));
            }

            cmd.ExecuteNonQuery();
        }
    }

    private void ApplyDelete(OracleConnection conn, string table, JObject primaryKey)
    {
        var pkConditions = BuildPrimaryKeyCondition(primaryKey);
        var query = $&amp;quot;DELETE FROM {table} WHERE {pkConditions}&amp;quot;;

        using (var cmd = new OracleCommand(query, conn))
        {
            cmd.ExecuteNonQuery();
        }
    }

    private string BuildPrimaryKeyCondition(JObject primaryKey)
    {
        var conditions = new List&amp;lt;string&amp;gt;();
        foreach (var prop in primaryKey.Properties())
        {
            conditions.Add($&amp;quot;{prop.Name} = :{prop.Name}&amp;quot;);
        }
        return string.Join(&amp;quot; AND &amp;quot;, conditions);
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ApplyChanges&lt;/strong&gt;: Parses the JSON input and determines the operation type.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ApplyUpdate&lt;/strong&gt;: Executes an UPDATE SQL command using parameters to prevent SQL injection.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ApplyInsert&lt;/strong&gt;: Executes an INSERT SQL command, constructing columns and values from the JSON.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ApplyDelete&lt;/strong&gt;: Executes a DELETE SQL command based on the primary key.
BuildPrimaryKeyCondition: Constructs the WHERE clause for SQL commands.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A side note, for the insert, you'll have the challenge if you are using auto-incremented IDs, this will mean you don't know the new IDs until you have inserted the data, so you should make sure to capture the new IDs and then create the audit log. This is left as a simple exercise to the reader in case it is necessary.&lt;/p&gt;
&lt;h2 id="csharp-to-revert-changes"&gt;CSharp to revert changes&lt;/h2&gt;
&lt;p&gt;To revert changes (undo operations), we'll process the audit trail in reverse order. Here I give the processing of a list of operations as an example of unrolling. It is to note that the reverse delete does only one table, so if there was some connected information that was deleted via referential identity, it was the task of the audit table to keep that in the audit.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public class ChangeReverter
{
    public void RevertChanges(List&amp;lt;string&amp;gt; jsonOperations)
    {
        using (var conn = new OracleConnection(_connectionString))
        {
            conn.Open();
            using (var transaction = conn.BeginTransaction())
            {
                try
                {
                    jsonOperations.Reverse(); // note: you could also have provided sorted by last time from the audit table instead of reversing them

                    foreach (var operationJson in jsonOperations)
                    {
                        var operation = JObject.Parse(operationJson);
                        string opType = operation[&amp;quot;Operation&amp;quot;].ToString();
                        var impactList = (JArray)operation[&amp;quot;impact&amp;quot;];

                        foreach (var impact in impactList)
                        {
                            string table = impact[&amp;quot;Table&amp;quot;].ToString();
                            var primaryKey = (JObject)impact[&amp;quot;PrimaryKey&amp;quot;];
                            string column = impact[&amp;quot;Column&amp;quot;]?.ToString();
                            string oldValue = impact[&amp;quot;OldValue&amp;quot;]?.ToString();

                            switch (opType)
                            {
                                case &amp;quot;update&amp;quot;:
                                    RevertUpdate(conn, table, primaryKey, column, oldValue);
                                    break;
                                case &amp;quot;insert&amp;quot;:
                                    ApplyDelete(conn, table, primaryKey);
                                    break;
                                case &amp;quot;delete&amp;quot;:
                                    RevertDelete(conn, table, impact);
                                    break;
                            }
                        }
                    }

                    transaction.Commit();
                }
                catch (Exception ex)
                {
                    transaction.Rollback();
                    Console.WriteLine($&amp;quot;Error reverting changes: {ex.Message}&amp;quot;);
                }
            }
        }
    }

    private void RevertUpdate(OracleConnection conn, string table, JObject primaryKey, string column, string oldValue)
    {
        var pkConditions = BuildPrimaryKeyCondition(primaryKey);
        var query = $&amp;quot;UPDATE {table} SET {column} = :oldValue WHERE {pkConditions}&amp;quot;;

        using (var cmd = new OracleCommand(query, conn))
        {
            cmd.Parameters.Add(new OracleParameter(&amp;quot;oldValue&amp;quot;, oldValue));
            cmd.ExecuteNonQuery();
        }
    }

    private void RevertDelete(OracleConnection conn, string table, JToken impact)
    {
        var primaryKey = (JObject)impact[&amp;quot;PrimaryKey&amp;quot;];
        var oldValues = (JObject)impact[&amp;quot;OldValues&amp;quot;];
        var columns = new List&amp;lt;string&amp;gt;();
        var values = new List&amp;lt;string&amp;gt;();

        foreach (var property in primaryKey.Properties())
        {
            columns.Add(property.Name);
            values.Add($&amp;quot;:{property.Name}&amp;quot;);
        }

        foreach (var property in oldValues.Properties())
        {
            columns.Add(property.Name);
            values.Add($&amp;quot;:{property.Name}&amp;quot;);
        }

        var query = $&amp;quot;INSERT INTO {table} ({string.Join(&amp;quot;, &amp;quot;, columns)}) VALUES ({string.Join(&amp;quot;, &amp;quot;, values)})&amp;quot;;

        using (var cmd = new OracleCommand(query, conn))
        {
            foreach (var property in primaryKey.Properties())
            {
                cmd.Parameters.Add(new OracleParameter(property.Name, property.Value.ToString()));
            }

            foreach (var property in oldValues.Properties())
            {
                cmd.Parameters.Add(new OracleParameter(property.Name, property.Value.ToString()));
            }

            cmd.ExecuteNonQuery();
        }
    }

    // Reuse BuildPrimaryKeyCondition and ApplyDelete methods from ChangeApplier
}
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;RevertChanges&lt;/strong&gt;: Processes the list of JSON operations in reverse order to undo changes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RevertUpdate&lt;/strong&gt;: Sets the column back to its old value.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RevertDelete&lt;/strong&gt;: Re-inserts a deleted row using the old values stored in the audit trail.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ApplyDelete&lt;/strong&gt;: Deletes a row, used here to undo an insert operation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="json-schema"&gt;JSON schema&lt;/h2&gt;
&lt;p&gt;The reason that I prefer to use the Json directly in the C# code is that actually making up the C# classes for this schema is actually more work that processing the json directly in the code.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
  &amp;quot;$schema&amp;quot;: &amp;quot;http://json-schema.org/draft-07/schema#&amp;quot;,
  &amp;quot;title&amp;quot;: &amp;quot;ImpactJsonRoot&amp;quot;,
  &amp;quot;type&amp;quot;: &amp;quot;object&amp;quot;,
  &amp;quot;properties&amp;quot;: {
    &amp;quot;Operation&amp;quot;: {
      &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;,
      &amp;quot;enum&amp;quot;: [&amp;quot;update&amp;quot;, &amp;quot;insert&amp;quot;, &amp;quot;delete&amp;quot;],
      &amp;quot;description&amp;quot;: &amp;quot;Type of operation&amp;quot;
    },
    &amp;quot;Impact&amp;quot;: {
      &amp;quot;type&amp;quot;: &amp;quot;array&amp;quot;,
      &amp;quot;items&amp;quot;: {
        &amp;quot;type&amp;quot;: &amp;quot;object&amp;quot;,
        &amp;quot;properties&amp;quot;: {
          &amp;quot;Table&amp;quot;: {
            &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;,
            &amp;quot;description&amp;quot;: &amp;quot;Name of the table affected&amp;quot;
          },
          &amp;quot;PrimaryKey&amp;quot;: {
            &amp;quot;type&amp;quot;: &amp;quot;object&amp;quot;,
            &amp;quot;description&amp;quot;: &amp;quot;Primary key fields and their values&amp;quot;,
            &amp;quot;additionalProperties&amp;quot;: {
              &amp;quot;type&amp;quot;: [&amp;quot;number&amp;quot;, &amp;quot;null&amp;quot;]
            }
          },
          &amp;quot;Column&amp;quot;: {
            &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;,
            &amp;quot;description&amp;quot;: &amp;quot;Column affected (for updates)&amp;quot;
          },
          &amp;quot;OldValue&amp;quot;: {
            &amp;quot;type&amp;quot;: [&amp;quot;string&amp;quot;, &amp;quot;number&amp;quot;, &amp;quot;boolean&amp;quot;, &amp;quot;null&amp;quot;],
            &amp;quot;description&amp;quot;: &amp;quot;Previous value (for updates and deletes)&amp;quot;
          },
          &amp;quot;NewValue&amp;quot;: {
            &amp;quot;type&amp;quot;: [&amp;quot;string&amp;quot;, &amp;quot;number&amp;quot;, &amp;quot;boolean&amp;quot;, &amp;quot;null&amp;quot;],
            &amp;quot;description&amp;quot;: &amp;quot;New value (for updates and inserts)&amp;quot;
          },
          &amp;quot;OldValues&amp;quot;: {
            &amp;quot;type&amp;quot;: &amp;quot;object&amp;quot;,
            &amp;quot;description&amp;quot;: &amp;quot;All old values (for deletes)&amp;quot;,
            &amp;quot;additionalProperties&amp;quot;: {
              &amp;quot;type&amp;quot;: [&amp;quot;string&amp;quot;, &amp;quot;number&amp;quot;, &amp;quot;boolean&amp;quot;, &amp;quot;null&amp;quot;]
            }
          },
          &amp;quot;NewValues&amp;quot;: {
            &amp;quot;type&amp;quot;: &amp;quot;object&amp;quot;,
            &amp;quot;description&amp;quot;: &amp;quot;All new values (for inserts)&amp;quot;,
            &amp;quot;additionalProperties&amp;quot;: {
              &amp;quot;type&amp;quot;: [&amp;quot;string&amp;quot;, &amp;quot;number&amp;quot;, &amp;quot;boolean&amp;quot;, &amp;quot;null&amp;quot;]
            }
          }
        },
        &amp;quot;required&amp;quot;: [&amp;quot;Table&amp;quot;, &amp;quot;PrimaryKey&amp;quot;]
      }
    }
  },
  &amp;quot;required&amp;quot;: [&amp;quot;Operation&amp;quot;, &amp;quot;Impact&amp;quot;]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and here are examples of operations:&lt;/p&gt;
&lt;h3 id="update"&gt;update&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
  &amp;quot;Operation&amp;quot;: &amp;quot;update&amp;quot;,
  &amp;quot;Impact&amp;quot;: [
    {
      &amp;quot;Table&amp;quot;: &amp;quot;Instruments&amp;quot;,
      &amp;quot;PrimaryKey&amp;quot;: { &amp;quot;ProjectID&amp;quot;: 4, &amp;quot;InstrumentID&amp;quot;: 2 },
      &amp;quot;Column&amp;quot;: &amp;quot;Name&amp;quot;,
      &amp;quot;OldValue&amp;quot;: &amp;quot;Old Instrument Name&amp;quot;,
      &amp;quot;NewValue&amp;quot;: &amp;quot;Updated Instrument Name&amp;quot;
    }
  ]
}

&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="insert"&gt;insert&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
  &amp;quot;Operation&amp;quot;: &amp;quot;insert&amp;quot;,
  &amp;quot;Impact&amp;quot;: [
    {
      &amp;quot;Table&amp;quot;: &amp;quot;Instruments&amp;quot;,
      &amp;quot;PrimaryKey&amp;quot;: { &amp;quot;ProjectID&amp;quot;: 4, &amp;quot;InstrumentID&amp;quot;: 10 },
      &amp;quot;NewValues&amp;quot;: {
        &amp;quot;Name&amp;quot;: &amp;quot;New Instrument&amp;quot;,
        &amp;quot;Type&amp;quot;: &amp;quot;Flexible Asset&amp;quot;,
        &amp;quot;LastUpdated&amp;quot;: &amp;quot;2024-10-05T12:34:56Z&amp;quot;
      }
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="delete"&gt;delete&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
  &amp;quot;Operation&amp;quot;: &amp;quot;delete&amp;quot;,
  &amp;quot;Impact&amp;quot;: [
    {
      &amp;quot;Table&amp;quot;: &amp;quot;Instruments&amp;quot;,
      &amp;quot;PrimaryKey&amp;quot;: { &amp;quot;ProjectID&amp;quot;: 4, &amp;quot;InstrumentID&amp;quot;: 5 },
      &amp;quot;OldValues&amp;quot;: {
        &amp;quot;Name&amp;quot;: &amp;quot;Obsolete Instrument&amp;quot;,
        &amp;quot;Type&amp;quot;: &amp;quot;Flexible Asset&amp;quot;,
        &amp;quot;LastUpdated&amp;quot;: &amp;quot;2024-10-01T09:15:00Z&amp;quot;
      }
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note: OpenAI's &lt;code&gt;o1-preview&lt;/code&gt; was used to assist in the creation of the post.&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>VSCode Snippets</title>
			<link>http://ewinnington.github.io/posts/snippets-in-vscode</link>
			<description>&lt;p&gt;I use Visual Studio Code to write markdown notes of my meetings. I datetime every meeting to be able to find it afterwards. One of the shorthands I use is 'dt'. I always forget how to set it up when I reinstall my vscode, so here's the instructions from github copilot chat, which explained how to set it up again!&lt;/p&gt;</description>
			<guid>http://ewinnington.github.io/posts/snippets-in-vscode</guid>
			<pubDate>Thu, 18 Jul 2024 11:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;I use Visual Studio Code to write markdown notes of my meetings. I datetime every meeting to be able to find it afterwards. One of the shorthands I use is 'dt'. I always forget how to set it up when I reinstall my vscode, so here's the instructions from github copilot chat, which explained how to set it up again!&lt;/p&gt;
&lt;p&gt;So now I know I can ask Copilot how to do it, but I still want to document it, so here it is:&lt;/p&gt;
&lt;h3 id="github-copilot-instructions"&gt;Github Copilot Instructions&lt;/h3&gt;
&lt;p&gt;To recreate the shorthand command in Visual Studio Code for inserting the current date and time in a Markdown file using &lt;code&gt;ctrl-space&lt;/code&gt;, you can use a snippet. Here's how to set it up:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Open the Command Palette with &lt;code&gt;Ctrl+Shift+P&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Type &lt;code&gt;Configure User Snippets&lt;/code&gt; and select it.&lt;/li&gt;
&lt;li&gt;Choose &lt;code&gt;markdown.json&lt;/code&gt; for Markdown files.&lt;/li&gt;
&lt;li&gt;Insert the following snippet configuration:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;&amp;quot;Insert Current Date and Time&amp;quot;: {
    &amp;quot;prefix&amp;quot;: &amp;quot;dt&amp;quot;,
    &amp;quot;body&amp;quot;: [
        &amp;quot;### $CURRENT_YEAR.$CURRENT_MONTH.$CURRENT_DATE $CURRENT_HOUR:$CURRENT_MINUTE:$CURRENT_SECOND&amp;quot;
    ],
    &amp;quot;description&amp;quot;: &amp;quot;Inserts the current date and time&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start="5"&gt;
&lt;li&gt;Save the &lt;code&gt;markdown.json&lt;/code&gt; file.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now, when you type &lt;code&gt;dt&lt;/code&gt; in a Markdown file and press &lt;code&gt;Ctrl+Space&lt;/code&gt;, it should suggest your snippet. Selecting it will insert the current date and time formatted as you described.&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>Adding LaTeX Maths to Writebook</title>
			<link>http://ewinnington.github.io/posts/HowToUpdateWritebook</link>
			<description>&lt;p&gt;I've started using ONCE &lt;a href="https://once.com/writebook"&gt;Writebook&lt;/a&gt; to host some markdown documents, including my book on Optimization applied to energy products. But that book contains tons of Markdown LaTeX formatted mathematics, which Writebook does not support at this time.&lt;/p&gt;</description>
			<guid>http://ewinnington.github.io/posts/HowToUpdateWritebook</guid>
			<pubDate>Sat, 13 Jul 2024 15:00:00 GMT</pubDate>
			<content:encoded>&lt;h1 id="how-to-add-latex-maths-to-writebook"&gt;How to add LaTeX maths to Writebook&lt;/h1&gt;
&lt;p&gt;I've started using ONCE &lt;a href="https://once.com/writebook"&gt;Writebook&lt;/a&gt; to host some markdown documents, including my book on Optimization applied to energy products. But that book contains tons of Markdown LaTeX formatted mathematics, which Writebook does not support at this time.&lt;/p&gt;
&lt;p&gt;So I patched support for it into the docker image.&lt;/p&gt;
&lt;h2 id="copy-out-the-app-layout-file"&gt;Copy out the app layout file&lt;/h2&gt;
&lt;p&gt;Assuming your docker image is named writebook.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo docker cp writebook:/rails/app/views/layouts/application.html.erb .
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="modify-the-application-erb-to-add-support"&gt;Modify the application erb to add support&lt;/h2&gt;
&lt;p&gt;In the Head Section add&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; &amp;lt;script type=&amp;quot;text/javascript&amp;quot;&amp;gt;
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$']],
          displayMath: [['$$', '$$']],
          processEscapes: true
        }
      };
    &amp;lt;/script&amp;gt;
      
    &amp;lt;script src=&amp;quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thing is, it doesn't render the first time round, so I have to refresh the page to render, but it's not too much of an issue right now, I actually like to see the LaTeX maths code before I see the rendered version.&lt;/p&gt;
&lt;!--
Not working right now ! 

## How to add a render on end of page load to ensure it typesets first time

Above the ```&lt;/Body&gt;``` tag at the bottom of the page, add
```
    &lt;script type="text/javascript"&gt;
    document.addEventListener("DOMContentLoaded", function() {
      MathJax.typeset();
    });
  &lt;/script&gt;
```

--&gt;
&lt;h2 id="save-and-copy-the-file-back"&gt;Save and copy the file back&lt;/h2&gt;
&lt;p&gt;Then finally copy the file back into the docker image and restart the image&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo docker cp application.html.erb writebook:/rails/app/views/layouts/application.html.erb
sudo docker restart writebook
&lt;/code&gt;&lt;/pre&gt;
</content:encoded>
		</item>
		<item>
			<title>Receiving compressed data from an http(s) endpoint</title>
			<link>http://ewinnington.github.io/posts/HttpClientCompression</link>
			<description>&lt;p&gt;With the amount of data that we are passing around across services, it is often beneficial to use compression on the data to reduce the transmission time. Modern platforms and algorithms are now very efficient at compressing regular data, particularly if that data is text or json data.&amp;nbsp;&lt;/p&gt;</description>
			<guid>http://ewinnington.github.io/posts/HttpClientCompression</guid>
			<pubDate>Wed, 20 Mar 2024 11:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;With the amount of data that we are passing around across services, it is often beneficial to use compression on the data to reduce the transmission time. Modern platforms and algorithms are now very efficient at compressing regular data, particularly if that data is text or json data. &lt;/p&gt;
&lt;p&gt;If the developer of the endpoint has prepared their service for compression, the client must still indicate that they are ready to receive the compressed data. Luckily, most implementations of modern http clients in R, Python, JavaScript and Dotnet support compression / decompression and are seamless for the client. This means that you can set the compression headers on and simply benefit from compressed data being received. &lt;/p&gt;
&lt;p&gt;We can also check in the Content-Encoding header which compression was used. I've found that example.com is sending responses compressed with gzip.&lt;/p&gt;
&lt;h2 id="python"&gt;Python&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import requests

url = &amp;quot;http://example.com&amp;quot;  # Replace with the actual URL you want to request

# Specify the accepted encoding methods in the headers
headers = {
    'Accept-Encoding': 'gzip, br',
}

response = requests.get(url, headers=headers)
print(response.text)

# In case you want to see if it was compressed, you can check via the headers
#if 'Content-Encoding' in response.headers:
#    print(f&amp;quot;Response was compressed using: {response.headers['Content-Encoding']}&amp;quot;)
#else:
#    print(&amp;quot;Response was not compressed.&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="r"&gt;R&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-R"&gt;library(httr)

# The URL to which you're sending the request
url &amp;lt;- &amp;quot;http://example.com&amp;quot;

# Setting the Accept-Encoding header
response &amp;lt;- GET(url, add_headers(`Accept-Encoding` = 'gzip, br'))

# The content of the response will be automatically decompressed by httr, so you can access it directly.
content(response, &amp;quot;text&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="c"&gt;C#&lt;/h2&gt;
&lt;p&gt;In C#, for some ungodly strange reason, the standard HTTP endpoint doesn't decompress for you automatically unless you add a decompression handler - see handler &lt;a href="https://learn.microsoft.com/en-us/dotnet/api/system.net.http.httpclienthandler.automaticdecompression?view=net-8.0#system-net-http-httpclienthandler-automaticdecompression"&gt;HttpClientHandler.AutomaticDecompression&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-Csharp"&gt;using System;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Threading.Tasks;
using System.Text;

class Program
{
    static async Task Main(string[] args)
    {
        HttpClientHandler handler = new HttpClientHandler();
        handler.AutomaticDecompression = System.Net.DecompressionMethods.GZip; //Adding automatic Decompression means that the accept headers are added automatically

        using (var client = new HttpClient(handler))
        {
            string url = &amp;quot;http://example.com&amp;quot;;
            HttpResponseMessage response = await client.GetAsync(url);
            response.EnsureSuccessStatusCode();

            Console.WriteLine(await response.Content.ReadAsStringAsync());
		}
	}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="javascript"&gt;JavaScript&lt;/h2&gt;
&lt;p&gt;it is so easy that you don't even need to do anything else than setting the gzip: true for the support&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-JS"&gt;const request = require('request');
const requestOptions = {
  url: 'http://example.com',
  gzip: true, // This is all that is required
};
request(requestOptions, (error, response, body) =&amp;gt; {
  // Handle the response here
});
&lt;/code&gt;&lt;/pre&gt;
</content:encoded>
		</item>
		<item>
			<title>DevContainers - The future of developer environments</title>
			<link>http://ewinnington.github.io/posts/devcontainers</link>
			<description>&lt;p&gt;It's been years now that we've had Infrastructure as Code (IaC), Containers and Desired state Configuration (DsC) tools to do our deployments. But these have been mostly focused on the deployment side of things, with fewer tools on the developer side. On the dev machine, installing and maintaining the development tools and package dependencies has been in flux, both in windows where finally tools like Ninite, Chocolatey and Winget allow management of dev tools, and on the linux side, which was always quite well served with apt - but has also gained Snap, Flatpack and other package management tools. The thing is, sometimes you need more that one version of a particular tool, Python3.10 and Python3.11, Java9 and Java17, Dotnet 4.8 and Dotnet 6, to work on the various projects you have during the day. Sometimes, they work side by side very well and sometimes they don't. And when they don't, it can be a long process to figure out why and also very difficult to get help without resorting to having a clean image refresh and starting again to install your dependencies.&lt;/p&gt;</description>
			<guid>http://ewinnington.github.io/posts/devcontainers</guid>
			<pubDate>Mon, 24 Jul 2023 00:00:00 GMT</pubDate>
			<content:encoded>&lt;h2 id="history"&gt;History&lt;/h2&gt;
&lt;p&gt;It's been years now that we've had Infrastructure as Code (IaC), Containers and Desired state Configuration (DsC) tools to do our deployments. But these have been mostly focused on the deployment side of things, with fewer tools on the developer side. On the dev machine, installing and maintaining the development tools and package dependencies has been in flux, both in windows where finally tools like Ninite, Chocolatey and Winget allow management of dev tools, and on the linux side, which was always quite well served with apt - but has also gained Snap, Flatpack and other package management tools. The thing is, sometimes you need more that one version of a particular tool, Python3.10 and Python3.11, Java9 and Java17, Dotnet 4.8 and Dotnet 6, to work on the various projects you have during the day. Sometimes, they work side by side very well and sometimes they don't. And when they don't, it can be a long process to figure out why and also very difficult to get help without resorting to having a clean image refresh and starting again to install your dependencies.&lt;/p&gt;
&lt;p&gt;Since the end of the 2010s and the early 2020s, with the rise of web hosted IDEs, there has been a need to define ways to have a base image that contained the environment and tools needed to work. I remember running some in the mid 2010s - Nitrous.IO (2013-16) - that allowed you to use a base container and configure it to do remote development.&lt;/p&gt;
&lt;h2 id="devcontainers"&gt;DevContainers&lt;/h2&gt;
&lt;p&gt;With the arrival of Docker on every desktop, Github's Cloudspaces and Visual Studio Code, there's been a new interest in this type of desired state environments with developer tooling. Microsoft published the &lt;a href="https://containers.dev/"&gt;DevContainer specification&lt;/a&gt; in early 2022 to formalize the language.&lt;/p&gt;
&lt;p&gt;So how does it help us? Well, with a DevContainer, we can setup a new development environment on Premise (in VSCode), on the cloud VM (Azure+VM) or on a Codespace environment with a single file that ensures that we always have the tools we want and need installed. Starting to work is as easy as openining the connection and cloning the repo we need if the .devcontainer file is located inside.&lt;/p&gt;
&lt;h2 id="devcontainer-example"&gt;DevContainer example&lt;/h2&gt;
&lt;p&gt;You can find below my &lt;a href="https://github.com/ewinnington/DevContainerTemplate/blob/master/.devcontainer/devcontainer.json"&gt;personal DevContainer&lt;/a&gt;, it is setup with Git, Node, AzureCLI, Docker control of hose, Dotnet, Terraform, Java with Maven, Python3 and Postgresql. I also have the VSCode extensions directly configured so I can directly start using them when I connect. I also use the &amp;quot;postStartCommand&amp;quot;: &amp;quot;nohup bash -c 'postgres &amp;amp;'&amp;quot; to run an instance of Postgresql directly inside the development container, so I can a directly have a DB to run requests against. And yes, this is a bit of a kitchen sink DevContainer, they can be smaller and more tailored to a project with only one or two of these features included, but here I use a generic one add added everything I use apart from the c++ and fortran compilers.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{
    &amp;quot;name&amp;quot;: &amp;quot;Erics-base-dev-container&amp;quot;,
    &amp;quot;image&amp;quot;: &amp;quot;mcr.microsoft.com/devcontainers/base:debian&amp;quot;,
 
    &amp;quot;features&amp;quot;: {
        &amp;quot;ghcr.io/devcontainers/features/git:1&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/node:1&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/azure-cli:1&amp;quot;: {}, //azure-cli,
        &amp;quot;ghcr.io/devcontainers/features/docker-outside-of-docker:1&amp;quot;: {}, //docker on host
        &amp;quot;ghcr.io/devcontainers/features/dotnet:1&amp;quot;: {}, //dotnet installed
        &amp;quot;ghcr.io/devcontainers/features/terraform:1&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/java:1&amp;quot;: { &amp;quot;installMaven&amp;quot; : true },
        &amp;quot;ghcr.io/devcontainers-contrib/features/postgres-asdf:1&amp;quot;: {}
    },
 
    // Configure tool-specific properties.
    &amp;quot;customizations&amp;quot;: {
        // Configure properties specific to VS Code.
        &amp;quot;vscode&amp;quot;: {
            &amp;quot;settings&amp;quot;: {},
            &amp;quot;extensions&amp;quot;: [
                &amp;quot;streetsidesoftware.code-spell-checker&amp;quot;,
                &amp;quot;ms-azuretools.vscode-docker&amp;quot;,
                &amp;quot;ms-dotnettools.csharp&amp;quot;,
                &amp;quot;HashiCorp.terraform&amp;quot;,
                &amp;quot;ms-azuretools.vscode-azureterraform&amp;quot;,
                &amp;quot;GitHub.copilot&amp;quot;,
                &amp;quot;GitHub.copilot-chat&amp;quot;,
                &amp;quot;vscjava.vscode-java-pack&amp;quot;,
                &amp;quot;ms-python.python&amp;quot;
            ]
        }
    },
 
    // Use 'forwardPorts' to make a list of ports inside the container available locally.
    // &amp;quot;forwardPorts&amp;quot;: [3000],
 
    // Use 'portsAttributes' to set default properties for specific forwarded ports.
    // More info: https://containers.dev/implementors/json_reference/#port-attributes
    &amp;quot;portsAttributes&amp;quot;: {
        &amp;quot;3000&amp;quot;: {
            &amp;quot;label&amp;quot;: &amp;quot;Hello Remote World&amp;quot;,
            &amp;quot;onAutoForward&amp;quot;: &amp;quot;notify&amp;quot;
        }
    },
 
    // Use 'postCreateCommand' to run commands after the container is created.
    &amp;quot;postCreateCommand&amp;quot;: &amp;quot;&amp;quot;,
 
    &amp;quot;postStartCommand&amp;quot;: &amp;quot;nohup bash -c 'postgres &amp;amp;'&amp;quot;
 
    // Uncomment to connect as root instead. More info: https://aka.ms/dev-containers-non-root.
    // &amp;quot;remoteUser&amp;quot;: &amp;quot;root&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="so-how-do-you-start-with-devcontainers"&gt;So how do you start with DevContainers?&lt;/h2&gt;
&lt;p&gt;There are 2 easy ways:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;(remote) Github Codespaces
By going to my repo, you can click &amp;quot;Create Codespace on Master&amp;quot; and get a running VSCode in the cloud with all those tools setup instantly.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;(at first build, the image might take time)&lt;/p&gt;
&lt;ol start="2"&gt;
&lt;li&gt;(local) Docker + VS Code
Ensure you have the &lt;a href="https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers"&gt;ms-vscode-remote.remote-containers&lt;/a&gt; extension installed in VS Code and Docker installed.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Clone the repo &lt;a href="https://github.com/ewinnington/DevContainerTemplate.git"&gt;https://github.com/ewinnington/DevContainerTemplate.git&lt;/a&gt;, then open it with VSCode. It should automatically detect the .devContainer and offer to rebuild the container image and open it up in the IDE for you.&lt;/p&gt;
&lt;p&gt;Once that is done, you should have access to a complete environment at the state you specified.&lt;/p&gt;
&lt;h2 id="whats-the-use-for-developers-at-corporations-where-computers-are-locked-down"&gt;What's the use for Developers at corporations where computers are locked down?&lt;/h2&gt;
&lt;p&gt;I think that providing developer windows machine with Git, Docker, WSL2 installed and using VS Code or another IDE that supports DevContainers is an excellent way forwards in providing a good fast and stable environment for developers to work faster and more efficiently. Using this configuration, any person showing up to a Hackathon would be able to start working in minutes after cloning a repository. It would really simplify daily operations, since every repo can provide the correct .DevContainer configuration, or teams can share a DevContainer basic configuration.&lt;/p&gt;
&lt;p&gt;This all simplifies operations, makes developer experience more consistent and increases productivity since you can move faster from one development environment to another in minutes. OnPrem → Remote VM → Cloudspace and back in minutes, without any friction.&lt;/p&gt;
&lt;p&gt;All in all, I'm convinced it is a tool that both IT support must understand and master how to best provide access to, and for developers to understand the devContainer to benefit from it.&lt;/p&gt;
&lt;p&gt;Have you used DevContainers? What is your experience?&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>Proxmox 8 on sub $200 mini PCs</title>
			<link>http://ewinnington.github.io/posts/Proxmox</link>
			<description>&lt;p&gt;This is a Beelink MiniS12 with an Intel N95s.&lt;/p&gt;</description>
			<guid>http://ewinnington.github.io/posts/Proxmox</guid>
			<pubDate>Sat, 01 Jul 2023 22:00:00 GMT</pubDate>
			<content:encoded>&lt;h1 id="installing-proxmox-tailscale-win11-vms-and-automation"&gt;Installing Proxmox, Tailscale, Win11 VMs and Automation&lt;/h1&gt;
&lt;p&gt;This is a Beelink MiniS12 with an Intel N95s.&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/minipc.jpg" class="img-fluid" width="80%" alt="" /&gt;
(coffee cup for scale)&lt;/p&gt;
&lt;p&gt;Up until Proxmox 8 dropped about a week ago, I was unable to install Proxmox due to compatibility with the graphics driver. Now in 8, that was fixed, so I've been able to install Proxmox on several of my machines.&lt;/p&gt;
&lt;p&gt;The procedure is trivial: write the proxmox iso to a usb key via a software that will make the iso bootable. Boot the machine with the usb key inserted and select the correct boot drive, then follow the proxmox installation prompts.&lt;/p&gt;
&lt;p&gt;It all worked out of the box.&lt;/p&gt;
&lt;h2 id="clustering"&gt;Clustering&lt;/h2&gt;
&lt;p&gt;I was able to connect to my proxmox installed machine on the port :8006 via a browser on my home network. Next step was enabling the management of multiple machines via a single UI. So as soon as I had two machines with Proxmox installer, I was able to go to my primary, click on Create cluster, confirm. Get the Join token, connect to the socond machine and paste the join token into the &amp;quot;Join Cluster&amp;quot;. Worked out of the box.&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/proxmox/proxmox-clustering.png" class="img-fluid" width="80%" alt="" /&gt;&lt;/p&gt;
&lt;h2 id="removing-the-update-repositories-to-work-on-the-free-version-of-proxmox"&gt;Removing the Update Repositories to work on the Free version of Proxmox&lt;/h2&gt;
&lt;p&gt;To stay within the Free licensing of Proxmox and be able to do apt-get, remember to go for each machine and to remove the non-free repos in the repository list.&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/proxmox/proxmox-remove-repos" class="img-fluid" width="80%" alt="" /&gt;&lt;/p&gt;
&lt;h2 id="installing-tailscale"&gt;Installing Tailscale&lt;/h2&gt;
&lt;p&gt;I use &lt;a href="https://tailscale.com/"&gt;Tailscale&lt;/a&gt; at home to connect across multiple locations and roaming devices. Every time I add a Tailscale device, I am amazed at how easy it is.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -fsSL https://tailscale.com/install.sh | sh
tailscale up
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Two lines, one URL to visit and the machines were enrolled.&lt;/p&gt;
&lt;h2 id="vms-and-lxcs"&gt;VMs and LXCs&lt;/h2&gt;
&lt;p&gt;To create VMs and LXCs, you need to add iso images or Templates to your local storage:&lt;/p&gt;
&lt;p&gt;Container templates, you can create your own and upload them or simply click on &amp;quot;Templates&amp;quot; and get a couple of ready made ones for use in your containers.&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/proxmox/container-templates.png" class="img-fluid" width="80%" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;You can now get the official windows ISO from the microsoft website. So download it to your local machine and then Upload it to the ISO images.&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/proxmox/windows-iso.png" class="img-fluid" width="80%" alt="" /&gt;&lt;/p&gt;
&lt;h3 id="installing-windows-11-pro-and-activating-with-the-hardware-license"&gt;Installing Windows 11 Pro and activating with the Hardware license&lt;/h3&gt;
&lt;p&gt;I had previously logged in and linked to my microsoft account on the windows 11 pro licensed version of the OS for each of the machines. This meant that when installing the Windows11 Pro from the ISO onto a VM running on the machines, I was able to activate the machine by referring to the previous activation. #Windows11Pro allows itself to be reactivated with the license that came with the hardware, inside a proxmox8 VM of the same machine as long as you pass the host-cpu - or so it seems to me.&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/proxmox/activated.jpg" class="img-fluid" width="80%" alt="Proxmox-shell-mode-issue" /&gt;&lt;/p&gt;
&lt;h2 id="issues-i-had-and-solutions"&gt;Issues I had and solutions&lt;/h2&gt;
&lt;h3 id="console-not-connecting-to-lxc-containers"&gt;Console not connecting to LXC containers&lt;/h3&gt;
&lt;p&gt;Several times, either while connecting to a container in Proxmox directly, or after a containter migation, I was not able to use the integrated shell. I therefore had to change the Container &amp;quot;Options-&amp;gt;Console mode&amp;quot; to &amp;quot;shell&amp;quot; to make it connect every time.
&lt;img src="/posts/images/proxmox/Proxmox-Shell-mode.png" class="img-fluid" width="80%" alt="Proxmox-shell-mode-issue" /&gt;&lt;/p&gt;
&lt;h3 id="apt-get-issue-in-proxmox-containers"&gt;apt-get issue in proxmox containers&lt;/h3&gt;
&lt;p&gt;The first thing I do upon entering a container is nearly always apt-get update. And sometimes it breaks. I couldn't update or install. Here is my checklist:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Check you gave and ip address to the container in the &amp;quot;Network&amp;quot; section, either a dhcp or a static address.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Check your DNS servers: I had not noticed that after installing Tailscale, my DNS records were only pointing to the talescale DNS resolver. Adding back google (8.8.8.8) and cloudflare (1.1.1.1) to my proxmox hosts helped.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src="/posts/images/proxmox/Tailscale-dns-issue-proxmox.png" class="img-fluid" width="80%" alt="Tailscale-dns-issue-proxmox" /&gt;&lt;/p&gt;
&lt;p&gt;By fixing both of these I was able to get the apt-get running correctly.&lt;/p&gt;
&lt;h2 id="automation"&gt;Automation&lt;/h2&gt;
&lt;h3 id="installing-on-a-client-machine-the-proxmox-cli-tools"&gt;Installing on a client machine the Proxmox CLI Tools&lt;/h3&gt;
&lt;p&gt;I'm planning on checking out automation of deployment on proxmox. Making a note here of the command line installation of the tools&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo pip3 install pve-cli
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I'll also look into Terraform + Ansible for a proxmox deployment, or the Packer LXC to make container templates, but that is for next time.&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>Network planning and VPN</title>
			<link>http://ewinnington.github.io/posts/network-vpn</link>
			<description>&lt;p&gt;I am in the process of setting up my homelab network between my two locations.&lt;/p&gt;</description>
			<guid>http://ewinnington.github.io/posts/network-vpn</guid>
			<pubDate>Thu, 20 Apr 2023 18:40:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;I am in the process of setting up my homelab network between my two locations.&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/network/network.png" class="img-fluid" width="100%" alt="Network" /&gt;&lt;/p&gt;
&lt;h2 id="zone-z"&gt;Zone Z&lt;/h2&gt;
&lt;p&gt;Z has a single fiber connection via Swisscom to internet.&lt;/p&gt;
&lt;h3 id="inventory"&gt;Inventory&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;DeepThread is an AMD Threadripper 1920x running Windows 10.&lt;/li&gt;
&lt;li&gt;Minis3 and Minis4 are the Beelink MiniS12 N95s running Ubuntu Server 23.04.&lt;/li&gt;
&lt;li&gt;NAS is an an older QNAP TS-269L&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="vpn"&gt;VPN&lt;/h3&gt;
&lt;p&gt;With a L2TP VPN connection configured to allow remote access onto the network, so I can get Red (Surface Laptop) and Xr (iPhone) onto the network in case.&lt;/p&gt;
&lt;h2 id="zone-n"&gt;Zone N&lt;/h2&gt;
&lt;p&gt;N has two connections, a Starlink (v1 round) with only the powerbrick router and Sosh as a backup DSL provider (with an ADSL Router) both connected to a Ubiquity UDM-PRO-SE in Failover mode. Getting a VPN to N is a little more involved, since the UDM is behind a separate router on each WAN.&lt;/p&gt;
&lt;h3 id="inventory-1"&gt;Inventory&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Minis1 is the Beelink MiniS12 N95s running Windows 11. Planned to Switch to Ubuntu 23.04, but enjoying it VESA mounted behind a screen in the office currently.&lt;/li&gt;
&lt;li&gt;Minis2 is the Beelink MiniS12 N95s running Ubuntu Server 23.04. Currently rackmounted with the UDM-PRO.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="vpn-1"&gt;VPN&lt;/h3&gt;
&lt;p&gt;On the UDM-PRO, a VPN is configured with Ubiquity and I can use the iOS application WifiMan to access the network.
On Minis2, a &lt;a href="https://github.com/cloudflare/cloudflared"&gt;cloudflared docker&lt;/a&gt; is running, reaching up to Cloudflare and providing an Zero trust tunnel to expose several dockerized websites hosted on it.&lt;/p&gt;
&lt;h1 id="the-issue-at-hand"&gt;The issue at hand&lt;/h1&gt;
&lt;p&gt;I would like the N Minis1 &amp;amp; Minis2 to be able to access the Z NAS, ideally with a relatively simple connection that I can leave running all the time, to be able to pull files from the NAS and ideally also access the NAS's front-end application from inside my N location. I could connect to the SwisscomVPN every time I do something that requires connectivity to the NAS, but I would really ideally like a more permanent solution where I make the Z NAS &amp;quot;visible&amp;quot; in the N network. Or go full and establish a site-to-site VPN and simply make the two areas N and Z communicate seamlessly while still having local connectivity.&lt;/p&gt;
&lt;p&gt;Do you have any suggestions as to how best to accomplish this?&lt;/p&gt;
&lt;h2 id="section"&gt;22.04.2023&lt;/h2&gt;
&lt;p&gt;I now put a OpenVPN on the Qnap NAS to act as a S2S VPN. Not sure that will be the solution I keep for the long term but it works for now.&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/network/network2.drawio.svg" class="img-fluid" width="100%" alt="Network" /&gt;&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>The era of the sub $200 PC</title>
			<link>http://ewinnington.github.io/posts/mini-pc</link>
			<description>&lt;p&gt;I recently purchased (arrived yesterday!) two mini PCs, for 170.- CHF each (~$187). They sport the latest low power CPU from Intel, the N95, which is amazingly efficient, powerful and cheap.&lt;/p&gt;</description>
			<guid>http://ewinnington.github.io/posts/mini-pc</guid>
			<pubDate>Wed, 05 Apr 2023 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;I recently purchased (arrived yesterday!) two mini PCs, for 170.- CHF each (~$187). They sport the latest low power CPU from Intel, the N95, which is amazingly efficient, powerful and cheap.&lt;/p&gt;
&lt;p&gt;The Beelink Mini S12 with the N95 intel CPU 4c, 16 GB Ram, 512 GB NVME storage drive, with space for an additional 2.5&amp;quot; SSD inside too. Amazingly small and light to carry around.&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/minipc.jpg" class="img-fluid" width="80%" alt="" /&gt;
(coffee cup for scale)&lt;/p&gt;
&lt;p&gt;I just got two of these to put in a Kubernetes cluster, and I've been playing around with them and they are super impressive. They pack a punch and sip power at 9-20W. The internal video card does 4k youtube playback without any issue.&lt;/p&gt;
&lt;p&gt;Windows 11 comes included with the machine for the price. I've already put ubuntu server on one of them. These machines are powerful enough for family members to do all their online activities, zoom (with a webcam), or use as a media machine.&lt;/p&gt;
&lt;p&gt;Over the next weeks, I'll be documenting configuring them and setting them up to serve as highly available servers from my home, using a reverse VPN tunneling.&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>Learning concepts from chatGPT - Operational Transform and Conflict-free Replicated Data Types</title>
			<link>http://ewinnington.github.io/posts/learn-from-chatgpt-Crdt-OT</link>
			<description>&lt;p&gt;I have been enjoying using &lt;a href="https://chat.openai.com/chat"&gt;chatGPT&lt;/a&gt; as a private tutor / reference on topics. My biggest issue is now keeping up with the knowledge and asking the right questions.&lt;/p&gt;</description>
			<guid>http://ewinnington.github.io/posts/learn-from-chatgpt-Crdt-OT</guid>
			<pubDate>Sun, 11 Dec 2022 00:20:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;I have been enjoying using &lt;a href="https://chat.openai.com/chat"&gt;chatGPT&lt;/a&gt; as a private tutor / reference on topics. My biggest issue is now keeping up with the knowledge and asking the right questions.&lt;/p&gt;
&lt;p&gt;But, remember that chatGPT is not all knowing and can get things wrong - even fabulating, so you have to double check the information. I did my best to validate the information with &lt;a href="https://mwhittaker.github.io/consistency_in_distributed_systems/3_crdt.html"&gt;Conflict-Free Replicated Data Types&lt;/a&gt;, &lt;a href="https://svn.apache.org/repos/asf/incubator/wave/whitepapers/operational-transform/operational-transform.html"&gt;Google's OT papers&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Operational_transformation"&gt;Wikipedia on Operational Transformation&lt;/a&gt;. I did not find any glaring errors in the text. For future crawlers, &lt;strong&gt;the following text was generated by chatGPT on 2022-12-11T01:12:45+0000&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;bold&lt;/strong&gt; is used for the prompts I send into chatGPT.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;italics&lt;/em&gt; is for my comments.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;can you explain CRDT's in data structures?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/gpt/gpt-logo.png" class="img-fluid" width="5%" alt="gpt-logo" /&gt;
Sure! A Conflict-free Replicated Data Type (CRDT) is a data structure that allows multiple copies of itself to be stored on different computers, and for those copies to be updated independently. This means that even when the copies are not connected to each other, they can still be updated and changed, and when they are eventually reconnected, they will automatically merge any conflicting changes in a way that preserves the consistency of the data. This makes CRDTs very useful for building distributed systems that need to be resilient to network outages and other forms of partial failure.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Is this similar to Google Wave's Operational Transform?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/gpt/gpt-logo.png" class="img-fluid" width="5%" alt="gpt-logo" /&gt;
Yes, Operational Transform (OT) is a technique that is related to CRDTs, and it is often used to solve the same kinds of problems. Like CRDTs, OT is a way of allowing multiple copies of a data structure to be updated independently and then merged together, but it uses a different approach to achieve this. OT works by applying a series of &amp;quot;operations&amp;quot; to the data, rather than directly modifying the data itself. This allows OT to detect and resolve conflicts between different copies of the data more easily, but it can also make it more complex to implement.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Can you give me an example of a CRDT design?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/gpt/gpt-logo.png" class="img-fluid" width="5%" alt="gpt-logo" /&gt;
Sure! One example of a CRDT is a &amp;quot;g-counter,&amp;quot; which is a data structure that stores a count. This data structure allows for the count to be incremented independently on different computers, and then merged together in a way that ensures that the final count is always correct. Here is one way that a g-counter could be implemented:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;class GCounter {
  Map&amp;lt;String, Integer&amp;gt; counts;

  void increment(String nodeId) {
    counts.put(nodeId, counts.getOrDefault(nodeId, 0) + 1);
  }

  void merge(GCounter other) {
    for (Map.Entry&amp;lt;String, Integer&amp;gt; entry : other.counts.entrySet()) {
      String nodeId = entry.getKey();
      int count = entry.getValue();
      counts.put(nodeId, Math.max(counts.getOrDefault(nodeId, 0), count));
    }
  }

  int getCount() {
    return counts.values().stream().mapToInt(x -&amp;gt; x).sum();
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;I never specified a programming language, so assume this is a kind of pseudocode&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In this implementation, the increment() method is used to increment the count on a particular node, and the merge() method is used to merge the counts from two different g-counters together. The getCount() method returns the current value of the count.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;And contrast with the design of an OT?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/gpt/gpt-logo.png" class="img-fluid" width="5%" alt="gpt-logo" /&gt;
Here is an example of how an Operational Transform (OT) might be implemented for a simple text editing application:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;class TextBuffer {
  StringBuilder text;

  void applyOperation(Operation op) {
    int start = op.getStartIndex();
    int end = op.getEndIndex();
    String newText = op.getText();
    text.replace(start, end, newText);
  }

  String getText() {
    return text.toString();
  }
}

class Operation {
  int startIndex;
  int endIndex;
  String text;

  int getStartIndex() {
    return startIndex;
  }

  int getEndIndex() {
    return endIndex;
  }

  String getText() {
    return text;
  }

  Operation transform(Operation other) {
    // Compute the transformed version of this operation, given the other operation
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;I never specified a programming language, so assume this is a kind of pseudocode&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In this example, the TextBuffer class represents the text being edited, and the Operation class represents a single edit operation (e.g. inserting or deleting some text). The applyOperation() method is used to apply an operation to the text buffer, and the getText() method is used to retrieve the current contents of the buffer. The transform() method is used to compute the transformed version of an operation, given another operation. This is used to ensure that concurrent operations on the same text buffer can be merged together consistently.&lt;/p&gt;
&lt;p&gt;One key difference between this approach and the g-counter example is that OT explicitly tracks the operations that are applied to the data, rather than directly modifying the data itself. This makes it easier to detect and resolve conflicts between concurrent operations, but it also makes the implementation more complex.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Can you list known CRDTs and their usage in a table?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/gpt/gpt-logo.png" class="img-fluid" width="5%" alt="gpt-logo" /&gt;
Here is a list of some common Conflict-free Replicated Data Types (CRDTs), along with a brief description of each one and some potential use cases:&lt;/p&gt;
&lt;table class="table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;CRDT&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Use Cases&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;G-Counter&lt;/td&gt;
&lt;td&gt;Stores a count that can be incremented independently on different nodes, and then merged together consistently.&lt;/td&gt;
&lt;td&gt;- Versioning systems that need to keep track of the number of times a file has been modified.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;PN-Counter&lt;/td&gt;
&lt;td&gt;Stores a count that can be incremented or decremented independently on different nodes, and then merged together consistently.&lt;/td&gt;
&lt;td&gt;- Tracking the number of likes or dislikes on a social media post.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Two-Phase Set&lt;/td&gt;
&lt;td&gt;Stores a set of unique values that can be added or removed independently on different nodes, and then merged together consistently.&lt;/td&gt;
&lt;td&gt;- Collaborative editing of a list of items (e.g. a to-do list).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;OR-Set&lt;/td&gt;
&lt;td&gt;Stores a set of values that can be added independently on different nodes, and then merged together consistently.&lt;/td&gt;
&lt;td&gt;- Storing the list of friends that a user has on a social network.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;LWW-Element-Set&lt;/td&gt;
&lt;td&gt;Stores a set of values, where each value has a timestamp associated with it. The value with the most recent timestamp wins in case of conflicts.&lt;/td&gt;
&lt;td&gt;- Storing the latest version of a user's profile on a social network.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Observed-Remove Set&lt;/td&gt;
&lt;td&gt;Stores a set of values that can be added or removed independently on different nodes, and then merged together consistently.&lt;/td&gt;
&lt;td&gt;- Storing the list of items that a user has added to their shopping cart on an e-commerce website.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;These are just a few examples of CRDTs, and there are many other types of CRDTs that have been developed for different types of data and applications. In general, CRDTs can be useful whenever you need to store and update data in a distributed system, and you want to ensure that the data remains consistent even in the face of network partitions and other forms of partial failure.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Can you make a similar table for OTs?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/gpt/gpt-logo.png" class="img-fluid" width="5%" alt="gpt-logo" /&gt;
Sure! Here is a table that lists some common Operational Transform (OT) techniques, along with a brief description of each one and some potential use cases:&lt;/p&gt;
&lt;table class="table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;OT Technique&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Use Cases&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Insert-Delete OT&lt;/td&gt;
&lt;td&gt;Allows for the insertion and deletion of text, and can detect and resolve conflicts between concurrent insertions and deletions.&lt;/td&gt;
&lt;td&gt;- Collaborative text editing in a real-time editor (e.g. Google Docs).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Block-Based OT&lt;/td&gt;
&lt;td&gt;Allows for the insertion, deletion, and movement of blocks of text, and can detect and resolve conflicts between concurrent insertions, deletions, and movements.&lt;/td&gt;
&lt;td&gt;- Collaborative editing of structured documents (e.g. a spreadsheet or a presentation).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Change-Based OT&lt;/td&gt;
&lt;td&gt;Allows for arbitrary changes to be made to a document, and can detect and resolve conflicts between concurrent changes.&lt;/td&gt;
&lt;td&gt;- Collaborative editing of free-form documents (e.g. a word processor or a code editor).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;State-Based OT&lt;/td&gt;
&lt;td&gt;Stores the complete state of the document at each point in time, and can detect and resolve conflicts between concurrent changes by comparing the state of the document before and after each change.&lt;/td&gt;
&lt;td&gt;- Collaborative editing of complex data structures (e.g. a database or a network topology diagram).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Hybrid OT&lt;/td&gt;
&lt;td&gt;Combines multiple OT techniques in order to support a wider range of operations and conflicts.&lt;/td&gt;
&lt;td&gt;- Collaborative editing of documents that contain both structured and unstructured data (e.g. a document that contains both text and images).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Composable OT&lt;/td&gt;
&lt;td&gt;Allows for OTs to be composed together in order to create more complex data structures and operations.&lt;/td&gt;
&lt;td&gt;- Building collaborative editing systems that support multiple types of data and operations (e.g. a system that supports text, images, and geometry).&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Like CRDTs, there are many different OT techniques that have been developed for different types of data and applications, and this table only covers a few of the most common ones. In general, OTs can be useful whenever you need to support real-time collaboration on data, and you want to ensure that concurrent changes can be merged together consistently.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;is there a use case to use both CRDTs and OTs together?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/gpt/gpt-logo.png" class="img-fluid" width="5%" alt="gpt-logo" /&gt;
It is possible to use Conflict-free Replicated Data Types (CRDTs) and Operational Transform (OT) techniques together in some cases, although this is not always necessary or desirable. Here are a few examples of situations where using both CRDTs and OTs together might be useful:&lt;/p&gt;
&lt;p&gt;If you have a data structure that is very complex and cannot be easily represented using a single CRDT, you could use a combination of CRDTs and OTs to break the data structure down into smaller, simpler pieces that can be managed independently. For example, you could use a CRDT to store the overall structure of the data, and then use OTs to handle the details of how the individual pieces of the data are updated and changed.&lt;/p&gt;
&lt;p&gt;If you have a data structure that is updated very frequently, and you need to ensure that updates can be applied and merged together quickly, you might use a CRDT to handle the &amp;quot;big picture&amp;quot; view of the data, and then use OTs to handle the finer-grained details of how the data is changed. This can help to reduce the amount of work that needs to be done in order to merge updates together, which can improve the overall performance of the system.&lt;/p&gt;
&lt;p&gt;If you have a data structure that needs to support both offline and online updates, you might use a CRDT to handle the offline updates, and then use OTs to handle the online updates. This can allow the system to continue functioning even when the network is unavailable, and then automatically merge any offline updates with the online updates when the network becomes available again.&lt;/p&gt;
&lt;p&gt;Overall, whether or not it makes sense to use CRDTs and OTs together in a particular system will depend on the specific requirements and constraints of the system, as well as the trade-offs between the different approaches.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Are there any other similar types of data structures that also cover this space?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/gpt/gpt-logo.png" class="img-fluid" width="5%" alt="gpt-logo" /&gt;
There are many different types of data structures and algorithms that can be used to solve similar problems to those addressed by Conflict-free Replicated Data Types (CRDTs) and Operational Transform (OT) techniques. Some examples of other approaches that are related to CRDTs and OTs include:&lt;/p&gt;
&lt;p&gt;Replication protocols: These are algorithms that are used to manage the process of replicating data between different nodes in a distributed system. Replication protocols can be used to ensure that multiple copies of the data are kept consistent with each other, and that conflicts between different copies of the data are resolved in a way that preserves the integrity of the data.&lt;/p&gt;
&lt;p&gt;Consensus algorithms: These are algorithms that are used to reach agreement on the state of a distributed system, even in the presence of faulty or malicious nodes. Consensus algorithms can be used to ensure that different nodes in the system have a consistent view of the data, and that updates to the data are applied in a way that is consistent with the consensus.&lt;/p&gt;
&lt;p&gt;Distributed hash tables (DHTs): These are data structures that are used to store and retrieve data in a distributed system. DHTs use a hash function to map data to different nodes in the system, and they can be used to ensure that data is stored and retrieved in a consistent and efficient manner.&lt;/p&gt;
&lt;p&gt;Distributed databases: These are databases that are designed to be used in a distributed system, and that provide mechanisms for storing, querying, and updating data in a way that is consistent and scalable. Distributed databases can use a variety of techniques, including CRDTs and OTs, to ensure that the data remains consistent and available even in the face of network partitions and other forms of partial failure.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Give me a example of a consensus algorithm that computes the final state of a text document that is edited in realtime in multiple distributed computers. I'm looking at a peer to peer network without central authority.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/gpt/gpt-logo.png" class="img-fluid" width="5%" alt="gpt-logo" /&gt;
One example of a consensus algorithm that could be used to compute the final state of a text document that is edited in realtime in multiple distributed computers is called the &amp;quot;Paxos&amp;quot; algorithm. This algorithm is a well-known method for reaching consensus in a distributed system, and it has been used in many different types of applications, including real-time collaborative text editing.&lt;/p&gt;
&lt;p&gt;Here is a high-level overview of how the Paxos algorithm might be used to compute the final state of a text document that is being edited in realtime by multiple users:&lt;/p&gt;
&lt;p&gt;Each user maintains a local copy of the text document, and as they make changes to the document, they also generate a sequence of &amp;quot;operations&amp;quot; that represent the changes they are making. These operations are sent to the other users in the network.&lt;/p&gt;
&lt;p&gt;When a user receives an operation from another user, they apply the operation to their local copy of the document. This may result in conflicts with other operations that have already been applied to the document, in which case the user will need to resolve the conflicts in some way (e.g. by using Operational Transform (OT) techniques).&lt;/p&gt;
&lt;p&gt;As the users continue to edit the document and generate new operations, they also participate in the Paxos consensus protocol. This involves sending messages to each other in order to agree on the sequence of operations that should be applied to the document. The messages are sent in a specific order, and they are used to ensure that all of the users eventually agree on the same sequence of operations.&lt;/p&gt;
&lt;p&gt;Once all of the users have agreed on the sequence of operations, they can apply those operations to their local copies of the document in order to compute the final state of the document. This final state will be the same for all of the users, and it will reflect all of the changes that have been made to the document by any of the users.&lt;/p&gt;
&lt;p&gt;The Paxos algorithm is a complex and subtle algorithm, and this is just a very high-level overview of how it might be used in this context. There are many different variations of the Paxos algorithm, and the specific details of how it is implemented will depend on the specific requirements and constraints of the system.&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>7 reasons to not use caching</title>
			<link>http://ewinnington.github.io/posts/7Reasons-no-cache</link>
			<description>&lt;p&gt;Inspired by &lt;a href="https://twitter.com/mjovanovictech"&gt;Milan Jovanović&lt;/a&gt; tweet on &lt;a href="https://twitter.com/mjovanovictech/status/1599124855542411264"&gt;5 reasons to use Redis for caching&lt;/a&gt;,&lt;/p&gt;</description>
			<guid>http://ewinnington.github.io/posts/7Reasons-no-cache</guid>
			<pubDate>Sun, 04 Dec 2022 16:30:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;Inspired by &lt;a href="https://twitter.com/mjovanovictech"&gt;Milan Jovanović&lt;/a&gt; tweet on &lt;a href="https://twitter.com/mjovanovictech/status/1599124855542411264"&gt;5 reasons to use Redis for caching&lt;/a&gt;,&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/caching/5reasonsCaching.png" class="img-fluid" width="80%" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;and &lt;a href="https://twitter.com/danielmarbach"&gt;Daniel Marbach's&lt;/a&gt; response &amp;quot;&lt;a href="https://twitter.com/danielmarbach/status/1599352526888849408"&gt;Now I want to see five reasons to avoid caching ✋😂&lt;/a&gt;&amp;quot;&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/caching/5reasonsNoCaching.png" class="img-fluid" width="80%" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;I found &lt;a href="https://twitter.com/ThrowATwit/status/1599356806874427392"&gt;seven reasons to not introduce caching&lt;/a&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Caching can increase complexity in your application, as you need to manage the cached data and ensure it remains consistent with the underlying data store.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Caching can increase latency, as the cache itself introduces an additional lookup step.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Caching can be expensive, both in terms of the additional hardware and storage required for the cache, and the overhead of managing the cache itself.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Caching can be unreliable, as cached data can become stale or inconsistent if it is not adequately managed or invalidated.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Caching can be a security risk, as sensitive data that is stored in the cache may be vulnerable to unauthorized access or exposure. It takes additional effort to ensure that the correct authorizations are applied to cached data, increasing application complexity.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Caching can be harder to debug. To determine why a piece of data is not being retrieved from the cache or is being retrieved from the underlying data store instead is difficult. This can make it challenging to diagnose and fix performance issues related to caching.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Caching can create additional maintenance overhead, as you need to monitor the cache and ensure it is working properly. Monitoring cache hit and miss rates, ensuring that the cache is not getting too full, and periodically purging expired or stale data from the cache.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;and a bonus &lt;a href="https://mobile.twitter.com/joslat/status/1599518029649678336"&gt;8.&lt;/a&gt; from &lt;a href="https://mobile.twitter.com/joslat"&gt;Jose Luis Latorre&lt;/a&gt;
&amp;quot;8. It should be also properly tested, and stress tested... without mention the security testing as well should include a check on this layer too... which would bring us to point 3. More expensive ;)&amp;quot;&lt;/p&gt;
&lt;p&gt;Introducing Caching into any architecture is a decision that must be made with care. We have to ask if it helps us fulfill a business requirement (latency requirements), and improves quality or responsiveness for the end user. And we must ensure the solution is appropriate in terms of cost of operation and cost of monitoring and support. Additionally, the security aspects of a cache should be considered in the solution design.&lt;/p&gt;
&lt;p&gt;In software architecture, there are very few single answers, everything is a compromise. Caching is a great hammer and use it when it is appropriate, but remember not every problem is a nail.&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>Software Architecture illustrations</title>
			<link>http://ewinnington.github.io/posts/Software-Architecture-Illustration</link>
			<description>&lt;p&gt;In software architecture, I find myself reaching more and more for tools that I can use to generate representations from a simple textual description, be it generated from a tool or hand written. And sometimes, nothing generated looks nice, so I have to do draw it myself!&lt;/p&gt;</description>
			<guid>http://ewinnington.github.io/posts/Software-Architecture-Illustration</guid>
			<pubDate>Wed, 16 Nov 2022 20:10:00 GMT</pubDate>
			<content:encoded>&lt;h1 id="illustrations-in-software-architecture"&gt;Illustrations in Software architecture&lt;/h1&gt;
&lt;p&gt;In software architecture, I find myself reaching more and more for tools that I can use to generate representations from a simple textual description, be it generated from a tool or hand written. And sometimes, nothing generated looks nice, so I have to do draw it myself!&lt;/p&gt;
&lt;p&gt;Here are a few of the tools I have recently used, for different purposes:&lt;/p&gt;
&lt;h2 id="diagrams-as-code"&gt;Diagrams as code:&lt;/h2&gt;
&lt;h3 id="mermaid"&gt;Mermaid&lt;/h3&gt;
&lt;p&gt;Mermaid is a language to generate flow charts, pie charts, entity relation diagrams and several other diagrams. I’ve used it in internal documentation and blog posts. The graph description language is simple enough that you can write code to generate charts, too.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://mermaid-js.github.io/mermaid/#/"&gt;https://mermaid-js.github.io/mermaid/#/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A live editor is also available online:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://mermaid.live/"&gt;https://mermaid.live/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Mermaid rendering has been integrated into several markdown renderers, GitHub markdown and VS Code both support it.&lt;/p&gt;
&lt;div class="mermaid"&gt;flowchart LR
    a[Airflow] ---&gt; b[AirFlowTask] --&gt; c[[RabbitMQ Queue Events]] --&gt; d[EventReceiver] --insert--&gt; e[(Postgresql)] --&gt; Monitoring
    d --failed--&gt; g[[Deadletter queue]] --&gt; h[Reconciliation] --&gt; e
&lt;/div&gt;
&lt;p&gt;&lt;img src="/posts/images/SA-Illustrations/Mermaid-flow.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;div class="mermaid"&gt;sequenceDiagram
    autonumber

    participant C as Client
    participant S as Target

    S --) C: Communicate API-Key
    C -&gt;&gt; S: Send request with API-Key
    activate S
    S --&gt;&gt; S: Validate API-Key
    S -X C: If not valid: Return 401 
    S -&gt;&gt; C: If valid: Return 200
    deactivate S
&lt;/div&gt;
&lt;p&gt;&lt;img src="/posts/images/SA-Illustrations/Mermaid-sequence.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;h3 id="python-diagrams"&gt;Python Diagrams&lt;/h3&gt;
&lt;p&gt;Python has a diagram library which has icons for most programming tools, from Airflow to ZeroMQ. You design the diagram with simple Python code and it uses the Graphviz library to render png images. Highly recommended for small architecture diagrams that just need a dozen or so elements.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://diagrams.mingrammer.com/"&gt;https://diagrams.mingrammer.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I’ve also done some pull requests to add symbols to the library and I recommend you do so too if you have elements that are missing in your diagrams.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;## pip install diagrams
## winget install -e --id Graphviz.Graphviz
## set PATH=C:\Program Files\Graphviz\bin;%PATH%
#
# Architecture of the docker-compose using a chart generated in py

from diagrams import Diagram, Cluster
from diagrams.onprem.inmemory import Redis
from diagrams.onprem.database import Postgresql
from diagrams.onprem.queue import Rabbitmq
from diagrams.programming.language import PHP
from diagrams.programming.language import Csharp
from diagrams.onprem.client import Users
from diagrams.onprem.network import Nginx

with Diagram(&amp;quot;Composed Docker&amp;quot;, show=False):
    users = Users(&amp;quot;users&amp;quot;)
    
    with Cluster(&amp;quot;Front-End&amp;quot;):
        web = Nginx(&amp;quot;ngweb&amp;quot;)
        php = PHP(&amp;quot;php&amp;quot;)

    with Cluster(&amp;quot;Back-Ends&amp;quot;):
        redis = Redis(&amp;quot;cache&amp;quot;)
        rabbit = Rabbitmq(&amp;quot;rabbit&amp;quot;)
        listener = Csharp(&amp;quot;listener&amp;quot;)
        db = Postgresql(&amp;quot;db&amp;quot;)
    
    users &amp;gt;&amp;gt; web &amp;gt;&amp;gt; php &amp;gt;&amp;gt; rabbit &amp;gt;&amp;gt; listener &amp;gt;&amp;gt; db
    php &amp;gt;&amp;gt; redis
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="/posts/images/SA-Illustrations/composed_docker.png" class="img-fluid" width="80%" alt="" /&gt;&lt;/p&gt;
&lt;h3 id="plantuml"&gt;PlantUML&lt;/h3&gt;
&lt;p&gt;The big one! Plant UML has a ton of diagrams, the language is maybe a bit more obscure and complicated than mermaid, but you gain a lot from PlantUML when you actually need those features.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://plantuml.com/"&gt;https://plantuml.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;PlantUML also has a live editor online:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.plantuml.com/plantuml/uml/"&gt;https://www.plantuml.com/plantuml/uml/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Actually, I’ve used it quite seldomly, being able to cover most requirements with Mermaid and Python Diagrams.&lt;/p&gt;
&lt;h3 id="d2"&gt;D2&lt;/h3&gt;
&lt;p&gt;D2 has recently been open-sourced and made available. I haven't yet had time to test it.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://d2lang.com/tour/intro/"&gt;https://d2lang.com/tour/intro/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="diagrams-as-drawing"&gt;Diagrams as Drawing:&lt;/h2&gt;
&lt;h3 id="diagrams"&gt;Diagrams&lt;/h3&gt;
&lt;p&gt;Diagrams.net has both an online and an offline version of a vector drawing software that works exceedingly well for software architecture illustrations. With symbols for most public cloud platforms included in their delectable libraries, you’ll be able to find the right symbol you need.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.diagrams.net/"&gt;https://www.diagrams.net/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There’s even a Visual Studio Code extension for editing a diagram inside the IDE. Export can be done to PNG easily. Diagrams produced are easily embedded in Atlassian’s wiki and other wiki products.&lt;/p&gt;
&lt;p&gt;Highly recommend if you need to place your architecture elements instead of relying on the auto layout of diagrams as code. Now I just wish that the diagrams as code tools could create a diagram baseline compatible with this tool to modify the layout.&lt;/p&gt;
&lt;h3 id="archi-archimate-modelling"&gt;Archi (Archimate modelling)&lt;/h3&gt;
&lt;p&gt;If you use the &lt;a href="https://en.m.wikipedia.org/wiki/ArchiMate"&gt;Archimate modelling language&lt;/a&gt;, then this is the tool for you to build your modelling concepts. The formalism is great for making something that everyone can “read” once trained on it, but the investment can be quite high to do the correct modelling of your infrastructure with this tool.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.archimatetool.com/"&gt;https://www.archimatetool.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I used the Local application running on Windows.&lt;/p&gt;
&lt;h4 id="online-architecture-repositories"&gt;Online architecture repositories&lt;/h4&gt;
&lt;p&gt;There is also online hosted versions of architecture repository tools using Archimate &lt;a href="https://www.boc-group.com/en/adoit/"&gt;Adoit EA Suite&lt;/a&gt; and an associated community version too.&lt;/p&gt;
&lt;p&gt;Alternatively, there's also:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.mega.com/hopex-platform"&gt;Hopex's MEGA&lt;/a&gt; which I only used as it was being decommissioned&lt;/li&gt;
&lt;li&gt;&lt;a href="https://sparxsystems.com/products/ea/index.html"&gt;Sparx Systems’ Enterprise Architect&lt;/a&gt; which I used for a short amount of time before the company standardised on Adoit.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="yed"&gt;yEd&lt;/h3&gt;
&lt;p&gt;In writing this article, I discovered the tool yEd and wanted to mention it for completeness, I haven't had the opportunity to use it yet. It does mention many of the illustration types that are useful (BPML, Flowcharts, UML, ...).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.yworks.com/products/yed"&gt;https://www.yworks.com/products/yed&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="diagrams-from-programming"&gt;Diagrams from programming:&lt;/h2&gt;
&lt;p&gt;Now these are more out there and not always directly applicable, but when you need a visualisation that the above tools cannot do, it’s time to break out these applications.&lt;/p&gt;
&lt;h3 id="d3js"&gt;D3js&lt;/h3&gt;
&lt;p&gt;Not sure if I need to introduce D3js,  it is probably one of the most commonly use and important is visualisation libraries. Used in everything from maps to genomics to economic data. It can do it all.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://d3js.org/"&gt;https://d3js.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I previously used d3js to embed charts of price curves generated from market data, overlapped with the delivery period of energy instruments.&lt;/p&gt;
&lt;h3 id="processing.js"&gt;Processing.js&lt;/h3&gt;
&lt;p&gt;Animated heart pulsating on a field of scintillating gold lace? Yes. That and many more things! Processing excels in the visual demos, animations and more. You’ll have to code it, but it’s no issue to get your custom Mandelbrot animated render and many more. Has interactions with sound and many more features. Does have a high bar of entry though and it takes a while to be productive with it.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://processing.org/"&gt;https://processing.org/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="graphviz"&gt;Graphviz&lt;/h3&gt;
&lt;p&gt;I would be remiss if I didn’t mention Graphviz. It is the library used to generate diagrams from textual descriptions using one of their many languages, Dot being one of them.&lt;/p&gt;
&lt;p&gt;Not something I use directly but more indirect usages via the Python diagrams library. You can learn it’s language and create charts.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://graphviz.org/"&gt;https://graphviz.org/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="online-services-collaborative"&gt;Online services (collaborative)&lt;/h2&gt;
&lt;p&gt;When you need online collaboration, which the tools above do not cover, you can turn to the following services.&lt;/p&gt;
&lt;h3 id="miro"&gt;Miro&lt;/h3&gt;
&lt;p&gt;I’ve had the most experience with Miro while running large organisation meetings as a place to collect ideas, do feedback rounds and generally plan activities.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://miro.com"&gt;https://miro.com&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="lucidspark-lucidchart-and-lucidscale"&gt;LucidSpark, LucidChart and LucidScale&lt;/h3&gt;
&lt;p&gt;Both collaboration and vector illustration online software. Also their ability with lucid scale to connect and document your cloud infrastructure is very impressive and helps to keep you infrastructure maps updated.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.lucidspark.com/"&gt;https://www.lucidspark.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.lucidscale.com/"&gt;https://www.lucidscale.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.lucidchart.com/"&gt;https://www.lucidchart.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Sadly, it’s one that I’ve not had the opportunity to use very often, usually because it was covered by other tools and no one else in the company was using it. But you should check it out to see if it does work for you and your team.&lt;/p&gt;
&lt;h3 id="microsoft-whiteboard"&gt;Microsoft Whiteboard&lt;/h3&gt;
&lt;p&gt;When all else fails, there’s Microsoft whiteboard. It works, there’s an online version, an integration in teams, a desktop app and even an iOS / iPadOS application. More suited to drawing with a pen, then it becomes a great collaborative whiteboard. I have given internal talks using a Microsoft Whiteboard as a backdrop. I really like to start small and progressively zoom out on these massive canvases.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.microsoft.com/en-us/microsoft-365/microsoft-whiteboard/digital-whiteboard-app"&gt;https://www.microsoft.com/en-us/microsoft-365/microsoft-whiteboard/digital-whiteboard-app&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="mind-maps"&gt;Mind maps&lt;/h2&gt;
&lt;p&gt;I don't use mind maps very often anymore. I was taught to use them as a child, but haven't kept up the practice. I'm just adding a couple of references in case you are looking for them:&lt;/p&gt;
&lt;h3 id="vscode-mindmap"&gt;vscode-mindmap&lt;/h3&gt;
&lt;p&gt;I've used vscode-mindmap when I needed to create a quick hierarchy map.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://marketplace.visualstudio.com/items?itemName=pmcxs.vscode-mindmap"&gt;https://marketplace.visualstudio.com/items?itemName=pmcxs.vscode-mindmap&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;There’s no one tool for everything in this day and age. Use what works for you and try out several to see if they stick!&lt;/p&gt;
&lt;p&gt;I highly recommend generating some charts in mermaid from your own database ER-Diagram (easy to do!) or using it to make pie charts like &lt;a href="https://youtu.be/IXRGa5m-Lbo"&gt;Microsoft Polyglot notebooks demonstrates at 15:05 onwards&lt;/a&gt; in their &lt;a href="https://github.com/dotnet/interactive/blob/main/samples/notebooks/polyglot/github%20repo%20milestone%20report.ipynb"&gt;GitHub demo notebook&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you have other recommendations for me, do feel free to reach out and I’ll see if they make to cut to get added to this list. :) You can even pull request this actual blog post on GitHub.&lt;/p&gt;
&lt;p&gt;Finally, if someone has a recommendation for a WAN/LAN topology visualisation or charting tool, I’d be happy to hear about it and your experience with it!&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>Data Lineage for dataflow and workflow processes</title>
			<link>http://ewinnington.github.io/posts/Data-Lineage</link>
			<description>&lt;p&gt;When working with large amounts of data, extraction, transforms and loads procedures can hide the source of the original data and make inquiries on "where did this data come from and what happened to it?" difficult to answer.&lt;/p&gt;</description>
			<guid>http://ewinnington.github.io/posts/Data-Lineage</guid>
			<pubDate>Sat, 12 Nov 2022 22:10:00 GMT</pubDate>
			<content:encoded>&lt;h1 id="data-lineage"&gt;Data lineage&lt;/h1&gt;
&lt;p&gt;When working with large amounts of data, extraction, transforms and loads procedures can hide the source of the original data and make inquiries on &amp;quot;where did this data come from and what happened to it?&amp;quot; difficult to answer.&lt;/p&gt;
&lt;p&gt;A data lineage is &amp;quot;the process of understanding, recording, and visualizing data as it flows from data sources to consumption&lt;a id="fnref:1" href="#fn:1" class="footnote-ref"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&amp;quot; and tries to answer that question.&lt;/p&gt;
&lt;p&gt;Using dataflow and ETL orchestration tools such as &lt;a href="https://airflow.apache.org/"&gt;Airflow&lt;/a&gt;, &lt;a href="https://www.prefect.io"&gt;Prefect&lt;/a&gt;, &lt;a href="https://nifi.apache.org/"&gt;NiFi&lt;/a&gt;, we move and transform data, but also lose the reference as to how the data was transformed.&lt;/p&gt;
&lt;p&gt;In this document, we will approach one open source tool OpenLineage and one &amp;quot;hand built&amp;quot; approach to capturing and storing data lineage information.&lt;/p&gt;
&lt;h1 id="openlineage-and-marquez-open-source-tools"&gt;OpenLineage and Marquez - Open source tools&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://openlineage.io/"&gt;OpenLineage&lt;/a&gt; is an open source project and framework for data lineage collection and analysis that helps collect lineage metadata from the data  processing applications. At its core, OpenLineage exposes a standard API for metadata collection - a single API call: &lt;a href="https://openlineage.io/apidocs/openapi/"&gt;&lt;strong&gt;postRunEvent&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To simplify its implementation with AirFlow, Open Lineage has an &lt;a href="https://github.com/OpenLineage/OpenLineage/tree/main/integration/airflow/openlineage/airflow"&gt;airflow connection module&lt;/a&gt; already available.&lt;/p&gt;
&lt;p&gt;On the back-end, the storage of run meta-data has a reference implementation named Marquez. The data model is illustrated here.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://lucid.app/lucidchart/f918ce01-9eb4-4900-b266-49935da271b8/view?page=8xAE.zxyknLQ#"&gt;Marquez data model&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/data-lineage/Marquez-Data-Model.png" class="img-fluid" width="80%" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;But it is also possible to implement one's own storage for metadata in case there is a need or added value, but adopting the open source solution will be an advantage for integration later.&lt;/p&gt;
&lt;h1 id="locally-grown-alternatives"&gt;Locally grown alternatives&lt;/h1&gt;
&lt;p&gt;Data correlation and lineage information can be generated via the emission of events while processing input data. Additionally, input data can be fingerprinted via a fast hash function to check for duplicate imports, so as to enable idempotent processing.&lt;/p&gt;
&lt;h3 id="input-dataset-fingerprinting-via-non-cryptographic-hash-function"&gt;Input dataset fingerprinting via non-cryptographic hash function&lt;/h3&gt;
&lt;p&gt;We can use a fast non-cryptographic hash function such as &lt;a href="https://github.com/backtrace-labs/umash"&gt;umash&lt;/a&gt; to generate a hash of the input data or xxhash &lt;code&gt;sudo apt-get install xxhash&lt;/code&gt;. xxhash is capable of taking streaming STDIN data from compressed files to generate a fast hash.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;time gunzip -c /mnt/d/smart_meter_data/ckw_opendata_smartmeter_dataset_a_202101.csv.gz | /usr/bin/xxhsum
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="correlation-ids-with-uuids"&gt;Correlation IDs with UUIDs&lt;/h3&gt;
&lt;p&gt;Generating uuid for correlations identifiers along &lt;a href="https://www.rfc-editor.org/rfc/rfc4122.html"&gt;rfc4122&lt;/a&gt; gives us multiple variant generation algorithms to give us sortable UUIDs which minimize collision possibilities even with a high UUID generation rate.&lt;/p&gt;
&lt;h3 id="hierarchical-correlation-ids-using-closure-tables"&gt;Hierarchical correlation IDs using closure tables&lt;/h3&gt;
&lt;p&gt;When inputs contain a dataset that is composed of multiple data points that identify unique sets in our final processed dataset, it becomes necessary to be able to trace their lineage back to the initial input. A recursive search through a table of entries to find the parent correlation identifier of a child time-series is quite inefficient.&lt;/p&gt;
&lt;p&gt;To have fast search over deep hierarchies of correlations IDs in relational databases, we can turn to the concept of closure tables.&lt;/p&gt;
&lt;table class="table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align: left;"&gt;Field&lt;/th&gt;
&lt;th style="text-align: left;"&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;ParentId&lt;/td&gt;
&lt;td style="text-align: left;"&gt;UUID&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;ChildId&lt;/td&gt;
&lt;td style="text-align: left;"&gt;UUID&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;Depth&lt;/td&gt;
&lt;td style="text-align: left;"&gt;integer&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This table structure allows to query in one query all parent or children of an identifier in one non-recursive sql query. This is done at the cost of having to insert the entire hierarchy of the correlationIDs upon insertion.&lt;/p&gt;
&lt;p&gt;Here we are representing the two hierarchies&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;aaa &amp;gt; bbb &amp;gt; ccc
aaa &amp;gt; eee 
&lt;/code&gt;&lt;/pre&gt;
&lt;table class="table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align: left;"&gt;ParentId&lt;/th&gt;
&lt;th style="text-align: left;"&gt;ChildId&lt;/th&gt;
&lt;th style="text-align: right;"&gt;Depth&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;aaa&lt;/td&gt;
&lt;td style="text-align: left;"&gt;aaa&lt;/td&gt;
&lt;td style="text-align: right;"&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;bbb&lt;/td&gt;
&lt;td style="text-align: left;"&gt;bbb&lt;/td&gt;
&lt;td style="text-align: right;"&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;ccc&lt;/td&gt;
&lt;td style="text-align: left;"&gt;ccc&lt;/td&gt;
&lt;td style="text-align: right;"&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;eee&lt;/td&gt;
&lt;td style="text-align: left;"&gt;eee&lt;/td&gt;
&lt;td style="text-align: right;"&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;aaa&lt;/td&gt;
&lt;td style="text-align: left;"&gt;bbb&lt;/td&gt;
&lt;td style="text-align: right;"&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;bbb&lt;/td&gt;
&lt;td style="text-align: left;"&gt;ccc&lt;/td&gt;
&lt;td style="text-align: right;"&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;aaa&lt;/td&gt;
&lt;td style="text-align: left;"&gt;ccc&lt;/td&gt;
&lt;td style="text-align: right;"&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;aaa&lt;/td&gt;
&lt;td style="text-align: left;"&gt;eee&lt;/td&gt;
&lt;td style="text-align: right;"&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;em&gt;note&lt;/em&gt;: &lt;em&gt;if desired the initial 0 depth nodes can be neglected from the insertion process without losing functionality, but can be useful in certain modeling processes (eg. rights, groups)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;If we need to find all children of &lt;strong&gt;aaa&lt;/strong&gt;, we can do a&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SELECT ChildId, Depth FROM Closure_Table WHERE ParentId = &amp;quot;aaa&amp;quot; ORDER BY Depth;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which returns the following&lt;/p&gt;
&lt;table class="table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align: left;"&gt;ChildId&lt;/th&gt;
&lt;th style="text-align: left;"&gt;Depth&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;aaa&lt;/td&gt;
&lt;td style="text-align: left;"&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;bbb&lt;/td&gt;
&lt;td style="text-align: left;"&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;eee&lt;/td&gt;
&lt;td style="text-align: left;"&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;ccc&lt;/td&gt;
&lt;td style="text-align: left;"&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;If we need to find all parents of &lt;strong&gt;eee&lt;/strong&gt;, we can do a&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SELECT ParentId, Depth FROM Closure_Table WHERE ChildId = &amp;quot;eee&amp;quot; ORDER BY Depth DESC;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which returns the following&lt;/p&gt;
&lt;table class="table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align: left;"&gt;ParentId&lt;/th&gt;
&lt;th style="text-align: left;"&gt;Depth&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;aaa&lt;/td&gt;
&lt;td style="text-align: left;"&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;eee&lt;/td&gt;
&lt;td style="text-align: left;"&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id="table-structure-for-a-correlated-fingerprinted-hierarchical-data-lineage"&gt;Table structure for a Correlated, fingerprinted hierarchical data lineage&lt;/h3&gt;
&lt;p&gt;With correlatedEvent and CorrelatedLineage tables, it becomes possible in a single request to generate a lineage graph for parents or descendants of correlated dataset.&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/data-lineage/CorrelatedLineageDataModel.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;erDiagram
    CorrelationEvent {
        uuid     IdEvent
        string   Process
        string   Version
        hash     Fingerprint
        int      InputSize
        datetime EventTime
        int      Forced
    }

    CorrelationDetails {
        uuid IdEvent
        string Field
        json Data 
    }

    CorrelationLineage {
        uuid IdParent
        uuid IdChild
        integer Depth
    }

    CorrelationEvent ||--o{ CorrelationDetails : &amp;quot;&amp;quot;
    CorrelationEvent ||--o{ CorrelationLineage : &amp;quot;parent&amp;quot;
    CorrelationEvent ||--o{ CorrelationLineage : &amp;quot;child&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It would even be possible to generate the diagrams using mermaid automatically to trace the flows through the system&lt;a id="fnref:2" href="#fn:2" class="footnote-ref"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/data-lineage/CorrelatedLineageFlow.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;flowchart LR
    id1((&amp;quot;DSO Timeseries&amp;quot;)) --&amp;gt; id2[SFTP Download] --&amp;gt; id3[Split]
    id3 --&amp;gt; TS01
    id3 --&amp;gt; TS02
    id3 --&amp;gt; TS03 
    id3 --&amp;gt; TS04
    id3 --&amp;gt; TS..
    TS01 --&amp;gt; id4[Delivery point sum]
    TS02 --&amp;gt; id4
    id4 --&amp;gt; id5[Load]

    met((Weather provider)) --&amp;gt; met2[API Download] --&amp;gt; met3[&amp;quot;Aggregate to hour&amp;quot;] --&amp;gt; met4[&amp;quot;delivery point history&amp;quot;]

    met4 --&amp;gt; for1[forecast consumption]
    id4 --&amp;gt; for1 --&amp;gt; for2[Load]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="solution-design"&gt;Solution design&lt;/h3&gt;
&lt;p&gt;A rabbitMQ message queue to receive correlation events emitted by the tasks, with several consumer tasks receiving and committing to the database is a preferred approach over an HTTP 1.1 connection due the the scaling efficiency of AMQP over pure HTTP&lt;a id="fnref:3" href="#fn:3" class="footnote-ref"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/data-lineage/CorrelatedApplicationFlow.png" class="img-fluid" width="80%" alt="" /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;flowchart LR
    a[Airflow] ---&amp;gt; b[AirFlowTask] --&amp;gt; c[[RabbitMQ Queue Events]] --&amp;gt; d[EventReceiver] -- success --&amp;gt; g[(Postgresql)] --&amp;gt; Monitoring
    d -- failed --&amp;gt; e[[RabbitMQ Queue Deadletter]] --&amp;gt; f[DLQ processing and Reconciliation] --&amp;gt; g
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A particular focus on the monitoring of the solution is necessary to truly have an operational system. The RabbitMQ should be a redundant, instrumented and reported to Graphana, with a queue length monitoring in place. The EventReceivers should employ a dead letter queue in case message are rejected by the database. These rejected messages could also also be a uuid collision - which can be treated by the daily reconciliation process and DeadLetter queue processing.&lt;/p&gt;
&lt;p&gt;A high availability Postgresql is recommended, either as a local instance or as a cloud hosted service - which would facilitate operations.&lt;/p&gt;
&lt;p&gt;The issue of Data retention should be discussed with Business. If we do not keep a time-series history in the time-series datastore, then the event correlation become actually an &lt;a href="https://microservices.io/patterns/data/event-sourcing.html"&gt;event sourcing pattern&lt;/a&gt;, enabling to re-create the history of how the time-series was updated.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Irrespective of the method chosen to capture and store the messages, the systems chosen must provide a high availability solution for data lineage - but must be sure to not block ingestion if the data lineage system is unresponsive. As long as the message queue is persistent and accessible, it can always be caught up later.&lt;/p&gt;
&lt;p&gt;The main task is emitting the events with meaningful data and unique correlation IDs. A focus on the semantics of the events while developing the workflow / dataflows is primordial. A callable event library provides the best developer experience to maximize standardization of code&lt;/p&gt;
&lt;p&gt;The design of idempotent imports into the system is important, it allows to replay events non-destructively and provides operational resilience.&lt;/p&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;&lt;a href="https://www.imperva.com/learn/data-security/data-lineage/"&gt;https://www.imperva.com/learn/data-security/data-lineage/&lt;/a&gt;&lt;a href="#fnref:1" class="footnote-back-ref"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;&lt;a href="https://github.com/dotnet/interactive/blob/main/samples/notebooks/polyglot/github%20repo%20milestone%20report.ipynb"&gt;https://github.com/dotnet/interactive/blob/main/samples/notebooks/polyglot/github%20repo%20milestone%20report.ipynb&lt;/a&gt; - See the PieWithMermaid C# task for a visualisation of such an interaction.&lt;a href="#fnref:2" class="footnote-back-ref"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:3"&gt;
&lt;p&gt;This should be re-evaluated when HTTP/3 oneshot becomes available in the servers and languages used. The expected performance improvement are such that at that time HTTP/3 QUIC might outrace any other streaming solution. &lt;a href="https://blog.cloudflare.com/http3-the-past-present-and-future/"&gt;https://blog.cloudflare.com/http3-the-past-present-and-future/&lt;/a&gt;&lt;a href="#fnref:3" class="footnote-back-ref"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</content:encoded>
		</item>
		<item>
			<title>Tesla Megapacks put into context</title>
			<link>http://ewinnington.github.io/posts/tesla-megapack</link>
			<description>&lt;p&gt;Tesla on Twitter announced: &lt;a href="https://t.co/aw85eHECXI"&gt;"Meet Megafactory, our new Megapack factory in Lathrop, CA 🔋🔋🔋"&lt;/a&gt;&lt;/p&gt;</description>
			<guid>http://ewinnington.github.io/posts/tesla-megapack</guid>
			<pubDate>Wed, 09 Nov 2022 21:40:00 GMT</pubDate>
			<content:encoded>&lt;h1 id="tesla-megapacks"&gt;Tesla Megapacks&lt;/h1&gt;
&lt;p&gt;Tesla on Twitter announced: &lt;a href="https://t.co/aw85eHECXI"&gt;&amp;quot;Meet Megafactory, our new Megapack factory in Lathrop, CA 🔋🔋🔋&amp;quot;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Tesla's energy division has recently completed their new Megapack factory in Lathrop California , which they claim can produce currently 10'000 Megapacks a year. How much storage is that and how does this compare to a Hydropower pump storage plant?&lt;/p&gt;
&lt;h2 id="tesla-megapacks-specs-per-pack"&gt;Tesla megapacks Specs per pack&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;4 Hour Duration&lt;/li&gt;
&lt;li&gt;Power: 970 kW&lt;/li&gt;
&lt;li&gt;Energy: 3,916 kWh per Megapack&lt;/li&gt;
&lt;li&gt;Round Trip Efficiency: 93.5%&lt;/li&gt;
&lt;li&gt;9.12 m x 1.65 m x 2.79 m&lt;/li&gt;
&lt;li&gt;38,100 kg&lt;/li&gt;
&lt;li&gt;~$2 million per pack&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="offer"&gt;Offer&lt;/h2&gt;
&lt;p&gt;An offer was generated on the Tesla Energy website to get an appropriate pricing for the largest system they offer.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1000 Megapack&lt;/li&gt;
&lt;li&gt;969.6 MW Power&lt;/li&gt;
&lt;li&gt;3916 MWh Energy Megapack&lt;/li&gt;
&lt;li&gt;Duration: 4 Hours&lt;/li&gt;
&lt;li&gt;Delivery: Q3 2024&lt;/li&gt;
&lt;li&gt;Estimated Price (California) $1,832,519,850&lt;/li&gt;
&lt;li&gt;Est. Annual Maintenance $4,821,480 - Maintenance Price escalates at 2% per year&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Based on this, we can see that 10'000 megapacks represent about 39160 MWh of storage (39 GWh), with a sales cost of approx $18 billion.&lt;/p&gt;
&lt;p&gt;So how does this compare to the two latest large Swiss Pump-Storage Hydropowerplants?&lt;/p&gt;
&lt;h2 id="hydropower-plants"&gt;Hydropower plants&lt;/h2&gt;
&lt;h3 id="nant-de-drance-pump-storage-extension"&gt;Nant-de-Drance pump-storage extension&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;1 Pump-storage power plant&lt;/li&gt;
&lt;li&gt;Power 900 MW (Turbines and Pumps)&lt;/li&gt;
&lt;li&gt;Storage 20 GWh&lt;/li&gt;
&lt;li&gt;Duration: 19 Hours&lt;/li&gt;
&lt;li&gt;Round trip efficiency: over 90%&lt;/li&gt;
&lt;li&gt;Estimated Price 2 billion CHF&lt;/li&gt;
&lt;li&gt;~14 years to build and bring into operation&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="kraftwerk-linthlimmern-pump-storage-extension"&gt;Kraftwerk-Linth–Limmern pump-storage extension&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;1 Pump-storage power plant&lt;/li&gt;
&lt;li&gt;Power 1000 MW (Turbines and Pumps)&lt;/li&gt;
&lt;li&gt;Storage 33 GWh&lt;/li&gt;
&lt;li&gt;Duration: 33 Hours&lt;/li&gt;
&lt;li&gt;Round trip efficiency: over 90%&lt;/li&gt;
&lt;li&gt;Estimated Price 2.1 billion CHF&lt;/li&gt;
&lt;li&gt;~10 years to build and bring into operation&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The storage cost of the batteries is currently about a factor 4-9x the price of the hydropower plant construction but have the advantage of being available within about 18 months. What remains to be seen is how much battery degradation is a factor in these grid scale battery installations. At least Tesla is offering, from my understanding, a 15 year warranty on the Megapack.&lt;/p&gt;
&lt;p&gt;The amount of storage produced by the factory represents more than 1 large hydropower plant per year.&lt;/p&gt;
&lt;p&gt;The 39 GWh storage produced by the factory in one year is a huge amount, so much that it would cover around 25% of the &lt;a href="https://www.iea.org/data-and-statistics/charts/battery-storage-capability-by-countries-2020-and-2026"&gt;expected total capacity that the IEA planned for the entire world by 2026&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="post-scriptum-new-gridscale-batteries-in-europe"&gt;Post-scriptum: new gridscale batteries in Europe&lt;/h2&gt;
&lt;p&gt;2022.11.22 - &lt;a href="https://www.bbc.com/news/uk-england-humber-63707463"&gt;Cottingham: Europe's biggest battery storage system switched on - 196MWh&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Power: ? (my estimate ~50-100 MW)&lt;/li&gt;
&lt;li&gt;Storage: 196 MWh&lt;/li&gt;
&lt;li&gt;use Tesla's AI software to match energy supply to demand&lt;/li&gt;
&lt;li&gt;Commissioning in two stages in December 2022 and March 2023.&lt;/li&gt;
&lt;li&gt;Supplier: Tesla&lt;/li&gt;
&lt;li&gt;Cost: ? (my estimate $100 million+)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As usual, BBC is terribly uninformative about specifications and cost. If we assume 50 Tesla Megapacks, cost should be around $100 million+ and 50 to 100 MW based on the 2h or 4h megapacks. Interesting to see a Tesla system in Europe. I expect many more to come online.&lt;/p&gt;
&lt;p&gt;2022.11.07 - &lt;a href="https://www.rwe.com/en/press/rwe-generation/2022-11-07-battery-storage-220-mw-neurath"&gt;RWE gives green light for 220-megawatt battery storage system in North Rhine-Westphalia&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Power: 80 + 140 MW = 220 MW&lt;/li&gt;
&lt;li&gt;Storage: delivering the required output for over an hour but full capacity not mentioned. 220 MWh to 440MWh.&lt;/li&gt;
&lt;li&gt;140 million euros&lt;/li&gt;
&lt;li&gt;commissioning in 2024&lt;/li&gt;
&lt;li&gt;Supplier: Not mentioned.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;2021.07.22 - &lt;a href="https://www.rwe.com/en/press/rwe-ag/2021-07-22-rwe-builds-one-of-the-largest-battery-storage-facilities-in-germany"&gt;RWE bringing 72MW BESS in Germany online in November&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Power: 72 + 45MW = 117 MW&lt;/li&gt;
&lt;li&gt;Storage: 128MWh&lt;/li&gt;
&lt;li&gt;€50 million&lt;/li&gt;
&lt;li&gt;commissioning in end 2022&lt;/li&gt;
&lt;li&gt;Supplier: CATL batteries&lt;/li&gt;
&lt;/ul&gt;
</content:encoded>
		</item>
		<item>
			<title>Embracing SQLite and living with micro-services</title>
			<link>http://ewinnington.github.io/posts/sqlite-microstores</link>
			<description>&lt;p&gt;The idea of micro-services and their own single purpose data stores is easy to describe. But then to implement and live with it is a different story. So as a developer and architect, I’ve decided to do just that! Make micro-services and micro-data stores to cover the tiny and small stuff in my life I want to keep track of.&lt;/p&gt;</description>
			<guid>http://ewinnington.github.io/posts/sqlite-microstores</guid>
			<pubDate>Sat, 22 Oct 2022 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;The idea of micro-services and their own single purpose data stores is easy to describe. But then to implement and live with it is a different story. So as a developer and architect, I’ve decided to do just that! Make micro-services and micro-data stores to cover the tiny and small stuff in my life I want to keep track of.&lt;/p&gt;
&lt;p&gt;As an example, I read online comics, light novels and mangas. I had a continuous list of a couple hundred bookmarks that I tried to keep updated with the last position I was when I read the story. But I always forget to update the bookmark and have so many of them that I lose the last read chapter. My solution?&lt;/p&gt;
&lt;p&gt;A SQLite db and some Python code to load it. Pass a single url on some command line Python and it gets added to the Db, a Request goes out, gets the title and chapter from the html, then adds it to the DB by title. Now I have a track of where I left off and I can get have last updated / last read records. Bonus, I can do a SELECT .. ORDER BY updated LIMIT 10 to check the last stories I was reading and pipe them to my browser to open up the chapters where I left them off.&lt;/p&gt;
&lt;p&gt;To really embrace SQLite is to make everything in your life become a new micro database, even if there's only a couple of tables with a dozen or a hundred rows.&lt;/p&gt;
&lt;p&gt;Stock tracking? an SQLite Db with Transactions and a roll-up Inventory table.&lt;/p&gt;
&lt;p&gt;In fact, even when sending data around from one system to another, we should even embrace the simplicity of SQLite over CSV files. See &lt;a href="https://berthub.eu/articles/posts/big-data-storage/"&gt;https://berthub.eu/articles/posts/big-data-storage/&lt;/a&gt; for his views and performance tests.&lt;/p&gt;
&lt;p&gt;Now I have micro data-stores, I can add a service on top which contains the CRUD commands I need to interact with them and show them in a personal dashboard.&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>The case for a SpaceX Starship laser ablation platform for orbital debris management</title>
			<link>http://ewinnington.github.io/posts/Starship-laser-ablation</link>
			<description>&lt;p&gt;SpaceX’s Starship program as a platform for specialised load-outs has many potential applications: Tanker variants for orbital refuelling, Crew variant for Dear Moon mission and dedicated satellite launcher for the Starlink satellite constellation deployment being the variants we already know about.&lt;/p&gt;</description>
			<guid>http://ewinnington.github.io/posts/Starship-laser-ablation</guid>
			<pubDate>Sun, 18 Sep 2022 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;SpaceX’s Starship program as a platform for specialised load-outs has many potential applications: Tanker variants for orbital refuelling, Crew variant for Dear Moon mission and dedicated satellite launcher for the Starlink satellite constellation deployment being the variants we already know about.&lt;/p&gt;
&lt;p&gt;With the latest discussion about Orbital debris fields, I suggest it is time to discuss about another variant: a dedicated laser ablation Starship variant for de-orbiting or destroying 1 to 10 cm sized debris.&lt;/p&gt;
&lt;p&gt;To deorbit or destroy debris in Earth’s orbit, laser ablation is one of the ideal techniques to use since it can directly burn up small debris or  deorbit larger ones via plasma propulsion (in effect, burning up the target object and causing it to be propelled by the plasma generated by the laser hitting the target).&lt;/p&gt;
&lt;p&gt;To be able to make orbital cleanup affordable, we need to have a cheap to fly, high power laser with a sufficient burn time and have the ability to choose the orbit of our laser platform.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cheap to fly: The launch costs of a SpaceX starship is estimated to be at least one order of magnitude less in dollar per kilo to orbit.&lt;/li&gt;
&lt;li&gt;High power laser: The high power of chemical lasers along with their high mass requirements make them a good fit for integration into a Starship. With sufficient mass for the chemicals to supply the laser, the lasers could be used long enough to clear the coplanar orbit. Once the chemicals are depleted, the starship can be landed and the laser refuelled for another mission. Other types of lasers, which have been developed recently, are also candidates: Solid state lasers, fiber lasers, diode lasers - with these the mass capability of Starship would be used for large batteries and potentially hydrogen fuel cells to provide enough power for the application.&lt;/li&gt;
&lt;li&gt;Ability to choose orbit: Laser ablation is most effective when the platform if shooting from a “same altitude and coplanar” orbit. Different launches are then the most effective way of reaching these orbits to get maximal efficiency.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If the &lt;a href="https://www.fastcompany.com/90789865/orbits-act-what-to-know-about-congress-effort-to-clean-up-space-debris"&gt;US government decides to start paying for orbital clearing services&lt;/a&gt; with the Orbital Clearance with the Orbital Sustainability (ORBITS) Act, it would be a clear case to develop such a Starship variant. Without other incentives, it might still be profitable by selling “deorbiting and clearance services” to other satellite providers. This remains to be seen how much “good citizenship” is to be expected from satellite constructors, launchers and operators.&lt;/p&gt;
&lt;h2 id="references-to-laser-ablation-papers"&gt;References to laser ablation papers&lt;/h2&gt;
&lt;p&gt;Space based -
&lt;a href="https://conference.sdo.esoc.esa.int/proceedings/sdc8/paper/43/SDC8-paper43.pdf"&gt;https://conference.sdo.esoc.esa.int/proceedings/sdc8/paper/43/SDC8-paper43.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ground based -
&lt;a href="https://conference.sdo.esoc.esa.int/proceedings/sdc6/paper/29/SDC6-paper29.pdf"&gt;https://conference.sdo.esoc.esa.int/proceedings/sdc6/paper/29/SDC6-paper29.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Air based Anti ballistic missile chemical laser - a lower powered could be used in Starsjip for de-orbiting, but this shows the feasibility.
&lt;a href="https://minutemanmissile.com/abl.html"&gt;https://minutemanmissile.com/abl.html&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="discussion"&gt;Discussion&lt;/h2&gt;
&lt;p&gt;A &lt;a href="https://www.reddit.com/r/SpaceXLounge/comments/xj8bjh/the_case_for_a_spacex_starship_laser_ablation/"&gt;discussion thread on reddit about this post&lt;/a&gt; has provided some feedback and the post has been updated.&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>We can leave viruses behind on Earth as we leave the gravity well</title>
			<link>http://ewinnington.github.io/posts/Viruses-left-behind</link>
			<description>&lt;p&gt;As humanity left for the stars, bacteria hitched a ride with us. They were on us, in us and around us - as much part of us as our own cells. Viruses, on the other hand, due to careful screening programs, quarantines and selective breeding programs of the few animals that took to the stars, were confined to the gravity well of Earth. Without hosts, they could not follow us. We had left our ancient enemy behind.&lt;/p&gt;</description>
			<guid>http://ewinnington.github.io/posts/Viruses-left-behind</guid>
			<pubDate>Fri, 14 Jan 2022 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;As humanity left for the stars, bacteria hitched a ride with us. They were on us, in us and around us - as much part of us as our own cells. Viruses, on the other hand, due to careful screening programs, quarantines and selective breeding programs of the few animals that took to the stars, were confined to the gravity well of Earth. Without hosts, they could not follow us. We had left our ancient enemy behind.&lt;/p&gt;
&lt;p&gt;Within two generations, the space born growing up on the O’Neill colony cylinders - even those who lived at 0.9 to 1g - realised that earth was going to be a forbidden planet for them. They had no defences against the viruses that continued to permeate the planet. A visit to earth required them to live in an isolation suit and sterile quarters - limiting contact with the earthers and fauna.&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>Checking for liveness on databases for health checks</title>
			<link>http://ewinnington.github.io/posts/db-health</link>
			<description>&lt;p&gt;When you just want to check a DB is reachable from your api or code, a health check is used. For the following DBs the simplest query is:&lt;/p&gt;</description>
			<guid>http://ewinnington.github.io/posts/db-health</guid>
			<pubDate>Tue, 14 Jul 2020 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;When you just want to check a DB is reachable from your api or code, a health check is used. For the following DBs the simplest query is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Oracle: &lt;code&gt;SELECT 1 FROM dual&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Postgresql: &lt;code&gt;SELECT 1&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;SQLite: &lt;code&gt;SELECT 1&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Mysql / MariaDb: &lt;code&gt;SELECT 1&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Microsoft SQL-Server: &lt;code&gt;SELECT 1&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There is really an odd one on the list.&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>The point of Space exploration</title>
			<link>http://ewinnington.github.io/posts/Point-of-exporing-space</link>
			<description>&lt;p&gt;In my view, the point of exploration is hope. For a small slice of humanity, hope in a better world is what drives us.&lt;/p&gt;</description>
			<guid>http://ewinnington.github.io/posts/Point-of-exporing-space</guid>
			<pubDate>Sat, 18 Apr 2020 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;In my view, the point of exploration is hope. For a small slice of humanity, hope in a better world is what drives us.&lt;/p&gt;
&lt;p&gt;By learning about the composition of the moon, setting up ultra precise reflectors on its surface to map its motion and exploring it, we are learning about our solar system and the universe, as well as the challenges we will face in the centuries to come.&lt;/p&gt;
&lt;p&gt;Humanity needs to expand beyond earth to realize how precious earth is. Every person who has been to space has come back profoundly changed and humbled by the experience. A little distance is needed to appreciate the view and realize how lucky we all are.&lt;/p&gt;
&lt;p&gt;The science benefits us all: Solar panels and microwaves to medical innovations. Theory of ecology in closed systems, advanced recycling  and so many more.&lt;/p&gt;
&lt;p&gt;Mars and the moon are but a stepping stone to the solar system. And hopefully to the milky way and beyond. We need to answer the question &amp;quot;Is there life out there?&amp;quot;, or at least I do.&lt;/p&gt;
</content:encoded>
		</item>
	</channel>
</rss>