<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
	<id>http://ewinnington.github.io/</id>
	<title>Eric Winnington</title>
	<link rel="self" href="http://ewinnington.github.io/" />
	<rights>2025</rights>
	<updated>2025-02-15T14:47:03Z</updated>
	<subtitle>A collection of thoughts, code and snippets.</subtitle>
	<entry>
		<id>http://ewinnington.github.io/posts/SimplePerf2</id>
		<title>In-memory software design 2025 - Applied to energy</title>
		<link href="http://ewinnington.github.io/posts/SimplePerf2" />
		<updated>2025-02-15T08:00:00Z</updated>
		<content>&lt;h1 id="in-memory-software-design"&gt;In-memory software design&lt;/h1&gt;
&lt;p&gt;In the last blog, &lt;a href="https://ewinnington.github.io/posts/SimplePerf"&gt;In-memory performance in 2025&lt;/a&gt; we looked at a simple design for an energy aggregation system to aggregate 9.8 GB of 100'000 trades rolled out and saw we got about 40 GB/s performance.&lt;/p&gt;
&lt;p&gt;I upped the numbers of trades to 500'000 trades (rolled out over quarter hours, with average trade length of 120 days in quarter hours) and validated that on a ~50 GB dataset, we are running at ~1200 ms for the complete aggregation, confirming a linear scaling of the compute time.&lt;/p&gt;
&lt;p&gt;But this brings us to a point where on commodity hardware, we are running our aggregations over 1 second in duration. So how can we improve this? This time, instead of trying to brute force, let's bring in some other techniques from databases: Indexes!&lt;/p&gt;
&lt;h2 id="simple-indexes-for-in-memory-aggregations"&gt;Simple indexes for in-memory aggregations&lt;/h2&gt;
&lt;p&gt;For our use case, we are going to look at two meta-data properties on our trades, Trader and Delivery area, but the concept scales efficiently to large collections of meta-data, because we are dealing with such a small amounts of Trades (500k).&lt;/p&gt;
&lt;h3 id="building-the-index"&gt;Building the index&lt;/h3&gt;
&lt;p&gt;We can generate at insertion a set for each meta-data, trader and delivery area.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-cpp"&gt;    // =================
    // 2) Build Indexes
    // =================
    // We'll do single-attribute indexes: trader -&amp;gt; set of tradeIDs, area -&amp;gt; set of tradeIDs
    // For large data, consider using std::vector&amp;lt;size_t&amp;gt; sorted, or some other structure.

    std::unordered_map&amp;lt;std::string, std::unordered_set&amp;lt;size_t&amp;gt;&amp;gt; traderIndex;
    traderIndex.reserve(10);  // if you know approx how many traders you have to avoid resizing continuously

    std::unordered_map&amp;lt;std::string, std::unordered_set&amp;lt;size_t&amp;gt;&amp;gt; areaIndex;
    areaIndex.reserve(10);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I'm using an unordered set, but could also use an ordered set, this might be more efficient.&lt;/p&gt;
&lt;p&gt;As keys, I'm using the string data of the meta-data on the trade - in production this would probably be integer keys, but for this use case is sufficient.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-cpp"&gt;    for (size_t i = 0; i &amp;lt; trades.size(); ++i)
    {
        const auto&amp;amp; t = trades[i];
        traderIndex[t.trader].trades(i);
        areaIndex[t.deliveryArea].trades(i);
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we insert a set of trades, we can add them to the set index.&lt;/p&gt;
&lt;p&gt;As a linear pass, it's extremely efficient to build this index and cheap to keep it updated as we insert new trades.&lt;/p&gt;
&lt;h3 id="using-the-index"&gt;Using the index&lt;/h3&gt;
&lt;p&gt;If we have a search query on a single attribute, we can now use the simple index and have directly the result.&lt;/p&gt;
&lt;p&gt;But if we are doing a query on two (or more), we are going to take use the smallest index first and match with the largest index. The unordered_set gives an acceptable performance for &lt;code&gt;bigger.find(id)&lt;/code&gt; but you can probably do even better with a set structure that is optimized for intersections. You can benchmark using &lt;code&gt;std::set_intersection&lt;/code&gt; against my simple implementation if you are using sorted sets.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-cpp"&gt;// ===================================
    // 3) Use the indexes for filtering
    // ===================================
    // For instance, let's do a query: TraderX, Area2
    // We'll find the intersection of (all trades for TraderX) and (all trades for Area2).

    std::string queryTrader = &amp;quot;TraderX&amp;quot;;
    std::string queryArea   = &amp;quot;Area2&amp;quot;;

    // Get sets from the index
    // (handle the case if the key doesn't exist -&amp;gt; empty set)
    auto itT = traderIndex.find(queryTrader);
    auto itA = areaIndex.find(queryArea);

    if (itT == traderIndex.end() || itA == areaIndex.end()) {
        std::cout &amp;lt;&amp;lt; &amp;quot;No trades found for &amp;quot; &amp;lt;&amp;lt; queryTrader &amp;lt;&amp;lt; &amp;quot; AND &amp;quot; &amp;lt;&amp;lt; queryArea &amp;lt;&amp;lt; &amp;quot;\n&amp;quot;;
        return 0;
    }

    const auto&amp;amp; traderSet = itT-&amp;gt;second;
    const auto&amp;amp; areaSet   = itA-&amp;gt;second;

    // Intersection
    // We'll create a vector of trade IDs that are in both sets
    // For speed, we can iterate over the smaller set and check membership in the larger set.
    const auto&amp;amp; smaller = (traderSet.size() &amp;lt; areaSet.size()) ? traderSet : areaSet;
    const auto&amp;amp; bigger  = (traderSet.size() &amp;lt; areaSet.size()) ? areaSet   : traderSet;

    std::vector&amp;lt;size_t&amp;gt; intersection;
    intersection.reserve(smaller.size());  // a safe upper bound if all the smaller set is selected, to avoid resizing

    for (auto id : smaller)
    {
        if (bigger.find(id) != bigger.end())
        {
            intersection.push_back(id);
        }
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This gives us the intersection and we iterate it to do the summation - again, here we are summing up over the entire year into a single value, but we could just as easily be doing daily, weekly or monthly sums.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-cpp"&gt;    // ====================================
    // 4) Aggregate deliveries for matches
    // ====================================
    // Let's sum up total power / total value for the intersection set.

    long long totalPower = 0;
    long long totalValue = 0;

    #ifdef _OPENMP
    #pragma omp parallel for num_threads(4) reduction(+:totalPower, totalValue)
    #endif
    for (auto id : intersection)
    {
        const Trade&amp;amp; t = trades[id];
        // sum up all deliveries
        for (const auto&amp;amp; dd : t.dailyDeliveries)
        {
            for (int slot = 0; slot &amp;lt; 100; ++slot)
            {
                totalPower += dd.power[slot];
                totalValue += dd.value[slot];
            }
        }
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This addition here is single threaded, but you can also use OpenMP to accelerate it - this will require some tuning, you don't want to use too many threads for these smaller aggregations, the &lt;code&gt;omp parallel for num_threads(4)&lt;/code&gt; can be added to example limit to 4 threads. (note: I added the openMP in the code above).&lt;/p&gt;
&lt;p&gt;Generally you get a 10x or more acceleration in single core, depending on how selective the indexes you are using are- In parallel, I'm getting 40-50x acceleration in multi-core with a num_threads to 4.&lt;/p&gt;
&lt;h2 id="why-arrayint100"&gt;Why array&amp;lt;int,100&amp;gt; ?&lt;/h2&gt;
&lt;p&gt;In my last post, I used an array of 100 points in a day. I'm using it because it's simplest to have all days have the same &amp;quot;size&amp;quot; for memory alignment, therefore if I am calculating days in local time I have a 25h and 23h hour day once a year. I would generally prefer to work in UTC and have constant 24h days -- but for some reason humans prefer local time so it aligns with expectations.&lt;/p&gt;
&lt;p&gt;Just be clear with the developer if you use an internal UTC or Local time representation. If using local make sure to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Properly sanitize your inputs, check trades fill the 96 quarter hours only for all days apart from the short (92) and long day (100) and zero fill the remainder.&lt;/li&gt;
&lt;li&gt;Keep summing on all 100 hours for summations, the 4% extra index length is not worth an if statement in the inner loop of the code - try to keep loops jump free.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="how-to-deal-with-canceled-amended-recalled-trades"&gt;How to deal with Canceled / Amended / Recalled trades ?&lt;/h2&gt;
&lt;p&gt;My first answer is don't! Let me explain: Normal trades should represent the 99.9% or 99.99% of your deals - unless there's something you haven't told me about the way you are trading!&lt;/p&gt;
&lt;p&gt;We can design this by having a tradeStatus on the trade.&lt;/p&gt;
&lt;p&gt;int TradeStatus:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;0 : trade is valid&lt;/li&gt;
&lt;li&gt;1 : trade is canceled&lt;/li&gt;
&lt;li&gt;2 : trade is amended (ie. replaced by a new one)&lt;/li&gt;
&lt;li&gt;... : any other status necessary&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When a trade is canceled, we leave it in the trade vector, but simply set the tradeStatus to a non-zero value, and skip it with a test at the beginning of the aggregation.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-cpp"&gt;    #ifdef _OPENMP
    #pragma omp parallel for num_threads(4) reduction(+:totalPower, totalValue)
    #endif
    for (auto id : intersection)
    {
        const Trade&amp;amp; t = trades[id];
        if(t.tradeStatus != 0) continue;  // skip canceled/amended trades
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If the trade is amended, same thing, we add a new trade to our list of trades and set the previous one to amended status. Generally, the this is not using up much memory. If it ever becomes a problem, we could:&lt;/p&gt;
&lt;ol type="a"&gt;
&lt;li&gt;have a &amp;quot;trade compression&amp;quot; which removes all non-zero trade status from the vector.&lt;/li&gt;
&lt;li&gt;flush the entire trade vector and reload the whole set.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Depending on your implementation, the flushing and reloading might be just as fast - not every program needs to stay resident in memory all the time.&lt;/p&gt;
&lt;h2 id="snapshot-state-to-disk-for-recovery-or-storage-fork-as-in-redis"&gt;Snapshot state to disk for recovery or storage - Fork() as in Redis&lt;/h2&gt;
&lt;p&gt;If we want to take snapshots of the state,  we can get inspired from Redis' famous &lt;a href="https://architecturenotes.co/i/143231289/forking"&gt;fork() snapshotting technique&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Use this when needing to snapshot a large data structure in RAM to disk (serialize the entire state), without blocking our main process from accepting new trades for the entire duration of the write.&lt;/p&gt;
&lt;h3 id="how-fork-helps"&gt;How fork() helps&lt;/h3&gt;
&lt;p&gt;On Linux, calling fork() creates a child process that initially shares the same physical memory pages as the parent.&lt;/p&gt;
&lt;p&gt;Copy-on-write (CoW): If either the parent or the child writes to a page after the fork, the kernel duplicates that page so each process sees consistent data.&lt;/p&gt;
&lt;p&gt;The child process can serialize the in-memory data (in a consistent state from the moment of forking) to disk, while the parent continues to run to accept new trades. New trades arriving in the parent process after the fork will not affect the child’s view of memory. The child effectively sees a snapshot as of the fork().&lt;/p&gt;
&lt;p&gt;You want the data structure to be in a consistent state at the instant of fork().
A brief lock (or pause writes) just before the fork() is triggered, ensuring no partial updates. Immediately after fork() returns, you can unlock, letting the parent continue. Meanwhile, the child proceeds to write out the data.&lt;/p&gt;
&lt;p&gt;We can store an atomic counter value in the program that represents the last tradeid inserted or a state version. This gives you a “version” or “stamp” number for the dataset.&lt;/p&gt;
&lt;p&gt;I won't put the full code for that here, since the design is a little more involved, but the basics are:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;static std::atomic&amp;lt;long&amp;gt; g_version{0}; //snapshot version id 
static std::mutex g_tradeMutex;  // protect g_trades from concurrent modification - lock on write to 

// ---------------------------------------------------
// fork() to create a child that writes the snapshot
// ---------------------------------------------------
int snapshotNow(const char* filename) {
    // 1) Acquire short lock to ensure no partial updates in progress
    g_tradeMutex.lock();
    long snapVer = g_version.load(std::memory_order_relaxed);

    // 2) Fork
    pid_t pid = fork();
    if(pid &amp;lt; 0) {
        // error
        std::cerr &amp;lt;&amp;lt; &amp;quot;fork() failed\n&amp;quot;;
        g_tradeMutex.unlock();
        return -1;
    }

    if(pid == 0) {
        // child
        // We have a consistent view of memory as of the fork.
        // release the lock in the child
        g_tradeMutex.unlock();

        // write the snapshot
        writeSnapshotToDisk(filename, snapVer);

        // exit child
        _exit(0);
    } else {
        // parent
        // release the lock and continue
        g_tradeMutex.unlock();
        std::cout &amp;lt;&amp;lt; &amp;quot;[Parent] Snapshot child pid=&amp;quot; &amp;lt;&amp;lt; pid 
                  &amp;lt;&amp;lt; &amp;quot;, version=&amp;quot; &amp;lt;&amp;lt; snapVer &amp;lt;&amp;lt; &amp;quot;\n&amp;quot;;
        return 0;
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In essence we have two processes continuing from the same command fork() return, each taking one branch.&lt;/p&gt;
&lt;h2 id="data-io"&gt;Data I/O&lt;/h2&gt;
&lt;p&gt;To integrate your cpp aggregation software into the rest of your stack depends on the software running around it.&lt;/p&gt;
&lt;p&gt;You can run the application as an on-demand aggregation, loading everything to memory, doing the aggregation and exiting - leaving the server to do something else - this can be worth it if you only do aggregations on-demand and can afford the load time of a second or two from your NVME storage.&lt;/p&gt;
&lt;p&gt;You can keep the cpp program running either :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;exposing an http RestAPI (&lt;a href="https://github.com/microsoft/cpprestsdk"&gt;https://github.com/microsoft/cpprestsdk&lt;/a&gt; is a good library for that, I've used it before).&lt;/li&gt;
&lt;li&gt;having a GRPC endpoint for performance&lt;/li&gt;
&lt;li&gt;receiving data from a Kafka stream - I'm sure Confluent can give you a good example of that.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="summary"&gt;Summary&lt;/h2&gt;
&lt;p&gt;In-memory data aggregation using cpp is relatively easy to write and maintain. Cpp is no longer the terrible monster it was - auto pointers help and using std:: components makes everything simple. OpenMP is an easy win to add to compute or memory intensive sections.&lt;/p&gt;
&lt;h4 id="sidenotes"&gt;Sidenotes&lt;/h4&gt;
&lt;p&gt;On Windows you can get everything you need to compile cpp by installing:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;winget install LLVM.LLVM
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On Linux (or wsl), you need to install the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo apt-get update
sudo apt-get install clang
sudo apt-get install libomp-dev 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then in both os, you can usually run a compilation on your source file (inmem_agg_omp.cpp) using:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;clang++ -O3 -march=native -flto -ffast-math -fopenmp -o inmem_agg_omp inmem_agg_omp.cpp
&lt;/code&gt;&lt;/pre&gt;
</content>
		<summary>&lt;p&gt;In the last blog, &lt;a href="https://ewinnington.github.io/posts/SimplePerf"&gt;In-memory performance in 2025&lt;/a&gt; we looked at a simple design for an energy aggregation system to aggregate 9.8 GB of 100'000 trades rolled out and saw we got about 40 GB/s performance.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://ewinnington.github.io/posts/SimplePerf</id>
		<title>Performance of in-memory in 2025</title>
		<link href="http://ewinnington.github.io/posts/SimplePerf" />
		<updated>2025-02-13T08:00:00Z</updated>
		<content>&lt;h1 id="performance-of-in-memory-software-in-2025"&gt;Performance of in-memory software in 2025&lt;/h1&gt;
&lt;p&gt;Yesterday, at an Energy panel in Essen, I mentioned that some heavy calculations should be done in-memory. It is something that people have a tendency to dismiss because generally they are not aware of the capability and speed of modern CPUs and RAM. A 32GB dataset in memory can now be processed every second by a commodity cpu in your laptop.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://tailscale.com/blog/living-in-the-future"&gt;Living in the Future, by the numbers&lt;/a&gt; is a great article on the progress we have had since 2004:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CPU Compute is 1000x faster&lt;/li&gt;
&lt;li&gt;Web servers are 100x faster&lt;/li&gt;
&lt;li&gt;Ram is 16x to 750x larger&lt;/li&gt;
&lt;li&gt;SSD can do 10'000x more transactions per second.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can also see this progression on &lt;a href="https://techcommunity.microsoft.com/blog/azurehighperformancecomputingblog/announcing-azure-hbv5-virtual-machines-a-breakthrough-in-memory-bandwidth-for-hp/4303504"&gt;Azure with the high-compute servers&lt;/a&gt;, Microsoft and AMD are packing so much more memory bandwidth in modern compute.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/b7561026-767e-4dc4-97a5-e5306c3fa36a" class="img-fluid" width="60%" alt="image" /&gt;&lt;/p&gt;
&lt;p&gt;We are going to have 7 TB/s of memory bandwidth!&lt;/p&gt;
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/f2c78f96-0eb0-4359-9f65-963b2b4b4f7b" class="img-fluid" width="60%" alt="image" /&gt;&lt;/p&gt;
&lt;p&gt;You can now run super-computer level problems on Azure!&lt;/p&gt;
&lt;p&gt;But what does that all mean? What can we do even on a commodity laptop? I have a Latitude 9440 laptop on my desk here, with a 13th gen i7-1365U with 32 GB of RAM.&lt;/p&gt;
&lt;h2 id="energy-trade-aggregation"&gt;Energy Trade aggregation&lt;/h2&gt;
&lt;p&gt;Let's start with a small calculation from the world of Energy. I have 100'000 trades, these trades affect one or multiple quarter hours of one year (8'784 hours =&amp;gt; 35'136 quarter hours).&lt;/p&gt;
&lt;p&gt;Pulling out a little C++, completely unoptimized, how long does it take to aggregate them and how much RAM is used?&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-cpp"&gt;// A struct to hold the daily delivery arrays (power, value).
struct DailyDelivery
{
    int dayOfYear;  // 1..365
    std::array&amp;lt;int, 100&amp;gt; power; 
    std::array&amp;lt;int, 100&amp;gt; value; 
};

// A struct to hold the trade metadata.
struct Trade
{
    int tradeId;
    std::string trader;        // e.g. &amp;quot;TraderX&amp;quot;
    std::string deliveryArea;  // e.g. &amp;quot;Area1&amp;quot;, &amp;quot;Area2&amp;quot;
    // You could store time points, but we'll just store day indexes for simplicity.
    // Real code might store start_delivery, end_delivery as std::chrono::system_clock::time_point.
    int startDay; // 1..365
    int endDay;   // 1..365

    std::vector&amp;lt;DailyDelivery&amp;gt; dailyDeliveries;
};
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Daily Delivery structure represents a day of delivery, with 100 slots (for the 25h day =&amp;gt; 100 quarter hours).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I'm storing the delivery of power in kW as an int32, meaning in a single trade I can do –2'147'483'648 kW to 2'147'483'647 kW.&lt;/li&gt;
&lt;li&gt;Same thing for the value, we store the individual value of the MW in milli values (decimal shift 3), so each MW could be priced at -2'147'483.648 € to 2'147'483.647 €.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The Trade stores:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;metadata: Trader and DeliveryArea. We could add as many metadata elements as we need, but for simplicity in the demo, I only use this&lt;/li&gt;
&lt;li&gt;a dailyDeliveries vector containing the array of all days affected by the trade.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now if we wanted to see what is the total sum of power of all trades, the sum of TraderX and the sum of TraderX's deals in Area1 and Area2, we can runn the aggregation over all the memory. This is completely straight forward code, no optimizations what so ever.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-cpp"&gt;    // --------------------------------------
    // 2) Run the aggregations (measure time)
    // --------------------------------------

    // The aggregates we want:
    // (a) All Trader total (yearly - all zones)
    // (b) Trader X total
    // (c) Trader X / Area1
    // (d) Trader X / Area2
    //
    // We'll assume we only have TraderX, so &amp;quot;All Trader&amp;quot; == &amp;quot;TraderX&amp;quot; in this simple version.
    //
    // But let's keep it generic. If you had multiple traders, you'd do some checks:
    //
    // For Weighted Average Cost = total_value / total_power (where total_power != 0)

    // We'll measure the time for a single pass that gathers all these sums.

    using Clock = std::chrono::steady_clock;
    auto startTime = Clock::now();

    long long all_totalPower = 0;
    long long all_totalValue = 0;

    long long traderX_totalPower = 0;
    long long traderX_totalValue = 0;

    long long traderX_area1_totalPower = 0;
    long long traderX_area1_totalValue = 0;

    long long traderX_area2_totalPower = 0;
    long long traderX_area2_totalValue = 0;

    for(const auto&amp;amp; trade : trades)
    {
        // (a) &amp;quot;All Trader&amp;quot; sums:
        //    Summation for all trades, all areas, all days
        //    Because this example is all TraderX, you might have to adapt if you had multiple traders
        for(const auto&amp;amp; dd : trade.dailyDeliveries)
        {
            for(int slot = 0; slot &amp;lt; 100; ++slot)
            {
                all_totalPower += dd.power[slot];
                all_totalValue += dd.value[slot];
            }
        }

        // (b) If trade.trader == &amp;quot;TraderX&amp;quot;
        if(trade.trader == &amp;quot;TraderX&amp;quot;)
        {
            for(const auto&amp;amp; dd : trade.dailyDeliveries)
            {
                for(int slot = 0; slot &amp;lt; 100; ++slot)
                {
                    traderX_totalPower += dd.power[slot];
                    traderX_totalValue += dd.value[slot];
                }
            }

            // (c) and (d) by area
            if(trade.deliveryArea == &amp;quot;Area1&amp;quot;)
            {
                for(const auto&amp;amp; dd : trade.dailyDeliveries)
                {
                    for(int slot = 0; slot &amp;lt; 100; ++slot)
                    {
                        traderX_area1_totalPower += dd.power[slot];
                        traderX_area1_totalValue += dd.value[slot];
                    }
                }
            }
            else if(trade.deliveryArea == &amp;quot;Area2&amp;quot;)
            {
                for(const auto&amp;amp; dd : trade.dailyDeliveries)
                {
                    for(int slot = 0; slot &amp;lt; 100; ++slot)
                    {
                        traderX_area2_totalPower += dd.power[slot];
                        traderX_area2_totalValue += dd.value[slot];
                    }
                }
            }
        }
    }

    auto endTime = Clock::now();
    auto durationMs = std::chrono::duration_cast&amp;lt;std::chrono::milliseconds&amp;gt;(endTime - startTime).count();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How long do you think that takes on a commodity laptop? It's 9.8 GB of RAM to scan and fully aggregate. This is also running inside a VM on my Windows WSL instance, with other software running at the same time.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Time for in-memory aggregation: 907 ms
--- Memory usage statistics (approx) ---
Total bytes allocated (cumulative): 9822202136 bytes
Peak bytes allocated (concurrent):  9822202136 bytes
Current bytes allocated:            9822202136 bytes
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="parallelization"&gt;Parallelization&lt;/h2&gt;
&lt;p&gt;Since we are running on a multicore CPU, we can use more than one core to do the aggregation. With OpenMP, it's extremely simple to setup some parallelization for the compute. At the beginning of the loop, we can define a parallel aggregation for reduction, meaning a final sum.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-cpp"&gt;    // Parallel over trades
    // The 'reduction(+: variableList)' tells OpenMP to create private copies of
    // these variables in each thread, accumulate them, and then combine them
    // at the end.
#ifdef _OPENMP
#pragma omp parallel for reduction(+ : all_totalPower, all_totalValue, \
                                       traderX_totalPower, traderX_totalValue, \
                                       traderX_area1_totalPower, traderX_area1_totalValue, \
                                       traderX_area2_totalPower, traderX_area2_totalValue)
#endif
    for (std::size_t i = 0; i &amp;lt; trades.size(); ++i)
    {
        const auto&amp;amp; trade = trades[i];
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With this, we improve the time to aggregate on the laptop to: 241ms. This means we can now do the &lt;strong&gt;complete aggregation on a laptop 4x per second&lt;/strong&gt; - even on a completely unoptimized, simplistic memory structure for trades.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Time for in-memory aggregation: 241 ms
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So when you are doing large numerical aggregations or calculations, ask yourself - can I do this in RAM? If so, you might be surprised at how quickly and efficiently you can do it with modern cpp.&lt;/p&gt;
&lt;h2 id="performance-vs-memory-bandwidth"&gt;Performance vs Memory bandwidth&lt;/h2&gt;
&lt;p&gt;This completely unoptimized implementation is running at :&lt;/p&gt;
&lt;p&gt;Data size (GB) / time (s) = bandwidth (GB/s) =&amp;gt; 9.8 GB / 0.241 s ≈ &lt;strong&gt;40.7 GB/s&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;My laptop has approx ~96 GB/s of memory bandwidth. I calculate it as follows: LPDDR5 at 6000 MT/s, 8 bytes, dual channel = 6000 * 8 * 2 =~ 96 GB/s. My laptop has less than half bandwidth of the HC family on Azure (AMD EPYC™ 7003-series CPU) using CPUs that were released in 2021. Still impressive for my laptop, but it shows you could do much better.&lt;/p&gt;
&lt;p&gt;If I really needed to optimize, I would re-organise the data structures to improve the memory aligment as an initial step. With that, we should get closer to the theoretical bandwidth of the machine.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;Yesterday, at an Energy panel in Essen, I mentioned that some heavy calculations should be done in-memory. It is something that people have a tendency to dismiss because generally they are not aware of the capability and speed of modern CPUs and RAM. A 32GB dataset in memory can now be processed every second by a commodity cpu in your laptop.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://ewinnington.github.io/posts/network-tailscale</id>
		<title>Network with Tailscale</title>
		<link href="http://ewinnington.github.io/posts/network-tailscale" />
		<updated>2025-01-13T10:40:00Z</updated>
		<content>&lt;p&gt;I updated my OpenVPN based network to use Tailscale instead in 2023 and it is game changing. I have used Tailscale ever since. I simply did not update my blog and network diagram.&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/network/network-update.png" class="img-fluid" width="100%" alt="Network" /&gt;&lt;/p&gt;
&lt;p&gt;With &lt;a href="https://tailscale.com/"&gt;Tailscale&lt;/a&gt;, all my machines appear seamlessly on a single control pane and I can reach any of them from any device.&lt;/p&gt;
&lt;h2 id="zone-z"&gt;Zone Z&lt;/h2&gt;
&lt;p&gt;Z has a single fiber connection via Swisscom to internet.&lt;/p&gt;
&lt;h3 id="inventory"&gt;Inventory&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;DeepThread is an AMD Threadripper 1920x running Windows 10.&lt;/li&gt;
&lt;li&gt;Minis4 is the Beelink MiniS12 N95s running Ubuntu Server 24.10.&lt;/li&gt;
&lt;li&gt;NAS is an an older QNAP TS-269L&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="zone-n"&gt;Zone N&lt;/h2&gt;
&lt;p&gt;N has two connections, a Starlink (v1 round) with only the powerbrick router and Sosh as a backup DSL provider (with an ADSL Router) both connected to a Ubiquity UDM-PRO-SE in Failover mode.&lt;/p&gt;
&lt;h3 id="inventory-1"&gt;Inventory&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Minis1 is the Beelink MiniS12 N95s running Windows 11, enjoying it VESA mounted behind a screen in the office currently. I originally thought I would also put Ubuntu, but a windows machine is useful.&lt;/li&gt;
&lt;li&gt;Minis2 and Minis3 are  the Beelink MiniS12 N95s running Ubuntu Server  24.10. Currently rackmounted with the UDM-PRO.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="vpn"&gt;VPN&lt;/h3&gt;
&lt;p&gt;On the UDM-PRO, a VPN is configured with Ubiquity and I can use the iOS application WifiMan to access the network. It's really a backup of a backup solution to have Wifiman.&lt;/p&gt;
&lt;p&gt;On Minis2 and minis4, a &lt;a href="https://github.com/cloudflare/cloudflared"&gt;cloudflared docker&lt;/a&gt; is running, reaching up to Cloudflare and providing an Zero trust tunnel to expose several dockerized websites hosted on it.&lt;/p&gt;
&lt;p&gt;I made a &lt;a href="https://suno.com/song/fb47c594-b22d-4504-83a1-75d8df705194"&gt;Suno song on how awesome&lt;/a&gt; it is.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;I updated my OpenVPN based network to use Tailscale instead in 2023 and it is game changing. I have used Tailscale ever since. I simply did not update my blog and network diagram.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://ewinnington.github.io/posts/2025-predictions</id>
		<title>On the Horizon - 2025 - My predictions</title>
		<link href="http://ewinnington.github.io/posts/2025-predictions" />
		<updated>2025-01-13T00:00:00Z</updated>
		<content>&lt;h1 id="on-the-horizon-2025-my-predictions"&gt;On the Horizon - 2025 - My predictions&lt;/h1&gt;
&lt;p&gt;In a way, predicting 2025 is somewhat harder and easier than 2024, a lot of what I see are the seeds of 2024 coming to bloom. But for what we will have by the end of the year is really unclear to me - but we will see some impact in research for sure - AI assisted research in fields will explode this year.&lt;/p&gt;
&lt;h2 id="predictions"&gt;Predictions:&lt;/h2&gt;
&lt;h3 id="ai-video-ai-audio"&gt;AI Video / AI Audio&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Cinema level visuals made on generative environments&lt;/strong&gt; - not only creating a video, but creating the entire space so that the camera can be then moved in post production - will become available to the high end customers. This approach is a complement to the diffusion models, which only generate a few frames of temporal consistency - using this method will allow much better time coherence and consistency / object permanence. Nvidia Cosmos is closest to this and I think the next version of it will satisfy this point. I expect many video models to actually start using this method with temporal control nets to avoid the inconsistency of object permanence.&lt;/li&gt;
&lt;li&gt;AI Chatbots will be allowed to sing, make music and emote more. While some LLMs are already capable of such things, they are generally removed in post-training but I think these restrictions will be removed this year.  Suno's lead on AI Music gets folded into a leading model, meaning you'll be able to ask a ChatGPT competitor &amp;quot;make me a song, with lyrics and background track&amp;quot;.&lt;/li&gt;
&lt;li&gt;Visual understanding models will be commonly deployed - Meaning point your camera and get full descriptions of what you see, It's nearly there anyway.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="ai-agents"&gt;AI Agents&lt;/h3&gt;
&lt;p&gt;By agent, I define as a &lt;strong&gt;application in which an AI model takes actions against external systems on behalf of a user in furtherance of a user's give task and goal&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A desktop based agent will become available to use on the computer. Interacting with your browser and mail client automatically. (Probably Claude's Anthropic will be there soonest) - doesn't mean the LLM has to run on the desktop.&lt;/li&gt;
&lt;li&gt;AI Agents included in softwares (Teams, Github, ...) will start to become available in preview at least before the end of the year.&lt;/li&gt;
&lt;li&gt;Programming Agents will start to be useful (see AI Devin in 2024 being still completely unusable) - but in 2025 AI coders will be the focus and ship mid year.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="ai-in-mathematics"&gt;AI in Mathematics&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;While the first theorem proof by an LLM has already been published, I expect 2025 to have a slew of progress on fundamental proofs rewritten by LLMs or LRMs, particularly towards automated proofing systems (Coq, ...) and several proofs generated by LLMs that humans did not independently derive.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="ai-in-medicine"&gt;AI in Medicine&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;One company will announce an AI diagnostics companion for health - that is certified as a support tool for doctors.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="ai-in-war"&gt;AI in War&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A semi-autonomous multi-agent AI will be used to control a tactical engagement in Ukraine. This has nearly happened due to the engagement of the first robot brigade (&lt;a href="https://www.forbes.com/sites/davidaxe/2024/12/21/ukraines-first-all-robot-assault-force-just-won-its-first-battle/"&gt;dec 2024 : Ukraine’s All-Robot Assault Force Just Won Its First Battle&lt;/a&gt; ) - but without AI - using tele-operation. I think AI will be used at more levels than just terminal guidance of the FPV drones.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="agi"&gt;AGI&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;One company claims their model has attained &lt;strong&gt;AGI&lt;/strong&gt; - defined as a model that is as good as a reasoning human - &lt;strong&gt;&lt;em&gt;in office work related tasks&lt;/em&gt;&lt;/strong&gt;. There is a lot of disagreement if this constitutes AGI or not.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="robotics"&gt;Robotics:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Humanoid robots become available in limited quantities to the general public, with at least 1 company shipping a home robot with AI onboard to do simple tasks (Unitree from 2024 does not count since the robots are &lt;strong&gt;only&lt;/strong&gt; remote controlled). These first robots will be sometimes teleoperated for specific tasks. Pricing will be lower than 50k$ per robot.&lt;/li&gt;
&lt;li&gt;One company announces wide scale drone deliveries in US cities: While drone delivery companies already exist (&lt;a href="https://builtin.com/articles/drone-delivery-companies"&gt;13 Drone Delivery Companies to Know | Built In&lt;/a&gt;), one of them is going to break out as an early leader this year.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="space"&gt;Space:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;SpaceX demonstrates first ship to ship refueling.&lt;/li&gt;
&lt;li&gt;Blue Origin gets to orbit with New Glenn and proves the landing system, but is not able to send a reused first stage to orbit yet.&lt;/li&gt;
&lt;li&gt;The first part of a new space station gets deployed, probably commercial.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="environment"&gt;Environment:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;2025 beats 2024 as hottest year.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="energy"&gt;Energy:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Record installation of Solar, Wind and renewables (beating 2024 worldwide - despite the US's drop).&lt;/li&gt;
&lt;li&gt;Price spikes on Gas and Petrol due to US actions and disruption of Russian production. Unsure if that will continue throughout the year but we will have shocks from policy changes.&lt;/li&gt;
&lt;/ul&gt;
</content>
		<summary>&lt;p&gt;In a way, predicting 2025 is somewhat harder and easier than 2024, a lot of what I see are the seeds of 2024 coming to bloom. But for what we will have by the end of the year is really unclear to me - but we will see some impact in research for sure - AI assisted research in fields will explode this year.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://ewinnington.github.io/posts/Disposable-Software</id>
		<title>Disposable software</title>
		<link href="http://ewinnington.github.io/posts/Disposable-Software" />
		<updated>2024-12-09T00:00:00Z</updated>
		<content>&lt;h1 id="the-era-of-throw-away-software-is-upon-us"&gt;The era of throw away software is upon us&lt;/h1&gt;
&lt;p&gt;With the advent of LLMs and their capability to create quick programs (&amp;quot;create me a flashcard app&amp;quot;, &amp;quot;I need a typing exercise software&amp;quot;, &amp;quot;make a dashboard to track my investments&amp;quot;), we might see a lot more software being written, used and then discarded since it's trivial for a LLM to re-write it next time it is needed.&lt;/p&gt;
&lt;p&gt;Where perfection is not required, just good enough, there will be a whole slew of applications, websites and programs that are used and put into production that are never even reviewed by a human programmer, just tested for their outputs and/or visually checked by a human. Even, we might see lots of code that is only ever read by a machine for bugs and issues, to then be corrected by a machine.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;With the advent of LLMs and their capability to create quick programs ("create me a flashcard app", "I need a typing exercise software", "make a dashboard to track my investments"), we might see a lot more software being written, used and then discarded since it's trivial for a LLM to re-write it next time it is needed.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://ewinnington.github.io/posts/Audit-Trail-Oracle</id>
		<title>Using an audit trail table on Oracle</title>
		<link href="http://ewinnington.github.io/posts/Audit-Trail-Oracle" />
		<updated>2024-10-05T08:00:00Z</updated>
		<content>&lt;h1 id="implementing-auditable-updates-in-a-relational-database"&gt;Implementing Auditable Updates in a Relational Database&lt;/h1&gt;
&lt;p&gt;In modern applications, maintaining an audit trail of changes to data is crucial for compliance, debugging, and data integrity. This blog post explores a straightforward approach to implementing auditable updates in a relational database system, specifically focusing on a project management scenario with hierarchical data.&lt;/p&gt;
&lt;h2 id="problem-description"&gt;Problem Description&lt;/h2&gt;
&lt;p&gt;We have a relational database containing &lt;code&gt;Projects&lt;/code&gt;, each of which includes &lt;code&gt;Instruments&lt;/code&gt;, &lt;code&gt;Markets&lt;/code&gt;, and &lt;code&gt;Valuations&lt;/code&gt;. These entities form a tree structure, adhering to the third normal form (3NF). Previously, any update to a project involved downloading the entire project tree, making changes, and uploading a new project under a new ID to ensure complete auditability.&lt;/p&gt;
&lt;p&gt;This approach is inefficient for small updates and doesn't allow for granular tracking of changes. The goal is to enable small, precise updates to projects while maintaining a comprehensive audit trail of all changes.&lt;/p&gt;
&lt;h2 id="solution-overview"&gt;Solution Overview&lt;/h2&gt;
&lt;p&gt;We introduce an audit table that records every change made to the database. The audit table will store serialized JSON representations of operations like &lt;code&gt;update&lt;/code&gt;, &lt;code&gt;insert&lt;/code&gt;, and &lt;code&gt;delete&lt;/code&gt;. We'll also provide C# code to apply and revert these changes, effectively creating an undo stack.&lt;/p&gt;
&lt;p&gt;Let's use the following DB Schema for illustration:&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/audit-trail/TableStructureBlog.png" class="img-fluid" width="60%" alt="TableSchema" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Primary Keys: Each table has a primary key (e.g., &lt;code&gt;ProjectID&lt;/code&gt;, &lt;code&gt;InstrumentID&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Foreign Keys: Child tables reference their parent via foreign keys (e.g., &lt;code&gt;instruments.ProjectID&lt;/code&gt; references &lt;code&gt;projects.ProjectID&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Audit Table: The &lt;code&gt;change_audit&lt;/code&gt; table records changes with fields like &lt;code&gt;ChangeAuditID&lt;/code&gt;, &lt;code&gt;TimeApplied&lt;/code&gt;, and &lt;code&gt;ImpactJson&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- 
```d2
projects: {
  shape: sql_table
  ProjectID: int {constraint: primary_key}
  Name: varchar(100)
  Description: text
  LastUpdated: timestamp with time zone
  VersionNumber: int
}

instruments: {
  shape: sql_table
  InstrumentID: int {constraint: primary_key}
  ProjectID: int {constraint: foreign_key}
  Name: varchar(100)
  Type: varchar(50)
  LastUpdated: timestamp with time zone
}

markets: {
  shape: sql_table
  MarketID: int {constraint: primary_key}
  ProjectID: int {constraint: foreign_key}
  Region: varchar(50)
  MarketType: varchar(50)
  LastUpdated: timestamp with time zone
}

valuations: {
  shape: sql_table
  ValuationID: int {constraint: primary_key}
  ProjectID: int {constraint: foreign_key}
  Value: decimal(10, 2)
  Currency: varchar(10)
  LastUpdated: timestamp with time zone
}

change_audit: {
  shape: sql_table
  ChangeAuditID: int {constraint: primary_key}
  TimeApplied: timestamp with time zone
  UserID: varchar(100)
  ImpactJson: jsonb
}

instruments.ProjectID -&gt; projects.ProjectID
markets.ProjectID -&gt; projects.ProjectID
valuations.ProjectID -&gt; projects.ProjectID

```
--&gt;
&lt;h2 id="implementing-change-auditing"&gt;Implementing Change Auditing&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;change_audit&lt;/code&gt; table is designed to store all changes in a JSON format for flexibility and ease of storage.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;CREATE TABLE change_audit (
  ChangeAuditID   NUMBER PRIMARY KEY,
  TimeApplied     TIMESTAMP,
  UserID          VARCHAR2(100),
  ImpactJson      CLOB
);
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="json-structure-for-changes"&gt;JSON Structure for Changes&lt;/h2&gt;
&lt;p&gt;Each change is recorded as a JSON object:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
  &amp;quot;Operation&amp;quot;: &amp;quot;update&amp;quot;,
  &amp;quot;impact&amp;quot;: [
    {
      &amp;quot;Table&amp;quot;: &amp;quot;Instruments&amp;quot;,
      &amp;quot;PrimaryKey&amp;quot;: {&amp;quot;ProjectID&amp;quot;: 4, &amp;quot;InstrumentID&amp;quot;: 2},
      &amp;quot;Column&amp;quot;: &amp;quot;Name&amp;quot;,
      &amp;quot;OldValue&amp;quot;: &amp;quot;Old Instrument Name&amp;quot;,
      &amp;quot;NewValue&amp;quot;: &amp;quot;Updated Instrument Name&amp;quot;
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="csharp-to-apply-changes-given-an-operation"&gt;CSharp to apply changes given an operation&lt;/h2&gt;
&lt;p&gt;To apply changes recorded in the JSON, we'll use C# code that parses the JSON and executes the corresponding SQL commands.&lt;/p&gt;
&lt;p&gt;I assume you have the &lt;code&gt;_connectionString&lt;/code&gt; available somewhere as a constant in the code.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;using Oracle.ManagedDataAccess.Client;
using Newtonsoft.Json.Linq;
using System;
using System.Collections.Generic;

public class ChangeApplier
{

    public void ApplyChanges(string jsonInput)
    {
        // Parse the JSON input
        var operation = JObject.Parse(jsonInput);
        string opType = operation[&amp;quot;Operation&amp;quot;].ToString();
        var impactList = (JArray)operation[&amp;quot;impact&amp;quot;];

        using (var conn = new OracleConnection(_connectionString))
        {
            conn.Open();
            using (var transaction = conn.BeginTransaction())
            {
                try
                {
                    foreach (var impact in impactList)
                    {
                        string table = impact[&amp;quot;Table&amp;quot;].ToString();
                        var primaryKey = (JObject)impact[&amp;quot;PrimaryKey&amp;quot;];
                        string column = impact[&amp;quot;Column&amp;quot;]?.ToString();
                        string newValue = impact[&amp;quot;NewValue&amp;quot;]?.ToString();

                        switch (opType)
                        {
                            case &amp;quot;update&amp;quot;:
                                ApplyUpdate(conn, table, primaryKey, column, newValue);
                                break;
                            case &amp;quot;insert&amp;quot;:
                                ApplyInsert(conn, table, impact);
                                break;
                            case &amp;quot;delete&amp;quot;:
                                ApplyDelete(conn, table, primaryKey);
                                break;
                        }
                    }

                    transaction.Commit();
                }
                catch (Exception ex)
                {
                    transaction.Rollback();
                    Console.WriteLine($&amp;quot;Error applying changes: {ex.Message}&amp;quot;);
                }
            }
        }
    }

    private void ApplyUpdate(OracleConnection conn, string table, JObject primaryKey, string column, string newValue)
    {
        var pkConditions = BuildPrimaryKeyCondition(primaryKey);
        var query = $&amp;quot;UPDATE {table} SET {column} = :newValue WHERE {pkConditions}&amp;quot;;

        using (var cmd = new OracleCommand(query, conn))
        {
            cmd.Parameters.Add(new OracleParameter(&amp;quot;newValue&amp;quot;, newValue));
            cmd.ExecuteNonQuery();
        }
    }

    private void ApplyInsert(OracleConnection conn, string table, JToken impact)
    {
        var primaryKey = (JObject)impact[&amp;quot;PrimaryKey&amp;quot;];
        var newValues = (JObject)impact[&amp;quot;NewValues&amp;quot;];
        var columns = new List&amp;lt;string&amp;gt;();
        var values = new List&amp;lt;string&amp;gt;();

        foreach (var property in primaryKey.Properties())
        {
            columns.Add(property.Name);
            values.Add($&amp;quot;:{property.Name}&amp;quot;);
        }

        foreach (var property in newValues.Properties())
        {
            columns.Add(property.Name);
            values.Add($&amp;quot;:{property.Name}&amp;quot;);
        }

        var query = $&amp;quot;INSERT INTO {table} ({string.Join(&amp;quot;, &amp;quot;, columns)}) VALUES ({string.Join(&amp;quot;, &amp;quot;, values)})&amp;quot;;

        using (var cmd = new OracleCommand(query, conn))
        {
            foreach (var property in primaryKey.Properties())
            {
                cmd.Parameters.Add(new OracleParameter(property.Name, property.Value.ToString()));
            }

            foreach (var property in newValues.Properties())
            {
                cmd.Parameters.Add(new OracleParameter(property.Name, property.Value.ToString()));
            }

            cmd.ExecuteNonQuery();
        }
    }

    private void ApplyDelete(OracleConnection conn, string table, JObject primaryKey)
    {
        var pkConditions = BuildPrimaryKeyCondition(primaryKey);
        var query = $&amp;quot;DELETE FROM {table} WHERE {pkConditions}&amp;quot;;

        using (var cmd = new OracleCommand(query, conn))
        {
            cmd.ExecuteNonQuery();
        }
    }

    private string BuildPrimaryKeyCondition(JObject primaryKey)
    {
        var conditions = new List&amp;lt;string&amp;gt;();
        foreach (var prop in primaryKey.Properties())
        {
            conditions.Add($&amp;quot;{prop.Name} = :{prop.Name}&amp;quot;);
        }
        return string.Join(&amp;quot; AND &amp;quot;, conditions);
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ApplyChanges&lt;/strong&gt;: Parses the JSON input and determines the operation type.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ApplyUpdate&lt;/strong&gt;: Executes an UPDATE SQL command using parameters to prevent SQL injection.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ApplyInsert&lt;/strong&gt;: Executes an INSERT SQL command, constructing columns and values from the JSON.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ApplyDelete&lt;/strong&gt;: Executes a DELETE SQL command based on the primary key.
BuildPrimaryKeyCondition: Constructs the WHERE clause for SQL commands.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A side note, for the insert, you'll have the challenge if you are using auto-incremented IDs, this will mean you don't know the new IDs until you have inserted the data, so you should make sure to capture the new IDs and then create the audit log. This is left as a simple exercise to the reader in case it is necessary.&lt;/p&gt;
&lt;h2 id="csharp-to-revert-changes"&gt;CSharp to revert changes&lt;/h2&gt;
&lt;p&gt;To revert changes (undo operations), we'll process the audit trail in reverse order. Here I give the processing of a list of operations as an example of unrolling. It is to note that the reverse delete does only one table, so if there was some connected information that was deleted via referential identity, it was the task of the audit table to keep that in the audit.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public class ChangeReverter
{
    public void RevertChanges(List&amp;lt;string&amp;gt; jsonOperations)
    {
        using (var conn = new OracleConnection(_connectionString))
        {
            conn.Open();
            using (var transaction = conn.BeginTransaction())
            {
                try
                {
                    jsonOperations.Reverse(); // note: you could also have provided sorted by last time from the audit table instead of reversing them

                    foreach (var operationJson in jsonOperations)
                    {
                        var operation = JObject.Parse(operationJson);
                        string opType = operation[&amp;quot;Operation&amp;quot;].ToString();
                        var impactList = (JArray)operation[&amp;quot;impact&amp;quot;];

                        foreach (var impact in impactList)
                        {
                            string table = impact[&amp;quot;Table&amp;quot;].ToString();
                            var primaryKey = (JObject)impact[&amp;quot;PrimaryKey&amp;quot;];
                            string column = impact[&amp;quot;Column&amp;quot;]?.ToString();
                            string oldValue = impact[&amp;quot;OldValue&amp;quot;]?.ToString();

                            switch (opType)
                            {
                                case &amp;quot;update&amp;quot;:
                                    RevertUpdate(conn, table, primaryKey, column, oldValue);
                                    break;
                                case &amp;quot;insert&amp;quot;:
                                    ApplyDelete(conn, table, primaryKey);
                                    break;
                                case &amp;quot;delete&amp;quot;:
                                    RevertDelete(conn, table, impact);
                                    break;
                            }
                        }
                    }

                    transaction.Commit();
                }
                catch (Exception ex)
                {
                    transaction.Rollback();
                    Console.WriteLine($&amp;quot;Error reverting changes: {ex.Message}&amp;quot;);
                }
            }
        }
    }

    private void RevertUpdate(OracleConnection conn, string table, JObject primaryKey, string column, string oldValue)
    {
        var pkConditions = BuildPrimaryKeyCondition(primaryKey);
        var query = $&amp;quot;UPDATE {table} SET {column} = :oldValue WHERE {pkConditions}&amp;quot;;

        using (var cmd = new OracleCommand(query, conn))
        {
            cmd.Parameters.Add(new OracleParameter(&amp;quot;oldValue&amp;quot;, oldValue));
            cmd.ExecuteNonQuery();
        }
    }

    private void RevertDelete(OracleConnection conn, string table, JToken impact)
    {
        var primaryKey = (JObject)impact[&amp;quot;PrimaryKey&amp;quot;];
        var oldValues = (JObject)impact[&amp;quot;OldValues&amp;quot;];
        var columns = new List&amp;lt;string&amp;gt;();
        var values = new List&amp;lt;string&amp;gt;();

        foreach (var property in primaryKey.Properties())
        {
            columns.Add(property.Name);
            values.Add($&amp;quot;:{property.Name}&amp;quot;);
        }

        foreach (var property in oldValues.Properties())
        {
            columns.Add(property.Name);
            values.Add($&amp;quot;:{property.Name}&amp;quot;);
        }

        var query = $&amp;quot;INSERT INTO {table} ({string.Join(&amp;quot;, &amp;quot;, columns)}) VALUES ({string.Join(&amp;quot;, &amp;quot;, values)})&amp;quot;;

        using (var cmd = new OracleCommand(query, conn))
        {
            foreach (var property in primaryKey.Properties())
            {
                cmd.Parameters.Add(new OracleParameter(property.Name, property.Value.ToString()));
            }

            foreach (var property in oldValues.Properties())
            {
                cmd.Parameters.Add(new OracleParameter(property.Name, property.Value.ToString()));
            }

            cmd.ExecuteNonQuery();
        }
    }

    // Reuse BuildPrimaryKeyCondition and ApplyDelete methods from ChangeApplier
}
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;RevertChanges&lt;/strong&gt;: Processes the list of JSON operations in reverse order to undo changes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RevertUpdate&lt;/strong&gt;: Sets the column back to its old value.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RevertDelete&lt;/strong&gt;: Re-inserts a deleted row using the old values stored in the audit trail.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ApplyDelete&lt;/strong&gt;: Deletes a row, used here to undo an insert operation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="json-schema"&gt;JSON schema&lt;/h2&gt;
&lt;p&gt;The reason that I prefer to use the Json directly in the C# code is that actually making up the C# classes for this schema is actually more work that processing the json directly in the code.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
  &amp;quot;$schema&amp;quot;: &amp;quot;http://json-schema.org/draft-07/schema#&amp;quot;,
  &amp;quot;title&amp;quot;: &amp;quot;ImpactJsonRoot&amp;quot;,
  &amp;quot;type&amp;quot;: &amp;quot;object&amp;quot;,
  &amp;quot;properties&amp;quot;: {
    &amp;quot;Operation&amp;quot;: {
      &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;,
      &amp;quot;enum&amp;quot;: [&amp;quot;update&amp;quot;, &amp;quot;insert&amp;quot;, &amp;quot;delete&amp;quot;],
      &amp;quot;description&amp;quot;: &amp;quot;Type of operation&amp;quot;
    },
    &amp;quot;Impact&amp;quot;: {
      &amp;quot;type&amp;quot;: &amp;quot;array&amp;quot;,
      &amp;quot;items&amp;quot;: {
        &amp;quot;type&amp;quot;: &amp;quot;object&amp;quot;,
        &amp;quot;properties&amp;quot;: {
          &amp;quot;Table&amp;quot;: {
            &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;,
            &amp;quot;description&amp;quot;: &amp;quot;Name of the table affected&amp;quot;
          },
          &amp;quot;PrimaryKey&amp;quot;: {
            &amp;quot;type&amp;quot;: &amp;quot;object&amp;quot;,
            &amp;quot;description&amp;quot;: &amp;quot;Primary key fields and their values&amp;quot;,
            &amp;quot;additionalProperties&amp;quot;: {
              &amp;quot;type&amp;quot;: [&amp;quot;number&amp;quot;, &amp;quot;null&amp;quot;]
            }
          },
          &amp;quot;Column&amp;quot;: {
            &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;,
            &amp;quot;description&amp;quot;: &amp;quot;Column affected (for updates)&amp;quot;
          },
          &amp;quot;OldValue&amp;quot;: {
            &amp;quot;type&amp;quot;: [&amp;quot;string&amp;quot;, &amp;quot;number&amp;quot;, &amp;quot;boolean&amp;quot;, &amp;quot;null&amp;quot;],
            &amp;quot;description&amp;quot;: &amp;quot;Previous value (for updates and deletes)&amp;quot;
          },
          &amp;quot;NewValue&amp;quot;: {
            &amp;quot;type&amp;quot;: [&amp;quot;string&amp;quot;, &amp;quot;number&amp;quot;, &amp;quot;boolean&amp;quot;, &amp;quot;null&amp;quot;],
            &amp;quot;description&amp;quot;: &amp;quot;New value (for updates and inserts)&amp;quot;
          },
          &amp;quot;OldValues&amp;quot;: {
            &amp;quot;type&amp;quot;: &amp;quot;object&amp;quot;,
            &amp;quot;description&amp;quot;: &amp;quot;All old values (for deletes)&amp;quot;,
            &amp;quot;additionalProperties&amp;quot;: {
              &amp;quot;type&amp;quot;: [&amp;quot;string&amp;quot;, &amp;quot;number&amp;quot;, &amp;quot;boolean&amp;quot;, &amp;quot;null&amp;quot;]
            }
          },
          &amp;quot;NewValues&amp;quot;: {
            &amp;quot;type&amp;quot;: &amp;quot;object&amp;quot;,
            &amp;quot;description&amp;quot;: &amp;quot;All new values (for inserts)&amp;quot;,
            &amp;quot;additionalProperties&amp;quot;: {
              &amp;quot;type&amp;quot;: [&amp;quot;string&amp;quot;, &amp;quot;number&amp;quot;, &amp;quot;boolean&amp;quot;, &amp;quot;null&amp;quot;]
            }
          }
        },
        &amp;quot;required&amp;quot;: [&amp;quot;Table&amp;quot;, &amp;quot;PrimaryKey&amp;quot;]
      }
    }
  },
  &amp;quot;required&amp;quot;: [&amp;quot;Operation&amp;quot;, &amp;quot;Impact&amp;quot;]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and here are examples of operations:&lt;/p&gt;
&lt;h3 id="update"&gt;update&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
  &amp;quot;Operation&amp;quot;: &amp;quot;update&amp;quot;,
  &amp;quot;Impact&amp;quot;: [
    {
      &amp;quot;Table&amp;quot;: &amp;quot;Instruments&amp;quot;,
      &amp;quot;PrimaryKey&amp;quot;: { &amp;quot;ProjectID&amp;quot;: 4, &amp;quot;InstrumentID&amp;quot;: 2 },
      &amp;quot;Column&amp;quot;: &amp;quot;Name&amp;quot;,
      &amp;quot;OldValue&amp;quot;: &amp;quot;Old Instrument Name&amp;quot;,
      &amp;quot;NewValue&amp;quot;: &amp;quot;Updated Instrument Name&amp;quot;
    }
  ]
}

&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="insert"&gt;insert&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
  &amp;quot;Operation&amp;quot;: &amp;quot;insert&amp;quot;,
  &amp;quot;Impact&amp;quot;: [
    {
      &amp;quot;Table&amp;quot;: &amp;quot;Instruments&amp;quot;,
      &amp;quot;PrimaryKey&amp;quot;: { &amp;quot;ProjectID&amp;quot;: 4, &amp;quot;InstrumentID&amp;quot;: 10 },
      &amp;quot;NewValues&amp;quot;: {
        &amp;quot;Name&amp;quot;: &amp;quot;New Instrument&amp;quot;,
        &amp;quot;Type&amp;quot;: &amp;quot;Flexible Asset&amp;quot;,
        &amp;quot;LastUpdated&amp;quot;: &amp;quot;2024-10-05T12:34:56Z&amp;quot;
      }
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="delete"&gt;delete&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
  &amp;quot;Operation&amp;quot;: &amp;quot;delete&amp;quot;,
  &amp;quot;Impact&amp;quot;: [
    {
      &amp;quot;Table&amp;quot;: &amp;quot;Instruments&amp;quot;,
      &amp;quot;PrimaryKey&amp;quot;: { &amp;quot;ProjectID&amp;quot;: 4, &amp;quot;InstrumentID&amp;quot;: 5 },
      &amp;quot;OldValues&amp;quot;: {
        &amp;quot;Name&amp;quot;: &amp;quot;Obsolete Instrument&amp;quot;,
        &amp;quot;Type&amp;quot;: &amp;quot;Flexible Asset&amp;quot;,
        &amp;quot;LastUpdated&amp;quot;: &amp;quot;2024-10-01T09:15:00Z&amp;quot;
      }
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note: OpenAI's &lt;code&gt;o1-preview&lt;/code&gt; was used to assist in the creation of the post.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;In modern applications, maintaining an audit trail of changes to data is crucial for compliance, debugging, and data integrity. This blog post explores a straightforward approach to implementing auditable updates in a relational database system, specifically focusing on a project management scenario with hierarchical data.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://ewinnington.github.io/posts/snippets-in-vscode</id>
		<title>VSCode Snippets</title>
		<link href="http://ewinnington.github.io/posts/snippets-in-vscode" />
		<updated>2024-07-18T11:00:00Z</updated>
		<content>&lt;p&gt;I use Visual Studio Code to write markdown notes of my meetings. I datetime every meeting to be able to find it afterwards. One of the shorthands I use is 'dt'. I always forget how to set it up when I reinstall my vscode, so here's the instructions from github copilot chat, which explained how to set it up again!&lt;/p&gt;
&lt;p&gt;So now I know I can ask Copilot how to do it, but I still want to document it, so here it is:&lt;/p&gt;
&lt;h3 id="github-copilot-instructions"&gt;Github Copilot Instructions&lt;/h3&gt;
&lt;p&gt;To recreate the shorthand command in Visual Studio Code for inserting the current date and time in a Markdown file using &lt;code&gt;ctrl-space&lt;/code&gt;, you can use a snippet. Here's how to set it up:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Open the Command Palette with &lt;code&gt;Ctrl+Shift+P&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Type &lt;code&gt;Configure User Snippets&lt;/code&gt; and select it.&lt;/li&gt;
&lt;li&gt;Choose &lt;code&gt;markdown.json&lt;/code&gt; for Markdown files.&lt;/li&gt;
&lt;li&gt;Insert the following snippet configuration:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;&amp;quot;Insert Current Date and Time&amp;quot;: {
    &amp;quot;prefix&amp;quot;: &amp;quot;dt&amp;quot;,
    &amp;quot;body&amp;quot;: [
        &amp;quot;### $CURRENT_YEAR.$CURRENT_MONTH.$CURRENT_DATE $CURRENT_HOUR:$CURRENT_MINUTE:$CURRENT_SECOND&amp;quot;
    ],
    &amp;quot;description&amp;quot;: &amp;quot;Inserts the current date and time&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start="5"&gt;
&lt;li&gt;Save the &lt;code&gt;markdown.json&lt;/code&gt; file.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now, when you type &lt;code&gt;dt&lt;/code&gt; in a Markdown file and press &lt;code&gt;Ctrl+Space&lt;/code&gt;, it should suggest your snippet. Selecting it will insert the current date and time formatted as you described.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;I use Visual Studio Code to write markdown notes of my meetings. I datetime every meeting to be able to find it afterwards. One of the shorthands I use is 'dt'. I always forget how to set it up when I reinstall my vscode, so here's the instructions from github copilot chat, which explained how to set it up again!&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://ewinnington.github.io/posts/HowToUpdateWritebook</id>
		<title>Adding LaTeX Maths to Writebook</title>
		<link href="http://ewinnington.github.io/posts/HowToUpdateWritebook" />
		<updated>2024-07-13T15:00:00Z</updated>
		<content>&lt;h1 id="how-to-add-latex-maths-to-writebook"&gt;How to add LaTeX maths to Writebook&lt;/h1&gt;
&lt;p&gt;I've started using ONCE &lt;a href="https://once.com/writebook"&gt;Writebook&lt;/a&gt; to host some markdown documents, including my book on Optimization applied to energy products. But that book contains tons of Markdown LaTeX formatted mathematics, which Writebook does not support at this time.&lt;/p&gt;
&lt;p&gt;So I patched support for it into the docker image.&lt;/p&gt;
&lt;h2 id="copy-out-the-app-layout-file"&gt;Copy out the app layout file&lt;/h2&gt;
&lt;p&gt;Assuming your docker image is named writebook.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo docker cp writebook:/rails/app/views/layouts/application.html.erb .
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="modify-the-application-erb-to-add-support"&gt;Modify the application erb to add support&lt;/h2&gt;
&lt;p&gt;In the Head Section add&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; &amp;lt;script type=&amp;quot;text/javascript&amp;quot;&amp;gt;
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$']],
          displayMath: [['$$', '$$']],
          processEscapes: true
        }
      };
    &amp;lt;/script&amp;gt;
      
    &amp;lt;script src=&amp;quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thing is, it doesn't render the first time round, so I have to refresh the page to render, but it's not too much of an issue right now, I actually like to see the LaTeX maths code before I see the rendered version.&lt;/p&gt;
&lt;!--
Not working right now ! 

## How to add a render on end of page load to ensure it typesets first time

Above the ```&lt;/Body&gt;``` tag at the bottom of the page, add
```
    &lt;script type="text/javascript"&gt;
    document.addEventListener("DOMContentLoaded", function() {
      MathJax.typeset();
    });
  &lt;/script&gt;
```

--&gt;
&lt;h2 id="save-and-copy-the-file-back"&gt;Save and copy the file back&lt;/h2&gt;
&lt;p&gt;Then finally copy the file back into the docker image and restart the image&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo docker cp application.html.erb writebook:/rails/app/views/layouts/application.html.erb
sudo docker restart writebook
&lt;/code&gt;&lt;/pre&gt;
</content>
		<summary>&lt;p&gt;I've started using ONCE &lt;a href="https://once.com/writebook"&gt;Writebook&lt;/a&gt; to host some markdown documents, including my book on Optimization applied to energy products. But that book contains tons of Markdown LaTeX formatted mathematics, which Writebook does not support at this time.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://ewinnington.github.io/posts/HttpClientCompression</id>
		<title>Receiving compressed data from an http(s) endpoint</title>
		<link href="http://ewinnington.github.io/posts/HttpClientCompression" />
		<updated>2024-03-20T11:00:00Z</updated>
		<content>&lt;p&gt;With the amount of data that we are passing around across services, it is often beneficial to use compression on the data to reduce the transmission time. Modern platforms and algorithms are now very efficient at compressing regular data, particularly if that data is text or json data. &lt;/p&gt;
&lt;p&gt;If the developer of the endpoint has prepared their service for compression, the client must still indicate that they are ready to receive the compressed data. Luckily, most implementations of modern http clients in R, Python, JavaScript and Dotnet support compression / decompression and are seamless for the client. This means that you can set the compression headers on and simply benefit from compressed data being received. &lt;/p&gt;
&lt;p&gt;We can also check in the Content-Encoding header which compression was used. I've found that example.com is sending responses compressed with gzip.&lt;/p&gt;
&lt;h2 id="python"&gt;Python&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import requests

url = &amp;quot;http://example.com&amp;quot;  # Replace with the actual URL you want to request

# Specify the accepted encoding methods in the headers
headers = {
    'Accept-Encoding': 'gzip, br',
}

response = requests.get(url, headers=headers)
print(response.text)

# In case you want to see if it was compressed, you can check via the headers
#if 'Content-Encoding' in response.headers:
#    print(f&amp;quot;Response was compressed using: {response.headers['Content-Encoding']}&amp;quot;)
#else:
#    print(&amp;quot;Response was not compressed.&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="r"&gt;R&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-R"&gt;library(httr)

# The URL to which you're sending the request
url &amp;lt;- &amp;quot;http://example.com&amp;quot;

# Setting the Accept-Encoding header
response &amp;lt;- GET(url, add_headers(`Accept-Encoding` = 'gzip, br'))

# The content of the response will be automatically decompressed by httr, so you can access it directly.
content(response, &amp;quot;text&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="c"&gt;C#&lt;/h2&gt;
&lt;p&gt;In C#, for some ungodly strange reason, the standard HTTP endpoint doesn't decompress for you automatically unless you add a decompression handler - see handler &lt;a href="https://learn.microsoft.com/en-us/dotnet/api/system.net.http.httpclienthandler.automaticdecompression?view=net-8.0#system-net-http-httpclienthandler-automaticdecompression"&gt;HttpClientHandler.AutomaticDecompression&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-Csharp"&gt;using System;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Threading.Tasks;
using System.Text;

class Program
{
    static async Task Main(string[] args)
    {
        HttpClientHandler handler = new HttpClientHandler();
        handler.AutomaticDecompression = System.Net.DecompressionMethods.GZip; //Adding automatic Decompression means that the accept headers are added automatically

        using (var client = new HttpClient(handler))
        {
            string url = &amp;quot;http://example.com&amp;quot;;
            HttpResponseMessage response = await client.GetAsync(url);
            response.EnsureSuccessStatusCode();

            Console.WriteLine(await response.Content.ReadAsStringAsync());
		}
	}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="javascript"&gt;JavaScript&lt;/h2&gt;
&lt;p&gt;it is so easy that you don't even need to do anything else than setting the gzip: true for the support&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-JS"&gt;const request = require('request');
const requestOptions = {
  url: 'http://example.com',
  gzip: true, // This is all that is required
};
request(requestOptions, (error, response, body) =&amp;gt; {
  // Handle the response here
});
&lt;/code&gt;&lt;/pre&gt;
</content>
		<summary>&lt;p&gt;With the amount of data that we are passing around across services, it is often beneficial to use compression on the data to reduce the transmission time. Modern platforms and algorithms are now very efficient at compressing regular data, particularly if that data is text or json data.&amp;nbsp;&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://ewinnington.github.io/posts/devcontainers</id>
		<title>DevContainers - The future of developer environments</title>
		<link href="http://ewinnington.github.io/posts/devcontainers" />
		<updated>2023-07-24T00:00:00Z</updated>
		<content>&lt;h2 id="history"&gt;History&lt;/h2&gt;
&lt;p&gt;It's been years now that we've had Infrastructure as Code (IaC), Containers and Desired state Configuration (DsC) tools to do our deployments. But these have been mostly focused on the deployment side of things, with fewer tools on the developer side. On the dev machine, installing and maintaining the development tools and package dependencies has been in flux, both in windows where finally tools like Ninite, Chocolatey and Winget allow management of dev tools, and on the linux side, which was always quite well served with apt - but has also gained Snap, Flatpack and other package management tools. The thing is, sometimes you need more that one version of a particular tool, Python3.10 and Python3.11, Java9 and Java17, Dotnet 4.8 and Dotnet 6, to work on the various projects you have during the day. Sometimes, they work side by side very well and sometimes they don't. And when they don't, it can be a long process to figure out why and also very difficult to get help without resorting to having a clean image refresh and starting again to install your dependencies.&lt;/p&gt;
&lt;p&gt;Since the end of the 2010s and the early 2020s, with the rise of web hosted IDEs, there has been a need to define ways to have a base image that contained the environment and tools needed to work. I remember running some in the mid 2010s - Nitrous.IO (2013-16) - that allowed you to use a base container and configure it to do remote development.&lt;/p&gt;
&lt;h2 id="devcontainers"&gt;DevContainers&lt;/h2&gt;
&lt;p&gt;With the arrival of Docker on every desktop, Github's Cloudspaces and Visual Studio Code, there's been a new interest in this type of desired state environments with developer tooling. Microsoft published the &lt;a href="https://containers.dev/"&gt;DevContainer specification&lt;/a&gt; in early 2022 to formalize the language.&lt;/p&gt;
&lt;p&gt;So how does it help us? Well, with a DevContainer, we can setup a new development environment on Premise (in VSCode), on the cloud VM (Azure+VM) or on a Codespace environment with a single file that ensures that we always have the tools we want and need installed. Starting to work is as easy as openining the connection and cloning the repo we need if the .devcontainer file is located inside.&lt;/p&gt;
&lt;h2 id="devcontainer-example"&gt;DevContainer example&lt;/h2&gt;
&lt;p&gt;You can find below my &lt;a href="https://github.com/ewinnington/DevContainerTemplate/blob/master/.devcontainer/devcontainer.json"&gt;personal DevContainer&lt;/a&gt;, it is setup with Git, Node, AzureCLI, Docker control of hose, Dotnet, Terraform, Java with Maven, Python3 and Postgresql. I also have the VSCode extensions directly configured so I can directly start using them when I connect. I also use the &amp;quot;postStartCommand&amp;quot;: &amp;quot;nohup bash -c 'postgres &amp;amp;'&amp;quot; to run an instance of Postgresql directly inside the development container, so I can a directly have a DB to run requests against. And yes, this is a bit of a kitchen sink DevContainer, they can be smaller and more tailored to a project with only one or two of these features included, but here I use a generic one add added everything I use apart from the c++ and fortran compilers.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{
    &amp;quot;name&amp;quot;: &amp;quot;Erics-base-dev-container&amp;quot;,
    &amp;quot;image&amp;quot;: &amp;quot;mcr.microsoft.com/devcontainers/base:debian&amp;quot;,
 
    &amp;quot;features&amp;quot;: {
        &amp;quot;ghcr.io/devcontainers/features/git:1&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/node:1&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/azure-cli:1&amp;quot;: {}, //azure-cli,
        &amp;quot;ghcr.io/devcontainers/features/docker-outside-of-docker:1&amp;quot;: {}, //docker on host
        &amp;quot;ghcr.io/devcontainers/features/dotnet:1&amp;quot;: {}, //dotnet installed
        &amp;quot;ghcr.io/devcontainers/features/terraform:1&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/java:1&amp;quot;: { &amp;quot;installMaven&amp;quot; : true },
        &amp;quot;ghcr.io/devcontainers-contrib/features/postgres-asdf:1&amp;quot;: {}
    },
 
    // Configure tool-specific properties.
    &amp;quot;customizations&amp;quot;: {
        // Configure properties specific to VS Code.
        &amp;quot;vscode&amp;quot;: {
            &amp;quot;settings&amp;quot;: {},
            &amp;quot;extensions&amp;quot;: [
                &amp;quot;streetsidesoftware.code-spell-checker&amp;quot;,
                &amp;quot;ms-azuretools.vscode-docker&amp;quot;,
                &amp;quot;ms-dotnettools.csharp&amp;quot;,
                &amp;quot;HashiCorp.terraform&amp;quot;,
                &amp;quot;ms-azuretools.vscode-azureterraform&amp;quot;,
                &amp;quot;GitHub.copilot&amp;quot;,
                &amp;quot;GitHub.copilot-chat&amp;quot;,
                &amp;quot;vscjava.vscode-java-pack&amp;quot;,
                &amp;quot;ms-python.python&amp;quot;
            ]
        }
    },
 
    // Use 'forwardPorts' to make a list of ports inside the container available locally.
    // &amp;quot;forwardPorts&amp;quot;: [3000],
 
    // Use 'portsAttributes' to set default properties for specific forwarded ports.
    // More info: https://containers.dev/implementors/json_reference/#port-attributes
    &amp;quot;portsAttributes&amp;quot;: {
        &amp;quot;3000&amp;quot;: {
            &amp;quot;label&amp;quot;: &amp;quot;Hello Remote World&amp;quot;,
            &amp;quot;onAutoForward&amp;quot;: &amp;quot;notify&amp;quot;
        }
    },
 
    // Use 'postCreateCommand' to run commands after the container is created.
    &amp;quot;postCreateCommand&amp;quot;: &amp;quot;&amp;quot;,
 
    &amp;quot;postStartCommand&amp;quot;: &amp;quot;nohup bash -c 'postgres &amp;amp;'&amp;quot;
 
    // Uncomment to connect as root instead. More info: https://aka.ms/dev-containers-non-root.
    // &amp;quot;remoteUser&amp;quot;: &amp;quot;root&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="so-how-do-you-start-with-devcontainers"&gt;So how do you start with DevContainers?&lt;/h2&gt;
&lt;p&gt;There are 2 easy ways:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;(remote) Github Codespaces
By going to my repo, you can click &amp;quot;Create Codespace on Master&amp;quot; and get a running VSCode in the cloud with all those tools setup instantly.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;(at first build, the image might take time)&lt;/p&gt;
&lt;ol start="2"&gt;
&lt;li&gt;(local) Docker + VS Code
Ensure you have the &lt;a href="https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers"&gt;ms-vscode-remote.remote-containers&lt;/a&gt; extension installed in VS Code and Docker installed.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Clone the repo &lt;a href="https://github.com/ewinnington/DevContainerTemplate.git"&gt;https://github.com/ewinnington/DevContainerTemplate.git&lt;/a&gt;, then open it with VSCode. It should automatically detect the .devContainer and offer to rebuild the container image and open it up in the IDE for you.&lt;/p&gt;
&lt;p&gt;Once that is done, you should have access to a complete environment at the state you specified.&lt;/p&gt;
&lt;h2 id="whats-the-use-for-developers-at-corporations-where-computers-are-locked-down"&gt;What's the use for Developers at corporations where computers are locked down?&lt;/h2&gt;
&lt;p&gt;I think that providing developer windows machine with Git, Docker, WSL2 installed and using VS Code or another IDE that supports DevContainers is an excellent way forwards in providing a good fast and stable environment for developers to work faster and more efficiently. Using this configuration, any person showing up to a Hackathon would be able to start working in minutes after cloning a repository. It would really simplify daily operations, since every repo can provide the correct .DevContainer configuration, or teams can share a DevContainer basic configuration.&lt;/p&gt;
&lt;p&gt;This all simplifies operations, makes developer experience more consistent and increases productivity since you can move faster from one development environment to another in minutes. OnPrem → Remote VM → Cloudspace and back in minutes, without any friction.&lt;/p&gt;
&lt;p&gt;All in all, I'm convinced it is a tool that both IT support must understand and master how to best provide access to, and for developers to understand the devContainer to benefit from it.&lt;/p&gt;
&lt;p&gt;Have you used DevContainers? What is your experience?&lt;/p&gt;
</content>
		<summary>&lt;p&gt;It's been years now that we've had Infrastructure as Code (IaC), Containers and Desired state Configuration (DsC) tools to do our deployments. But these have been mostly focused on the deployment side of things, with fewer tools on the developer side. On the dev machine, installing and maintaining the development tools and package dependencies has been in flux, both in windows where finally tools like Ninite, Chocolatey and Winget allow management of dev tools, and on the linux side, which was always quite well served with apt - but has also gained Snap, Flatpack and other package management tools. The thing is, sometimes you need more that one version of a particular tool, Python3.10 and Python3.11, Java9 and Java17, Dotnet 4.8 and Dotnet 6, to work on the various projects you have during the day. Sometimes, they work side by side very well and sometimes they don't. And when they don't, it can be a long process to figure out why and also very difficult to get help without resorting to having a clean image refresh and starting again to install your dependencies.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://ewinnington.github.io/posts/Proxmox</id>
		<title>Proxmox 8 on sub $200 mini PCs</title>
		<link href="http://ewinnington.github.io/posts/Proxmox" />
		<updated>2023-07-01T22:00:00Z</updated>
		<content>&lt;h1 id="installing-proxmox-tailscale-win11-vms-and-automation"&gt;Installing Proxmox, Tailscale, Win11 VMs and Automation&lt;/h1&gt;
&lt;p&gt;This is a Beelink MiniS12 with an Intel N95s.&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/minipc.jpg" class="img-fluid" width="80%" alt="" /&gt;
(coffee cup for scale)&lt;/p&gt;
&lt;p&gt;Up until Proxmox 8 dropped about a week ago, I was unable to install Proxmox due to compatibility with the graphics driver. Now in 8, that was fixed, so I've been able to install Proxmox on several of my machines.&lt;/p&gt;
&lt;p&gt;The procedure is trivial: write the proxmox iso to a usb key via a software that will make the iso bootable. Boot the machine with the usb key inserted and select the correct boot drive, then follow the proxmox installation prompts.&lt;/p&gt;
&lt;p&gt;It all worked out of the box.&lt;/p&gt;
&lt;h2 id="clustering"&gt;Clustering&lt;/h2&gt;
&lt;p&gt;I was able to connect to my proxmox installed machine on the port :8006 via a browser on my home network. Next step was enabling the management of multiple machines via a single UI. So as soon as I had two machines with Proxmox installer, I was able to go to my primary, click on Create cluster, confirm. Get the Join token, connect to the socond machine and paste the join token into the &amp;quot;Join Cluster&amp;quot;. Worked out of the box.&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/proxmox/proxmox-clustering.png" class="img-fluid" width="80%" alt="" /&gt;&lt;/p&gt;
&lt;h2 id="removing-the-update-repositories-to-work-on-the-free-version-of-proxmox"&gt;Removing the Update Repositories to work on the Free version of Proxmox&lt;/h2&gt;
&lt;p&gt;To stay within the Free licensing of Proxmox and be able to do apt-get, remember to go for each machine and to remove the non-free repos in the repository list.&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/proxmox/proxmox-remove-repos" class="img-fluid" width="80%" alt="" /&gt;&lt;/p&gt;
&lt;h2 id="installing-tailscale"&gt;Installing Tailscale&lt;/h2&gt;
&lt;p&gt;I use &lt;a href="https://tailscale.com/"&gt;Tailscale&lt;/a&gt; at home to connect across multiple locations and roaming devices. Every time I add a Tailscale device, I am amazed at how easy it is.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -fsSL https://tailscale.com/install.sh | sh
tailscale up
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Two lines, one URL to visit and the machines were enrolled.&lt;/p&gt;
&lt;h2 id="vms-and-lxcs"&gt;VMs and LXCs&lt;/h2&gt;
&lt;p&gt;To create VMs and LXCs, you need to add iso images or Templates to your local storage:&lt;/p&gt;
&lt;p&gt;Container templates, you can create your own and upload them or simply click on &amp;quot;Templates&amp;quot; and get a couple of ready made ones for use in your containers.&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/proxmox/container-templates.png" class="img-fluid" width="80%" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;You can now get the official windows ISO from the microsoft website. So download it to your local machine and then Upload it to the ISO images.&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/proxmox/windows-iso.png" class="img-fluid" width="80%" alt="" /&gt;&lt;/p&gt;
&lt;h3 id="installing-windows-11-pro-and-activating-with-the-hardware-license"&gt;Installing Windows 11 Pro and activating with the Hardware license&lt;/h3&gt;
&lt;p&gt;I had previously logged in and linked to my microsoft account on the windows 11 pro licensed version of the OS for each of the machines. This meant that when installing the Windows11 Pro from the ISO onto a VM running on the machines, I was able to activate the machine by referring to the previous activation. #Windows11Pro allows itself to be reactivated with the license that came with the hardware, inside a proxmox8 VM of the same machine as long as you pass the host-cpu - or so it seems to me.&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/proxmox/activated.jpg" class="img-fluid" width="80%" alt="Proxmox-shell-mode-issue" /&gt;&lt;/p&gt;
&lt;h2 id="issues-i-had-and-solutions"&gt;Issues I had and solutions&lt;/h2&gt;
&lt;h3 id="console-not-connecting-to-lxc-containers"&gt;Console not connecting to LXC containers&lt;/h3&gt;
&lt;p&gt;Several times, either while connecting to a container in Proxmox directly, or after a containter migation, I was not able to use the integrated shell. I therefore had to change the Container &amp;quot;Options-&amp;gt;Console mode&amp;quot; to &amp;quot;shell&amp;quot; to make it connect every time.
&lt;img src="/posts/images/proxmox/Proxmox-Shell-mode.png" class="img-fluid" width="80%" alt="Proxmox-shell-mode-issue" /&gt;&lt;/p&gt;
&lt;h3 id="apt-get-issue-in-proxmox-containers"&gt;apt-get issue in proxmox containers&lt;/h3&gt;
&lt;p&gt;The first thing I do upon entering a container is nearly always apt-get update. And sometimes it breaks. I couldn't update or install. Here is my checklist:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Check you gave and ip address to the container in the &amp;quot;Network&amp;quot; section, either a dhcp or a static address.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Check your DNS servers: I had not noticed that after installing Tailscale, my DNS records were only pointing to the talescale DNS resolver. Adding back google (8.8.8.8) and cloudflare (1.1.1.1) to my proxmox hosts helped.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src="/posts/images/proxmox/Tailscale-dns-issue-proxmox.png" class="img-fluid" width="80%" alt="Tailscale-dns-issue-proxmox" /&gt;&lt;/p&gt;
&lt;p&gt;By fixing both of these I was able to get the apt-get running correctly.&lt;/p&gt;
&lt;h2 id="automation"&gt;Automation&lt;/h2&gt;
&lt;h3 id="installing-on-a-client-machine-the-proxmox-cli-tools"&gt;Installing on a client machine the Proxmox CLI Tools&lt;/h3&gt;
&lt;p&gt;I'm planning on checking out automation of deployment on proxmox. Making a note here of the command line installation of the tools&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo pip3 install pve-cli
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I'll also look into Terraform + Ansible for a proxmox deployment, or the Packer LXC to make container templates, but that is for next time.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;This is a Beelink MiniS12 with an Intel N95s.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://ewinnington.github.io/posts/network-vpn</id>
		<title>Network planning and VPN</title>
		<link href="http://ewinnington.github.io/posts/network-vpn" />
		<updated>2023-04-20T18:40:00Z</updated>
		<content>&lt;p&gt;I am in the process of setting up my homelab network between my two locations.&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/network/network.png" class="img-fluid" width="100%" alt="Network" /&gt;&lt;/p&gt;
&lt;h2 id="zone-z"&gt;Zone Z&lt;/h2&gt;
&lt;p&gt;Z has a single fiber connection via Swisscom to internet.&lt;/p&gt;
&lt;h3 id="inventory"&gt;Inventory&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;DeepThread is an AMD Threadripper 1920x running Windows 10.&lt;/li&gt;
&lt;li&gt;Minis3 and Minis4 are the Beelink MiniS12 N95s running Ubuntu Server 23.04.&lt;/li&gt;
&lt;li&gt;NAS is an an older QNAP TS-269L&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="vpn"&gt;VPN&lt;/h3&gt;
&lt;p&gt;With a L2TP VPN connection configured to allow remote access onto the network, so I can get Red (Surface Laptop) and Xr (iPhone) onto the network in case.&lt;/p&gt;
&lt;h2 id="zone-n"&gt;Zone N&lt;/h2&gt;
&lt;p&gt;N has two connections, a Starlink (v1 round) with only the powerbrick router and Sosh as a backup DSL provider (with an ADSL Router) both connected to a Ubiquity UDM-PRO-SE in Failover mode. Getting a VPN to N is a little more involved, since the UDM is behind a separate router on each WAN.&lt;/p&gt;
&lt;h3 id="inventory-1"&gt;Inventory&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Minis1 is the Beelink MiniS12 N95s running Windows 11. Planned to Switch to Ubuntu 23.04, but enjoying it VESA mounted behind a screen in the office currently.&lt;/li&gt;
&lt;li&gt;Minis2 is the Beelink MiniS12 N95s running Ubuntu Server 23.04. Currently rackmounted with the UDM-PRO.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="vpn-1"&gt;VPN&lt;/h3&gt;
&lt;p&gt;On the UDM-PRO, a VPN is configured with Ubiquity and I can use the iOS application WifiMan to access the network.
On Minis2, a &lt;a href="https://github.com/cloudflare/cloudflared"&gt;cloudflared docker&lt;/a&gt; is running, reaching up to Cloudflare and providing an Zero trust tunnel to expose several dockerized websites hosted on it.&lt;/p&gt;
&lt;h1 id="the-issue-at-hand"&gt;The issue at hand&lt;/h1&gt;
&lt;p&gt;I would like the N Minis1 &amp;amp; Minis2 to be able to access the Z NAS, ideally with a relatively simple connection that I can leave running all the time, to be able to pull files from the NAS and ideally also access the NAS's front-end application from inside my N location. I could connect to the SwisscomVPN every time I do something that requires connectivity to the NAS, but I would really ideally like a more permanent solution where I make the Z NAS &amp;quot;visible&amp;quot; in the N network. Or go full and establish a site-to-site VPN and simply make the two areas N and Z communicate seamlessly while still having local connectivity.&lt;/p&gt;
&lt;p&gt;Do you have any suggestions as to how best to accomplish this?&lt;/p&gt;
&lt;h2 id="section"&gt;22.04.2023&lt;/h2&gt;
&lt;p&gt;I now put a OpenVPN on the Qnap NAS to act as a S2S VPN. Not sure that will be the solution I keep for the long term but it works for now.&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/network/network2.drawio.svg" class="img-fluid" width="100%" alt="Network" /&gt;&lt;/p&gt;
</content>
		<summary>&lt;p&gt;I am in the process of setting up my homelab network between my two locations.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://ewinnington.github.io/posts/mini-pc</id>
		<title>The era of the sub $200 PC</title>
		<link href="http://ewinnington.github.io/posts/mini-pc" />
		<updated>2023-04-05T00:00:00Z</updated>
		<content>&lt;p&gt;I recently purchased (arrived yesterday!) two mini PCs, for 170.- CHF each (~$187). They sport the latest low power CPU from Intel, the N95, which is amazingly efficient, powerful and cheap.&lt;/p&gt;
&lt;p&gt;The Beelink Mini S12 with the N95 intel CPU 4c, 16 GB Ram, 512 GB NVME storage drive, with space for an additional 2.5&amp;quot; SSD inside too. Amazingly small and light to carry around.&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/minipc.jpg" class="img-fluid" width="80%" alt="" /&gt;
(coffee cup for scale)&lt;/p&gt;
&lt;p&gt;I just got two of these to put in a Kubernetes cluster, and I've been playing around with them and they are super impressive. They pack a punch and sip power at 9-20W. The internal video card does 4k youtube playback without any issue.&lt;/p&gt;
&lt;p&gt;Windows 11 comes included with the machine for the price. I've already put ubuntu server on one of them. These machines are powerful enough for family members to do all their online activities, zoom (with a webcam), or use as a media machine.&lt;/p&gt;
&lt;p&gt;Over the next weeks, I'll be documenting configuring them and setting them up to serve as highly available servers from my home, using a reverse VPN tunneling.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;I recently purchased (arrived yesterday!) two mini PCs, for 170.- CHF each (~$187). They sport the latest low power CPU from Intel, the N95, which is amazingly efficient, powerful and cheap.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://ewinnington.github.io/posts/learn-from-chatgpt-Crdt-OT</id>
		<title>Learning concepts from chatGPT - Operational Transform and Conflict-free Replicated Data Types</title>
		<link href="http://ewinnington.github.io/posts/learn-from-chatgpt-Crdt-OT" />
		<updated>2022-12-11T00:20:00Z</updated>
		<content>&lt;p&gt;I have been enjoying using &lt;a href="https://chat.openai.com/chat"&gt;chatGPT&lt;/a&gt; as a private tutor / reference on topics. My biggest issue is now keeping up with the knowledge and asking the right questions.&lt;/p&gt;
&lt;p&gt;But, remember that chatGPT is not all knowing and can get things wrong - even fabulating, so you have to double check the information. I did my best to validate the information with &lt;a href="https://mwhittaker.github.io/consistency_in_distributed_systems/3_crdt.html"&gt;Conflict-Free Replicated Data Types&lt;/a&gt;, &lt;a href="https://svn.apache.org/repos/asf/incubator/wave/whitepapers/operational-transform/operational-transform.html"&gt;Google's OT papers&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Operational_transformation"&gt;Wikipedia on Operational Transformation&lt;/a&gt;. I did not find any glaring errors in the text. For future crawlers, &lt;strong&gt;the following text was generated by chatGPT on 2022-12-11T01:12:45+0000&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;bold&lt;/strong&gt; is used for the prompts I send into chatGPT.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;italics&lt;/em&gt; is for my comments.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;can you explain CRDT's in data structures?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/gpt/gpt-logo.png" class="img-fluid" width="5%" alt="gpt-logo" /&gt;
Sure! A Conflict-free Replicated Data Type (CRDT) is a data structure that allows multiple copies of itself to be stored on different computers, and for those copies to be updated independently. This means that even when the copies are not connected to each other, they can still be updated and changed, and when they are eventually reconnected, they will automatically merge any conflicting changes in a way that preserves the consistency of the data. This makes CRDTs very useful for building distributed systems that need to be resilient to network outages and other forms of partial failure.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Is this similar to Google Wave's Operational Transform?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/gpt/gpt-logo.png" class="img-fluid" width="5%" alt="gpt-logo" /&gt;
Yes, Operational Transform (OT) is a technique that is related to CRDTs, and it is often used to solve the same kinds of problems. Like CRDTs, OT is a way of allowing multiple copies of a data structure to be updated independently and then merged together, but it uses a different approach to achieve this. OT works by applying a series of &amp;quot;operations&amp;quot; to the data, rather than directly modifying the data itself. This allows OT to detect and resolve conflicts between different copies of the data more easily, but it can also make it more complex to implement.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Can you give me an example of a CRDT design?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/gpt/gpt-logo.png" class="img-fluid" width="5%" alt="gpt-logo" /&gt;
Sure! One example of a CRDT is a &amp;quot;g-counter,&amp;quot; which is a data structure that stores a count. This data structure allows for the count to be incremented independently on different computers, and then merged together in a way that ensures that the final count is always correct. Here is one way that a g-counter could be implemented:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;class GCounter {
  Map&amp;lt;String, Integer&amp;gt; counts;

  void increment(String nodeId) {
    counts.put(nodeId, counts.getOrDefault(nodeId, 0) + 1);
  }

  void merge(GCounter other) {
    for (Map.Entry&amp;lt;String, Integer&amp;gt; entry : other.counts.entrySet()) {
      String nodeId = entry.getKey();
      int count = entry.getValue();
      counts.put(nodeId, Math.max(counts.getOrDefault(nodeId, 0), count));
    }
  }

  int getCount() {
    return counts.values().stream().mapToInt(x -&amp;gt; x).sum();
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;I never specified a programming language, so assume this is a kind of pseudocode&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In this implementation, the increment() method is used to increment the count on a particular node, and the merge() method is used to merge the counts from two different g-counters together. The getCount() method returns the current value of the count.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;And contrast with the design of an OT?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/gpt/gpt-logo.png" class="img-fluid" width="5%" alt="gpt-logo" /&gt;
Here is an example of how an Operational Transform (OT) might be implemented for a simple text editing application:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;class TextBuffer {
  StringBuilder text;

  void applyOperation(Operation op) {
    int start = op.getStartIndex();
    int end = op.getEndIndex();
    String newText = op.getText();
    text.replace(start, end, newText);
  }

  String getText() {
    return text.toString();
  }
}

class Operation {
  int startIndex;
  int endIndex;
  String text;

  int getStartIndex() {
    return startIndex;
  }

  int getEndIndex() {
    return endIndex;
  }

  String getText() {
    return text;
  }

  Operation transform(Operation other) {
    // Compute the transformed version of this operation, given the other operation
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;I never specified a programming language, so assume this is a kind of pseudocode&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In this example, the TextBuffer class represents the text being edited, and the Operation class represents a single edit operation (e.g. inserting or deleting some text). The applyOperation() method is used to apply an operation to the text buffer, and the getText() method is used to retrieve the current contents of the buffer. The transform() method is used to compute the transformed version of an operation, given another operation. This is used to ensure that concurrent operations on the same text buffer can be merged together consistently.&lt;/p&gt;
&lt;p&gt;One key difference between this approach and the g-counter example is that OT explicitly tracks the operations that are applied to the data, rather than directly modifying the data itself. This makes it easier to detect and resolve conflicts between concurrent operations, but it also makes the implementation more complex.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Can you list known CRDTs and their usage in a table?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/gpt/gpt-logo.png" class="img-fluid" width="5%" alt="gpt-logo" /&gt;
Here is a list of some common Conflict-free Replicated Data Types (CRDTs), along with a brief description of each one and some potential use cases:&lt;/p&gt;
&lt;table class="table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;CRDT&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Use Cases&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;G-Counter&lt;/td&gt;
&lt;td&gt;Stores a count that can be incremented independently on different nodes, and then merged together consistently.&lt;/td&gt;
&lt;td&gt;- Versioning systems that need to keep track of the number of times a file has been modified.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;PN-Counter&lt;/td&gt;
&lt;td&gt;Stores a count that can be incremented or decremented independently on different nodes, and then merged together consistently.&lt;/td&gt;
&lt;td&gt;- Tracking the number of likes or dislikes on a social media post.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Two-Phase Set&lt;/td&gt;
&lt;td&gt;Stores a set of unique values that can be added or removed independently on different nodes, and then merged together consistently.&lt;/td&gt;
&lt;td&gt;- Collaborative editing of a list of items (e.g. a to-do list).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;OR-Set&lt;/td&gt;
&lt;td&gt;Stores a set of values that can be added independently on different nodes, and then merged together consistently.&lt;/td&gt;
&lt;td&gt;- Storing the list of friends that a user has on a social network.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;LWW-Element-Set&lt;/td&gt;
&lt;td&gt;Stores a set of values, where each value has a timestamp associated with it. The value with the most recent timestamp wins in case of conflicts.&lt;/td&gt;
&lt;td&gt;- Storing the latest version of a user's profile on a social network.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Observed-Remove Set&lt;/td&gt;
&lt;td&gt;Stores a set of values that can be added or removed independently on different nodes, and then merged together consistently.&lt;/td&gt;
&lt;td&gt;- Storing the list of items that a user has added to their shopping cart on an e-commerce website.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;These are just a few examples of CRDTs, and there are many other types of CRDTs that have been developed for different types of data and applications. In general, CRDTs can be useful whenever you need to store and update data in a distributed system, and you want to ensure that the data remains consistent even in the face of network partitions and other forms of partial failure.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Can you make a similar table for OTs?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/gpt/gpt-logo.png" class="img-fluid" width="5%" alt="gpt-logo" /&gt;
Sure! Here is a table that lists some common Operational Transform (OT) techniques, along with a brief description of each one and some potential use cases:&lt;/p&gt;
&lt;table class="table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;OT Technique&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Use Cases&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Insert-Delete OT&lt;/td&gt;
&lt;td&gt;Allows for the insertion and deletion of text, and can detect and resolve conflicts between concurrent insertions and deletions.&lt;/td&gt;
&lt;td&gt;- Collaborative text editing in a real-time editor (e.g. Google Docs).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Block-Based OT&lt;/td&gt;
&lt;td&gt;Allows for the insertion, deletion, and movement of blocks of text, and can detect and resolve conflicts between concurrent insertions, deletions, and movements.&lt;/td&gt;
&lt;td&gt;- Collaborative editing of structured documents (e.g. a spreadsheet or a presentation).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Change-Based OT&lt;/td&gt;
&lt;td&gt;Allows for arbitrary changes to be made to a document, and can detect and resolve conflicts between concurrent changes.&lt;/td&gt;
&lt;td&gt;- Collaborative editing of free-form documents (e.g. a word processor or a code editor).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;State-Based OT&lt;/td&gt;
&lt;td&gt;Stores the complete state of the document at each point in time, and can detect and resolve conflicts between concurrent changes by comparing the state of the document before and after each change.&lt;/td&gt;
&lt;td&gt;- Collaborative editing of complex data structures (e.g. a database or a network topology diagram).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Hybrid OT&lt;/td&gt;
&lt;td&gt;Combines multiple OT techniques in order to support a wider range of operations and conflicts.&lt;/td&gt;
&lt;td&gt;- Collaborative editing of documents that contain both structured and unstructured data (e.g. a document that contains both text and images).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Composable OT&lt;/td&gt;
&lt;td&gt;Allows for OTs to be composed together in order to create more complex data structures and operations.&lt;/td&gt;
&lt;td&gt;- Building collaborative editing systems that support multiple types of data and operations (e.g. a system that supports text, images, and geometry).&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Like CRDTs, there are many different OT techniques that have been developed for different types of data and applications, and this table only covers a few of the most common ones. In general, OTs can be useful whenever you need to support real-time collaboration on data, and you want to ensure that concurrent changes can be merged together consistently.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;is there a use case to use both CRDTs and OTs together?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/gpt/gpt-logo.png" class="img-fluid" width="5%" alt="gpt-logo" /&gt;
It is possible to use Conflict-free Replicated Data Types (CRDTs) and Operational Transform (OT) techniques together in some cases, although this is not always necessary or desirable. Here are a few examples of situations where using both CRDTs and OTs together might be useful:&lt;/p&gt;
&lt;p&gt;If you have a data structure that is very complex and cannot be easily represented using a single CRDT, you could use a combination of CRDTs and OTs to break the data structure down into smaller, simpler pieces that can be managed independently. For example, you could use a CRDT to store the overall structure of the data, and then use OTs to handle the details of how the individual pieces of the data are updated and changed.&lt;/p&gt;
&lt;p&gt;If you have a data structure that is updated very frequently, and you need to ensure that updates can be applied and merged together quickly, you might use a CRDT to handle the &amp;quot;big picture&amp;quot; view of the data, and then use OTs to handle the finer-grained details of how the data is changed. This can help to reduce the amount of work that needs to be done in order to merge updates together, which can improve the overall performance of the system.&lt;/p&gt;
&lt;p&gt;If you have a data structure that needs to support both offline and online updates, you might use a CRDT to handle the offline updates, and then use OTs to handle the online updates. This can allow the system to continue functioning even when the network is unavailable, and then automatically merge any offline updates with the online updates when the network becomes available again.&lt;/p&gt;
&lt;p&gt;Overall, whether or not it makes sense to use CRDTs and OTs together in a particular system will depend on the specific requirements and constraints of the system, as well as the trade-offs between the different approaches.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Are there any other similar types of data structures that also cover this space?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/gpt/gpt-logo.png" class="img-fluid" width="5%" alt="gpt-logo" /&gt;
There are many different types of data structures and algorithms that can be used to solve similar problems to those addressed by Conflict-free Replicated Data Types (CRDTs) and Operational Transform (OT) techniques. Some examples of other approaches that are related to CRDTs and OTs include:&lt;/p&gt;
&lt;p&gt;Replication protocols: These are algorithms that are used to manage the process of replicating data between different nodes in a distributed system. Replication protocols can be used to ensure that multiple copies of the data are kept consistent with each other, and that conflicts between different copies of the data are resolved in a way that preserves the integrity of the data.&lt;/p&gt;
&lt;p&gt;Consensus algorithms: These are algorithms that are used to reach agreement on the state of a distributed system, even in the presence of faulty or malicious nodes. Consensus algorithms can be used to ensure that different nodes in the system have a consistent view of the data, and that updates to the data are applied in a way that is consistent with the consensus.&lt;/p&gt;
&lt;p&gt;Distributed hash tables (DHTs): These are data structures that are used to store and retrieve data in a distributed system. DHTs use a hash function to map data to different nodes in the system, and they can be used to ensure that data is stored and retrieved in a consistent and efficient manner.&lt;/p&gt;
&lt;p&gt;Distributed databases: These are databases that are designed to be used in a distributed system, and that provide mechanisms for storing, querying, and updating data in a way that is consistent and scalable. Distributed databases can use a variety of techniques, including CRDTs and OTs, to ensure that the data remains consistent and available even in the face of network partitions and other forms of partial failure.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Give me a example of a consensus algorithm that computes the final state of a text document that is edited in realtime in multiple distributed computers. I'm looking at a peer to peer network without central authority.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/gpt/gpt-logo.png" class="img-fluid" width="5%" alt="gpt-logo" /&gt;
One example of a consensus algorithm that could be used to compute the final state of a text document that is edited in realtime in multiple distributed computers is called the &amp;quot;Paxos&amp;quot; algorithm. This algorithm is a well-known method for reaching consensus in a distributed system, and it has been used in many different types of applications, including real-time collaborative text editing.&lt;/p&gt;
&lt;p&gt;Here is a high-level overview of how the Paxos algorithm might be used to compute the final state of a text document that is being edited in realtime by multiple users:&lt;/p&gt;
&lt;p&gt;Each user maintains a local copy of the text document, and as they make changes to the document, they also generate a sequence of &amp;quot;operations&amp;quot; that represent the changes they are making. These operations are sent to the other users in the network.&lt;/p&gt;
&lt;p&gt;When a user receives an operation from another user, they apply the operation to their local copy of the document. This may result in conflicts with other operations that have already been applied to the document, in which case the user will need to resolve the conflicts in some way (e.g. by using Operational Transform (OT) techniques).&lt;/p&gt;
&lt;p&gt;As the users continue to edit the document and generate new operations, they also participate in the Paxos consensus protocol. This involves sending messages to each other in order to agree on the sequence of operations that should be applied to the document. The messages are sent in a specific order, and they are used to ensure that all of the users eventually agree on the same sequence of operations.&lt;/p&gt;
&lt;p&gt;Once all of the users have agreed on the sequence of operations, they can apply those operations to their local copies of the document in order to compute the final state of the document. This final state will be the same for all of the users, and it will reflect all of the changes that have been made to the document by any of the users.&lt;/p&gt;
&lt;p&gt;The Paxos algorithm is a complex and subtle algorithm, and this is just a very high-level overview of how it might be used in this context. There are many different variations of the Paxos algorithm, and the specific details of how it is implemented will depend on the specific requirements and constraints of the system.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;I have been enjoying using &lt;a href="https://chat.openai.com/chat"&gt;chatGPT&lt;/a&gt; as a private tutor / reference on topics. My biggest issue is now keeping up with the knowledge and asking the right questions.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://ewinnington.github.io/posts/7Reasons-no-cache</id>
		<title>7 reasons to not use caching</title>
		<link href="http://ewinnington.github.io/posts/7Reasons-no-cache" />
		<updated>2022-12-04T16:30:00Z</updated>
		<content>&lt;p&gt;Inspired by &lt;a href="https://twitter.com/mjovanovictech"&gt;Milan Jovanović&lt;/a&gt; tweet on &lt;a href="https://twitter.com/mjovanovictech/status/1599124855542411264"&gt;5 reasons to use Redis for caching&lt;/a&gt;,&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/caching/5reasonsCaching.png" class="img-fluid" width="80%" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;and &lt;a href="https://twitter.com/danielmarbach"&gt;Daniel Marbach's&lt;/a&gt; response &amp;quot;&lt;a href="https://twitter.com/danielmarbach/status/1599352526888849408"&gt;Now I want to see five reasons to avoid caching ✋😂&lt;/a&gt;&amp;quot;&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/caching/5reasonsNoCaching.png" class="img-fluid" width="80%" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;I found &lt;a href="https://twitter.com/ThrowATwit/status/1599356806874427392"&gt;seven reasons to not introduce caching&lt;/a&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Caching can increase complexity in your application, as you need to manage the cached data and ensure it remains consistent with the underlying data store.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Caching can increase latency, as the cache itself introduces an additional lookup step.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Caching can be expensive, both in terms of the additional hardware and storage required for the cache, and the overhead of managing the cache itself.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Caching can be unreliable, as cached data can become stale or inconsistent if it is not adequately managed or invalidated.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Caching can be a security risk, as sensitive data that is stored in the cache may be vulnerable to unauthorized access or exposure. It takes additional effort to ensure that the correct authorizations are applied to cached data, increasing application complexity.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Caching can be harder to debug. To determine why a piece of data is not being retrieved from the cache or is being retrieved from the underlying data store instead is difficult. This can make it challenging to diagnose and fix performance issues related to caching.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Caching can create additional maintenance overhead, as you need to monitor the cache and ensure it is working properly. Monitoring cache hit and miss rates, ensuring that the cache is not getting too full, and periodically purging expired or stale data from the cache.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;and a bonus &lt;a href="https://mobile.twitter.com/joslat/status/1599518029649678336"&gt;8.&lt;/a&gt; from &lt;a href="https://mobile.twitter.com/joslat"&gt;Jose Luis Latorre&lt;/a&gt;
&amp;quot;8. It should be also properly tested, and stress tested... without mention the security testing as well should include a check on this layer too... which would bring us to point 3. More expensive ;)&amp;quot;&lt;/p&gt;
&lt;p&gt;Introducing Caching into any architecture is a decision that must be made with care. We have to ask if it helps us fulfill a business requirement (latency requirements), and improves quality or responsiveness for the end user. And we must ensure the solution is appropriate in terms of cost of operation and cost of monitoring and support. Additionally, the security aspects of a cache should be considered in the solution design.&lt;/p&gt;
&lt;p&gt;In software architecture, there are very few single answers, everything is a compromise. Caching is a great hammer and use it when it is appropriate, but remember not every problem is a nail.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;Inspired by &lt;a href="https://twitter.com/mjovanovictech"&gt;Milan Jovanović&lt;/a&gt; tweet on &lt;a href="https://twitter.com/mjovanovictech/status/1599124855542411264"&gt;5 reasons to use Redis for caching&lt;/a&gt;,&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://ewinnington.github.io/posts/Software-Architecture-Illustration</id>
		<title>Software Architecture illustrations</title>
		<link href="http://ewinnington.github.io/posts/Software-Architecture-Illustration" />
		<updated>2022-11-16T20:10:00Z</updated>
		<content>&lt;h1 id="illustrations-in-software-architecture"&gt;Illustrations in Software architecture&lt;/h1&gt;
&lt;p&gt;In software architecture, I find myself reaching more and more for tools that I can use to generate representations from a simple textual description, be it generated from a tool or hand written. And sometimes, nothing generated looks nice, so I have to do draw it myself!&lt;/p&gt;
&lt;p&gt;Here are a few of the tools I have recently used, for different purposes:&lt;/p&gt;
&lt;h2 id="diagrams-as-code"&gt;Diagrams as code:&lt;/h2&gt;
&lt;h3 id="mermaid"&gt;Mermaid&lt;/h3&gt;
&lt;p&gt;Mermaid is a language to generate flow charts, pie charts, entity relation diagrams and several other diagrams. I’ve used it in internal documentation and blog posts. The graph description language is simple enough that you can write code to generate charts, too.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://mermaid-js.github.io/mermaid/#/"&gt;https://mermaid-js.github.io/mermaid/#/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A live editor is also available online:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://mermaid.live/"&gt;https://mermaid.live/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Mermaid rendering has been integrated into several markdown renderers, GitHub markdown and VS Code both support it.&lt;/p&gt;
&lt;div class="mermaid"&gt;flowchart LR
    a[Airflow] ---&gt; b[AirFlowTask] --&gt; c[[RabbitMQ Queue Events]] --&gt; d[EventReceiver] --insert--&gt; e[(Postgresql)] --&gt; Monitoring
    d --failed--&gt; g[[Deadletter queue]] --&gt; h[Reconciliation] --&gt; e
&lt;/div&gt;
&lt;p&gt;&lt;img src="/posts/images/SA-Illustrations/Mermaid-flow.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;div class="mermaid"&gt;sequenceDiagram
    autonumber

    participant C as Client
    participant S as Target

    S --) C: Communicate API-Key
    C -&gt;&gt; S: Send request with API-Key
    activate S
    S --&gt;&gt; S: Validate API-Key
    S -X C: If not valid: Return 401 
    S -&gt;&gt; C: If valid: Return 200
    deactivate S
&lt;/div&gt;
&lt;p&gt;&lt;img src="/posts/images/SA-Illustrations/Mermaid-sequence.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;h3 id="python-diagrams"&gt;Python Diagrams&lt;/h3&gt;
&lt;p&gt;Python has a diagram library which has icons for most programming tools, from Airflow to ZeroMQ. You design the diagram with simple Python code and it uses the Graphviz library to render png images. Highly recommended for small architecture diagrams that just need a dozen or so elements.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://diagrams.mingrammer.com/"&gt;https://diagrams.mingrammer.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I’ve also done some pull requests to add symbols to the library and I recommend you do so too if you have elements that are missing in your diagrams.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;## pip install diagrams
## winget install -e --id Graphviz.Graphviz
## set PATH=C:\Program Files\Graphviz\bin;%PATH%
#
# Architecture of the docker-compose using a chart generated in py

from diagrams import Diagram, Cluster
from diagrams.onprem.inmemory import Redis
from diagrams.onprem.database import Postgresql
from diagrams.onprem.queue import Rabbitmq
from diagrams.programming.language import PHP
from diagrams.programming.language import Csharp
from diagrams.onprem.client import Users
from diagrams.onprem.network import Nginx

with Diagram(&amp;quot;Composed Docker&amp;quot;, show=False):
    users = Users(&amp;quot;users&amp;quot;)
    
    with Cluster(&amp;quot;Front-End&amp;quot;):
        web = Nginx(&amp;quot;ngweb&amp;quot;)
        php = PHP(&amp;quot;php&amp;quot;)

    with Cluster(&amp;quot;Back-Ends&amp;quot;):
        redis = Redis(&amp;quot;cache&amp;quot;)
        rabbit = Rabbitmq(&amp;quot;rabbit&amp;quot;)
        listener = Csharp(&amp;quot;listener&amp;quot;)
        db = Postgresql(&amp;quot;db&amp;quot;)
    
    users &amp;gt;&amp;gt; web &amp;gt;&amp;gt; php &amp;gt;&amp;gt; rabbit &amp;gt;&amp;gt; listener &amp;gt;&amp;gt; db
    php &amp;gt;&amp;gt; redis
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="/posts/images/SA-Illustrations/composed_docker.png" class="img-fluid" width="80%" alt="" /&gt;&lt;/p&gt;
&lt;h3 id="plantuml"&gt;PlantUML&lt;/h3&gt;
&lt;p&gt;The big one! Plant UML has a ton of diagrams, the language is maybe a bit more obscure and complicated than mermaid, but you gain a lot from PlantUML when you actually need those features.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://plantuml.com/"&gt;https://plantuml.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;PlantUML also has a live editor online:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.plantuml.com/plantuml/uml/"&gt;https://www.plantuml.com/plantuml/uml/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Actually, I’ve used it quite seldomly, being able to cover most requirements with Mermaid and Python Diagrams.&lt;/p&gt;
&lt;h3 id="d2"&gt;D2&lt;/h3&gt;
&lt;p&gt;D2 has recently been open-sourced and made available. I haven't yet had time to test it.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://d2lang.com/tour/intro/"&gt;https://d2lang.com/tour/intro/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="diagrams-as-drawing"&gt;Diagrams as Drawing:&lt;/h2&gt;
&lt;h3 id="diagrams"&gt;Diagrams&lt;/h3&gt;
&lt;p&gt;Diagrams.net has both an online and an offline version of a vector drawing software that works exceedingly well for software architecture illustrations. With symbols for most public cloud platforms included in their delectable libraries, you’ll be able to find the right symbol you need.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.diagrams.net/"&gt;https://www.diagrams.net/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There’s even a Visual Studio Code extension for editing a diagram inside the IDE. Export can be done to PNG easily. Diagrams produced are easily embedded in Atlassian’s wiki and other wiki products.&lt;/p&gt;
&lt;p&gt;Highly recommend if you need to place your architecture elements instead of relying on the auto layout of diagrams as code. Now I just wish that the diagrams as code tools could create a diagram baseline compatible with this tool to modify the layout.&lt;/p&gt;
&lt;h3 id="archi-archimate-modelling"&gt;Archi (Archimate modelling)&lt;/h3&gt;
&lt;p&gt;If you use the &lt;a href="https://en.m.wikipedia.org/wiki/ArchiMate"&gt;Archimate modelling language&lt;/a&gt;, then this is the tool for you to build your modelling concepts. The formalism is great for making something that everyone can “read” once trained on it, but the investment can be quite high to do the correct modelling of your infrastructure with this tool.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.archimatetool.com/"&gt;https://www.archimatetool.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I used the Local application running on Windows.&lt;/p&gt;
&lt;h4 id="online-architecture-repositories"&gt;Online architecture repositories&lt;/h4&gt;
&lt;p&gt;There is also online hosted versions of architecture repository tools using Archimate &lt;a href="https://www.boc-group.com/en/adoit/"&gt;Adoit EA Suite&lt;/a&gt; and an associated community version too.&lt;/p&gt;
&lt;p&gt;Alternatively, there's also:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.mega.com/hopex-platform"&gt;Hopex's MEGA&lt;/a&gt; which I only used as it was being decommissioned&lt;/li&gt;
&lt;li&gt;&lt;a href="https://sparxsystems.com/products/ea/index.html"&gt;Sparx Systems’ Enterprise Architect&lt;/a&gt; which I used for a short amount of time before the company standardised on Adoit.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="yed"&gt;yEd&lt;/h3&gt;
&lt;p&gt;In writing this article, I discovered the tool yEd and wanted to mention it for completeness, I haven't had the opportunity to use it yet. It does mention many of the illustration types that are useful (BPML, Flowcharts, UML, ...).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.yworks.com/products/yed"&gt;https://www.yworks.com/products/yed&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="diagrams-from-programming"&gt;Diagrams from programming:&lt;/h2&gt;
&lt;p&gt;Now these are more out there and not always directly applicable, but when you need a visualisation that the above tools cannot do, it’s time to break out these applications.&lt;/p&gt;
&lt;h3 id="d3js"&gt;D3js&lt;/h3&gt;
&lt;p&gt;Not sure if I need to introduce D3js,  it is probably one of the most commonly use and important is visualisation libraries. Used in everything from maps to genomics to economic data. It can do it all.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://d3js.org/"&gt;https://d3js.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I previously used d3js to embed charts of price curves generated from market data, overlapped with the delivery period of energy instruments.&lt;/p&gt;
&lt;h3 id="processing.js"&gt;Processing.js&lt;/h3&gt;
&lt;p&gt;Animated heart pulsating on a field of scintillating gold lace? Yes. That and many more things! Processing excels in the visual demos, animations and more. You’ll have to code it, but it’s no issue to get your custom Mandelbrot animated render and many more. Has interactions with sound and many more features. Does have a high bar of entry though and it takes a while to be productive with it.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://processing.org/"&gt;https://processing.org/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="graphviz"&gt;Graphviz&lt;/h3&gt;
&lt;p&gt;I would be remiss if I didn’t mention Graphviz. It is the library used to generate diagrams from textual descriptions using one of their many languages, Dot being one of them.&lt;/p&gt;
&lt;p&gt;Not something I use directly but more indirect usages via the Python diagrams library. You can learn it’s language and create charts.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://graphviz.org/"&gt;https://graphviz.org/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="online-services-collaborative"&gt;Online services (collaborative)&lt;/h2&gt;
&lt;p&gt;When you need online collaboration, which the tools above do not cover, you can turn to the following services.&lt;/p&gt;
&lt;h3 id="miro"&gt;Miro&lt;/h3&gt;
&lt;p&gt;I’ve had the most experience with Miro while running large organisation meetings as a place to collect ideas, do feedback rounds and generally plan activities.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://miro.com"&gt;https://miro.com&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="lucidspark-lucidchart-and-lucidscale"&gt;LucidSpark, LucidChart and LucidScale&lt;/h3&gt;
&lt;p&gt;Both collaboration and vector illustration online software. Also their ability with lucid scale to connect and document your cloud infrastructure is very impressive and helps to keep you infrastructure maps updated.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.lucidspark.com/"&gt;https://www.lucidspark.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.lucidscale.com/"&gt;https://www.lucidscale.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.lucidchart.com/"&gt;https://www.lucidchart.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Sadly, it’s one that I’ve not had the opportunity to use very often, usually because it was covered by other tools and no one else in the company was using it. But you should check it out to see if it does work for you and your team.&lt;/p&gt;
&lt;h3 id="microsoft-whiteboard"&gt;Microsoft Whiteboard&lt;/h3&gt;
&lt;p&gt;When all else fails, there’s Microsoft whiteboard. It works, there’s an online version, an integration in teams, a desktop app and even an iOS / iPadOS application. More suited to drawing with a pen, then it becomes a great collaborative whiteboard. I have given internal talks using a Microsoft Whiteboard as a backdrop. I really like to start small and progressively zoom out on these massive canvases.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.microsoft.com/en-us/microsoft-365/microsoft-whiteboard/digital-whiteboard-app"&gt;https://www.microsoft.com/en-us/microsoft-365/microsoft-whiteboard/digital-whiteboard-app&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="mind-maps"&gt;Mind maps&lt;/h2&gt;
&lt;p&gt;I don't use mind maps very often anymore. I was taught to use them as a child, but haven't kept up the practice. I'm just adding a couple of references in case you are looking for them:&lt;/p&gt;
&lt;h3 id="vscode-mindmap"&gt;vscode-mindmap&lt;/h3&gt;
&lt;p&gt;I've used vscode-mindmap when I needed to create a quick hierarchy map.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://marketplace.visualstudio.com/items?itemName=pmcxs.vscode-mindmap"&gt;https://marketplace.visualstudio.com/items?itemName=pmcxs.vscode-mindmap&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;There’s no one tool for everything in this day and age. Use what works for you and try out several to see if they stick!&lt;/p&gt;
&lt;p&gt;I highly recommend generating some charts in mermaid from your own database ER-Diagram (easy to do!) or using it to make pie charts like &lt;a href="https://youtu.be/IXRGa5m-Lbo"&gt;Microsoft Polyglot notebooks demonstrates at 15:05 onwards&lt;/a&gt; in their &lt;a href="https://github.com/dotnet/interactive/blob/main/samples/notebooks/polyglot/github%20repo%20milestone%20report.ipynb"&gt;GitHub demo notebook&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you have other recommendations for me, do feel free to reach out and I’ll see if they make to cut to get added to this list. :) You can even pull request this actual blog post on GitHub.&lt;/p&gt;
&lt;p&gt;Finally, if someone has a recommendation for a WAN/LAN topology visualisation or charting tool, I’d be happy to hear about it and your experience with it!&lt;/p&gt;
</content>
		<summary>&lt;p&gt;In software architecture, I find myself reaching more and more for tools that I can use to generate representations from a simple textual description, be it generated from a tool or hand written. And sometimes, nothing generated looks nice, so I have to do draw it myself!&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://ewinnington.github.io/posts/Data-Lineage</id>
		<title>Data Lineage for dataflow and workflow processes</title>
		<link href="http://ewinnington.github.io/posts/Data-Lineage" />
		<updated>2022-11-12T22:10:00Z</updated>
		<content>&lt;h1 id="data-lineage"&gt;Data lineage&lt;/h1&gt;
&lt;p&gt;When working with large amounts of data, extraction, transforms and loads procedures can hide the source of the original data and make inquiries on &amp;quot;where did this data come from and what happened to it?&amp;quot; difficult to answer.&lt;/p&gt;
&lt;p&gt;A data lineage is &amp;quot;the process of understanding, recording, and visualizing data as it flows from data sources to consumption&lt;a id="fnref:1" href="#fn:1" class="footnote-ref"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&amp;quot; and tries to answer that question.&lt;/p&gt;
&lt;p&gt;Using dataflow and ETL orchestration tools such as &lt;a href="https://airflow.apache.org/"&gt;Airflow&lt;/a&gt;, &lt;a href="https://www.prefect.io"&gt;Prefect&lt;/a&gt;, &lt;a href="https://nifi.apache.org/"&gt;NiFi&lt;/a&gt;, we move and transform data, but also lose the reference as to how the data was transformed.&lt;/p&gt;
&lt;p&gt;In this document, we will approach one open source tool OpenLineage and one &amp;quot;hand built&amp;quot; approach to capturing and storing data lineage information.&lt;/p&gt;
&lt;h1 id="openlineage-and-marquez-open-source-tools"&gt;OpenLineage and Marquez - Open source tools&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://openlineage.io/"&gt;OpenLineage&lt;/a&gt; is an open source project and framework for data lineage collection and analysis that helps collect lineage metadata from the data  processing applications. At its core, OpenLineage exposes a standard API for metadata collection - a single API call: &lt;a href="https://openlineage.io/apidocs/openapi/"&gt;&lt;strong&gt;postRunEvent&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To simplify its implementation with AirFlow, Open Lineage has an &lt;a href="https://github.com/OpenLineage/OpenLineage/tree/main/integration/airflow/openlineage/airflow"&gt;airflow connection module&lt;/a&gt; already available.&lt;/p&gt;
&lt;p&gt;On the back-end, the storage of run meta-data has a reference implementation named Marquez. The data model is illustrated here.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://lucid.app/lucidchart/f918ce01-9eb4-4900-b266-49935da271b8/view?page=8xAE.zxyknLQ#"&gt;Marquez data model&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/data-lineage/Marquez-Data-Model.png" class="img-fluid" width="80%" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;But it is also possible to implement one's own storage for metadata in case there is a need or added value, but adopting the open source solution will be an advantage for integration later.&lt;/p&gt;
&lt;h1 id="locally-grown-alternatives"&gt;Locally grown alternatives&lt;/h1&gt;
&lt;p&gt;Data correlation and lineage information can be generated via the emission of events while processing input data. Additionally, input data can be fingerprinted via a fast hash function to check for duplicate imports, so as to enable idempotent processing.&lt;/p&gt;
&lt;h3 id="input-dataset-fingerprinting-via-non-cryptographic-hash-function"&gt;Input dataset fingerprinting via non-cryptographic hash function&lt;/h3&gt;
&lt;p&gt;We can use a fast non-cryptographic hash function such as &lt;a href="https://github.com/backtrace-labs/umash"&gt;umash&lt;/a&gt; to generate a hash of the input data or xxhash &lt;code&gt;sudo apt-get install xxhash&lt;/code&gt;. xxhash is capable of taking streaming STDIN data from compressed files to generate a fast hash.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;time gunzip -c /mnt/d/smart_meter_data/ckw_opendata_smartmeter_dataset_a_202101.csv.gz | /usr/bin/xxhsum
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="correlation-ids-with-uuids"&gt;Correlation IDs with UUIDs&lt;/h3&gt;
&lt;p&gt;Generating uuid for correlations identifiers along &lt;a href="https://www.rfc-editor.org/rfc/rfc4122.html"&gt;rfc4122&lt;/a&gt; gives us multiple variant generation algorithms to give us sortable UUIDs which minimize collision possibilities even with a high UUID generation rate.&lt;/p&gt;
&lt;h3 id="hierarchical-correlation-ids-using-closure-tables"&gt;Hierarchical correlation IDs using closure tables&lt;/h3&gt;
&lt;p&gt;When inputs contain a dataset that is composed of multiple data points that identify unique sets in our final processed dataset, it becomes necessary to be able to trace their lineage back to the initial input. A recursive search through a table of entries to find the parent correlation identifier of a child time-series is quite inefficient.&lt;/p&gt;
&lt;p&gt;To have fast search over deep hierarchies of correlations IDs in relational databases, we can turn to the concept of closure tables.&lt;/p&gt;
&lt;table class="table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align: left;"&gt;Field&lt;/th&gt;
&lt;th style="text-align: left;"&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;ParentId&lt;/td&gt;
&lt;td style="text-align: left;"&gt;UUID&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;ChildId&lt;/td&gt;
&lt;td style="text-align: left;"&gt;UUID&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;Depth&lt;/td&gt;
&lt;td style="text-align: left;"&gt;integer&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This table structure allows to query in one query all parent or children of an identifier in one non-recursive sql query. This is done at the cost of having to insert the entire hierarchy of the correlationIDs upon insertion.&lt;/p&gt;
&lt;p&gt;Here we are representing the two hierarchies&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;aaa &amp;gt; bbb &amp;gt; ccc
aaa &amp;gt; eee 
&lt;/code&gt;&lt;/pre&gt;
&lt;table class="table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align: left;"&gt;ParentId&lt;/th&gt;
&lt;th style="text-align: left;"&gt;ChildId&lt;/th&gt;
&lt;th style="text-align: right;"&gt;Depth&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;aaa&lt;/td&gt;
&lt;td style="text-align: left;"&gt;aaa&lt;/td&gt;
&lt;td style="text-align: right;"&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;bbb&lt;/td&gt;
&lt;td style="text-align: left;"&gt;bbb&lt;/td&gt;
&lt;td style="text-align: right;"&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;ccc&lt;/td&gt;
&lt;td style="text-align: left;"&gt;ccc&lt;/td&gt;
&lt;td style="text-align: right;"&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;eee&lt;/td&gt;
&lt;td style="text-align: left;"&gt;eee&lt;/td&gt;
&lt;td style="text-align: right;"&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;aaa&lt;/td&gt;
&lt;td style="text-align: left;"&gt;bbb&lt;/td&gt;
&lt;td style="text-align: right;"&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;bbb&lt;/td&gt;
&lt;td style="text-align: left;"&gt;ccc&lt;/td&gt;
&lt;td style="text-align: right;"&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;aaa&lt;/td&gt;
&lt;td style="text-align: left;"&gt;ccc&lt;/td&gt;
&lt;td style="text-align: right;"&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;aaa&lt;/td&gt;
&lt;td style="text-align: left;"&gt;eee&lt;/td&gt;
&lt;td style="text-align: right;"&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;em&gt;note&lt;/em&gt;: &lt;em&gt;if desired the initial 0 depth nodes can be neglected from the insertion process without losing functionality, but can be useful in certain modeling processes (eg. rights, groups)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;If we need to find all children of &lt;strong&gt;aaa&lt;/strong&gt;, we can do a&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SELECT ChildId, Depth FROM Closure_Table WHERE ParentId = &amp;quot;aaa&amp;quot; ORDER BY Depth;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which returns the following&lt;/p&gt;
&lt;table class="table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align: left;"&gt;ChildId&lt;/th&gt;
&lt;th style="text-align: left;"&gt;Depth&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;aaa&lt;/td&gt;
&lt;td style="text-align: left;"&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;bbb&lt;/td&gt;
&lt;td style="text-align: left;"&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;eee&lt;/td&gt;
&lt;td style="text-align: left;"&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;ccc&lt;/td&gt;
&lt;td style="text-align: left;"&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;If we need to find all parents of &lt;strong&gt;eee&lt;/strong&gt;, we can do a&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SELECT ParentId, Depth FROM Closure_Table WHERE ChildId = &amp;quot;eee&amp;quot; ORDER BY Depth DESC;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which returns the following&lt;/p&gt;
&lt;table class="table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align: left;"&gt;ParentId&lt;/th&gt;
&lt;th style="text-align: left;"&gt;Depth&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;aaa&lt;/td&gt;
&lt;td style="text-align: left;"&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;eee&lt;/td&gt;
&lt;td style="text-align: left;"&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id="table-structure-for-a-correlated-fingerprinted-hierarchical-data-lineage"&gt;Table structure for a Correlated, fingerprinted hierarchical data lineage&lt;/h3&gt;
&lt;p&gt;With correlatedEvent and CorrelatedLineage tables, it becomes possible in a single request to generate a lineage graph for parents or descendants of correlated dataset.&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/data-lineage/CorrelatedLineageDataModel.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;erDiagram
    CorrelationEvent {
        uuid     IdEvent
        string   Process
        string   Version
        hash     Fingerprint
        int      InputSize
        datetime EventTime
        int      Forced
    }

    CorrelationDetails {
        uuid IdEvent
        string Field
        json Data 
    }

    CorrelationLineage {
        uuid IdParent
        uuid IdChild
        integer Depth
    }

    CorrelationEvent ||--o{ CorrelationDetails : &amp;quot;&amp;quot;
    CorrelationEvent ||--o{ CorrelationLineage : &amp;quot;parent&amp;quot;
    CorrelationEvent ||--o{ CorrelationLineage : &amp;quot;child&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It would even be possible to generate the diagrams using mermaid automatically to trace the flows through the system&lt;a id="fnref:2" href="#fn:2" class="footnote-ref"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/data-lineage/CorrelatedLineageFlow.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;flowchart LR
    id1((&amp;quot;DSO Timeseries&amp;quot;)) --&amp;gt; id2[SFTP Download] --&amp;gt; id3[Split]
    id3 --&amp;gt; TS01
    id3 --&amp;gt; TS02
    id3 --&amp;gt; TS03 
    id3 --&amp;gt; TS04
    id3 --&amp;gt; TS..
    TS01 --&amp;gt; id4[Delivery point sum]
    TS02 --&amp;gt; id4
    id4 --&amp;gt; id5[Load]

    met((Weather provider)) --&amp;gt; met2[API Download] --&amp;gt; met3[&amp;quot;Aggregate to hour&amp;quot;] --&amp;gt; met4[&amp;quot;delivery point history&amp;quot;]

    met4 --&amp;gt; for1[forecast consumption]
    id4 --&amp;gt; for1 --&amp;gt; for2[Load]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="solution-design"&gt;Solution design&lt;/h3&gt;
&lt;p&gt;A rabbitMQ message queue to receive correlation events emitted by the tasks, with several consumer tasks receiving and committing to the database is a preferred approach over an HTTP 1.1 connection due the the scaling efficiency of AMQP over pure HTTP&lt;a id="fnref:3" href="#fn:3" class="footnote-ref"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/data-lineage/CorrelatedApplicationFlow.png" class="img-fluid" width="80%" alt="" /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;flowchart LR
    a[Airflow] ---&amp;gt; b[AirFlowTask] --&amp;gt; c[[RabbitMQ Queue Events]] --&amp;gt; d[EventReceiver] -- success --&amp;gt; g[(Postgresql)] --&amp;gt; Monitoring
    d -- failed --&amp;gt; e[[RabbitMQ Queue Deadletter]] --&amp;gt; f[DLQ processing and Reconciliation] --&amp;gt; g
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A particular focus on the monitoring of the solution is necessary to truly have an operational system. The RabbitMQ should be a redundant, instrumented and reported to Graphana, with a queue length monitoring in place. The EventReceivers should employ a dead letter queue in case message are rejected by the database. These rejected messages could also also be a uuid collision - which can be treated by the daily reconciliation process and DeadLetter queue processing.&lt;/p&gt;
&lt;p&gt;A high availability Postgresql is recommended, either as a local instance or as a cloud hosted service - which would facilitate operations.&lt;/p&gt;
&lt;p&gt;The issue of Data retention should be discussed with Business. If we do not keep a time-series history in the time-series datastore, then the event correlation become actually an &lt;a href="https://microservices.io/patterns/data/event-sourcing.html"&gt;event sourcing pattern&lt;/a&gt;, enabling to re-create the history of how the time-series was updated.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Irrespective of the method chosen to capture and store the messages, the systems chosen must provide a high availability solution for data lineage - but must be sure to not block ingestion if the data lineage system is unresponsive. As long as the message queue is persistent and accessible, it can always be caught up later.&lt;/p&gt;
&lt;p&gt;The main task is emitting the events with meaningful data and unique correlation IDs. A focus on the semantics of the events while developing the workflow / dataflows is primordial. A callable event library provides the best developer experience to maximize standardization of code&lt;/p&gt;
&lt;p&gt;The design of idempotent imports into the system is important, it allows to replay events non-destructively and provides operational resilience.&lt;/p&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;&lt;a href="https://www.imperva.com/learn/data-security/data-lineage/"&gt;https://www.imperva.com/learn/data-security/data-lineage/&lt;/a&gt;&lt;a href="#fnref:1" class="footnote-back-ref"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;&lt;a href="https://github.com/dotnet/interactive/blob/main/samples/notebooks/polyglot/github%20repo%20milestone%20report.ipynb"&gt;https://github.com/dotnet/interactive/blob/main/samples/notebooks/polyglot/github%20repo%20milestone%20report.ipynb&lt;/a&gt; - See the PieWithMermaid C# task for a visualisation of such an interaction.&lt;a href="#fnref:2" class="footnote-back-ref"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:3"&gt;
&lt;p&gt;This should be re-evaluated when HTTP/3 oneshot becomes available in the servers and languages used. The expected performance improvement are such that at that time HTTP/3 QUIC might outrace any other streaming solution. &lt;a href="https://blog.cloudflare.com/http3-the-past-present-and-future/"&gt;https://blog.cloudflare.com/http3-the-past-present-and-future/&lt;/a&gt;&lt;a href="#fnref:3" class="footnote-back-ref"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</content>
		<summary>&lt;p&gt;When working with large amounts of data, extraction, transforms and loads procedures can hide the source of the original data and make inquiries on "where did this data come from and what happened to it?" difficult to answer.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://ewinnington.github.io/posts/tesla-megapack</id>
		<title>Tesla Megapacks put into context</title>
		<link href="http://ewinnington.github.io/posts/tesla-megapack" />
		<updated>2022-11-09T21:40:00Z</updated>
		<content>&lt;h1 id="tesla-megapacks"&gt;Tesla Megapacks&lt;/h1&gt;
&lt;p&gt;Tesla on Twitter announced: &lt;a href="https://t.co/aw85eHECXI"&gt;&amp;quot;Meet Megafactory, our new Megapack factory in Lathrop, CA 🔋🔋🔋&amp;quot;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Tesla's energy division has recently completed their new Megapack factory in Lathrop California , which they claim can produce currently 10'000 Megapacks a year. How much storage is that and how does this compare to a Hydropower pump storage plant?&lt;/p&gt;
&lt;h2 id="tesla-megapacks-specs-per-pack"&gt;Tesla megapacks Specs per pack&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;4 Hour Duration&lt;/li&gt;
&lt;li&gt;Power: 970 kW&lt;/li&gt;
&lt;li&gt;Energy: 3,916 kWh per Megapack&lt;/li&gt;
&lt;li&gt;Round Trip Efficiency: 93.5%&lt;/li&gt;
&lt;li&gt;9.12 m x 1.65 m x 2.79 m&lt;/li&gt;
&lt;li&gt;38,100 kg&lt;/li&gt;
&lt;li&gt;~$2 million per pack&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="offer"&gt;Offer&lt;/h2&gt;
&lt;p&gt;An offer was generated on the Tesla Energy website to get an appropriate pricing for the largest system they offer.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1000 Megapack&lt;/li&gt;
&lt;li&gt;969.6 MW Power&lt;/li&gt;
&lt;li&gt;3916 MWh Energy Megapack&lt;/li&gt;
&lt;li&gt;Duration: 4 Hours&lt;/li&gt;
&lt;li&gt;Delivery: Q3 2024&lt;/li&gt;
&lt;li&gt;Estimated Price (California) $1,832,519,850&lt;/li&gt;
&lt;li&gt;Est. Annual Maintenance $4,821,480 - Maintenance Price escalates at 2% per year&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Based on this, we can see that 10'000 megapacks represent about 39160 MWh of storage (39 GWh), with a sales cost of approx $18 billion.&lt;/p&gt;
&lt;p&gt;So how does this compare to the two latest large Swiss Pump-Storage Hydropowerplants?&lt;/p&gt;
&lt;h2 id="hydropower-plants"&gt;Hydropower plants&lt;/h2&gt;
&lt;h3 id="nant-de-drance-pump-storage-extension"&gt;Nant-de-Drance pump-storage extension&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;1 Pump-storage power plant&lt;/li&gt;
&lt;li&gt;Power 900 MW (Turbines and Pumps)&lt;/li&gt;
&lt;li&gt;Storage 20 GWh&lt;/li&gt;
&lt;li&gt;Duration: 19 Hours&lt;/li&gt;
&lt;li&gt;Round trip efficiency: over 90%&lt;/li&gt;
&lt;li&gt;Estimated Price 2 billion CHF&lt;/li&gt;
&lt;li&gt;~14 years to build and bring into operation&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="kraftwerk-linthlimmern-pump-storage-extension"&gt;Kraftwerk-Linth–Limmern pump-storage extension&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;1 Pump-storage power plant&lt;/li&gt;
&lt;li&gt;Power 1000 MW (Turbines and Pumps)&lt;/li&gt;
&lt;li&gt;Storage 33 GWh&lt;/li&gt;
&lt;li&gt;Duration: 33 Hours&lt;/li&gt;
&lt;li&gt;Round trip efficiency: over 90%&lt;/li&gt;
&lt;li&gt;Estimated Price 2.1 billion CHF&lt;/li&gt;
&lt;li&gt;~10 years to build and bring into operation&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The storage cost of the batteries is currently about a factor 4-9x the price of the hydropower plant construction but have the advantage of being available within about 18 months. What remains to be seen is how much battery degradation is a factor in these grid scale battery installations. At least Tesla is offering, from my understanding, a 15 year warranty on the Megapack.&lt;/p&gt;
&lt;p&gt;The amount of storage produced by the factory represents more than 1 large hydropower plant per year.&lt;/p&gt;
&lt;p&gt;The 39 GWh storage produced by the factory in one year is a huge amount, so much that it would cover around 25% of the &lt;a href="https://www.iea.org/data-and-statistics/charts/battery-storage-capability-by-countries-2020-and-2026"&gt;expected total capacity that the IEA planned for the entire world by 2026&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="post-scriptum-new-gridscale-batteries-in-europe"&gt;Post-scriptum: new gridscale batteries in Europe&lt;/h2&gt;
&lt;p&gt;2022.11.22 - &lt;a href="https://www.bbc.com/news/uk-england-humber-63707463"&gt;Cottingham: Europe's biggest battery storage system switched on - 196MWh&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Power: ? (my estimate ~50-100 MW)&lt;/li&gt;
&lt;li&gt;Storage: 196 MWh&lt;/li&gt;
&lt;li&gt;use Tesla's AI software to match energy supply to demand&lt;/li&gt;
&lt;li&gt;Commissioning in two stages in December 2022 and March 2023.&lt;/li&gt;
&lt;li&gt;Supplier: Tesla&lt;/li&gt;
&lt;li&gt;Cost: ? (my estimate $100 million+)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As usual, BBC is terribly uninformative about specifications and cost. If we assume 50 Tesla Megapacks, cost should be around $100 million+ and 50 to 100 MW based on the 2h or 4h megapacks. Interesting to see a Tesla system in Europe. I expect many more to come online.&lt;/p&gt;
&lt;p&gt;2022.11.07 - &lt;a href="https://www.rwe.com/en/press/rwe-generation/2022-11-07-battery-storage-220-mw-neurath"&gt;RWE gives green light for 220-megawatt battery storage system in North Rhine-Westphalia&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Power: 80 + 140 MW = 220 MW&lt;/li&gt;
&lt;li&gt;Storage: delivering the required output for over an hour but full capacity not mentioned. 220 MWh to 440MWh.&lt;/li&gt;
&lt;li&gt;140 million euros&lt;/li&gt;
&lt;li&gt;commissioning in 2024&lt;/li&gt;
&lt;li&gt;Supplier: Not mentioned.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;2021.07.22 - &lt;a href="https://www.rwe.com/en/press/rwe-ag/2021-07-22-rwe-builds-one-of-the-largest-battery-storage-facilities-in-germany"&gt;RWE bringing 72MW BESS in Germany online in November&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Power: 72 + 45MW = 117 MW&lt;/li&gt;
&lt;li&gt;Storage: 128MWh&lt;/li&gt;
&lt;li&gt;€50 million&lt;/li&gt;
&lt;li&gt;commissioning in end 2022&lt;/li&gt;
&lt;li&gt;Supplier: CATL batteries&lt;/li&gt;
&lt;/ul&gt;
</content>
		<summary>&lt;p&gt;Tesla on Twitter announced: &lt;a href="https://t.co/aw85eHECXI"&gt;"Meet Megafactory, our new Megapack factory in Lathrop, CA 🔋🔋🔋"&lt;/a&gt;&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://ewinnington.github.io/posts/sqlite-microstores</id>
		<title>Embracing SQLite and living with micro-services</title>
		<link href="http://ewinnington.github.io/posts/sqlite-microstores" />
		<updated>2022-10-22T00:00:00Z</updated>
		<content>&lt;p&gt;The idea of micro-services and their own single purpose data stores is easy to describe. But then to implement and live with it is a different story. So as a developer and architect, I’ve decided to do just that! Make micro-services and micro-data stores to cover the tiny and small stuff in my life I want to keep track of.&lt;/p&gt;
&lt;p&gt;As an example, I read online comics, light novels and mangas. I had a continuous list of a couple hundred bookmarks that I tried to keep updated with the last position I was when I read the story. But I always forget to update the bookmark and have so many of them that I lose the last read chapter. My solution?&lt;/p&gt;
&lt;p&gt;A SQLite db and some Python code to load it. Pass a single url on some command line Python and it gets added to the Db, a Request goes out, gets the title and chapter from the html, then adds it to the DB by title. Now I have a track of where I left off and I can get have last updated / last read records. Bonus, I can do a SELECT .. ORDER BY updated LIMIT 10 to check the last stories I was reading and pipe them to my browser to open up the chapters where I left them off.&lt;/p&gt;
&lt;p&gt;To really embrace SQLite is to make everything in your life become a new micro database, even if there's only a couple of tables with a dozen or a hundred rows.&lt;/p&gt;
&lt;p&gt;Stock tracking? an SQLite Db with Transactions and a roll-up Inventory table.&lt;/p&gt;
&lt;p&gt;In fact, even when sending data around from one system to another, we should even embrace the simplicity of SQLite over CSV files. See &lt;a href="https://berthub.eu/articles/posts/big-data-storage/"&gt;https://berthub.eu/articles/posts/big-data-storage/&lt;/a&gt; for his views and performance tests.&lt;/p&gt;
&lt;p&gt;Now I have micro data-stores, I can add a service on top which contains the CRUD commands I need to interact with them and show them in a personal dashboard.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;The idea of micro-services and their own single purpose data stores is easy to describe. But then to implement and live with it is a different story. So as a developer and architect, I’ve decided to do just that! Make micro-services and micro-data stores to cover the tiny and small stuff in my life I want to keep track of.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://ewinnington.github.io/posts/Starship-laser-ablation</id>
		<title>The case for a SpaceX Starship laser ablation platform for orbital debris management</title>
		<link href="http://ewinnington.github.io/posts/Starship-laser-ablation" />
		<updated>2022-09-18T00:00:00Z</updated>
		<content>&lt;p&gt;SpaceX’s Starship program as a platform for specialised load-outs has many potential applications: Tanker variants for orbital refuelling, Crew variant for Dear Moon mission and dedicated satellite launcher for the Starlink satellite constellation deployment being the variants we already know about.&lt;/p&gt;
&lt;p&gt;With the latest discussion about Orbital debris fields, I suggest it is time to discuss about another variant: a dedicated laser ablation Starship variant for de-orbiting or destroying 1 to 10 cm sized debris.&lt;/p&gt;
&lt;p&gt;To deorbit or destroy debris in Earth’s orbit, laser ablation is one of the ideal techniques to use since it can directly burn up small debris or  deorbit larger ones via plasma propulsion (in effect, burning up the target object and causing it to be propelled by the plasma generated by the laser hitting the target).&lt;/p&gt;
&lt;p&gt;To be able to make orbital cleanup affordable, we need to have a cheap to fly, high power laser with a sufficient burn time and have the ability to choose the orbit of our laser platform.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cheap to fly: The launch costs of a SpaceX starship is estimated to be at least one order of magnitude less in dollar per kilo to orbit.&lt;/li&gt;
&lt;li&gt;High power laser: The high power of chemical lasers along with their high mass requirements make them a good fit for integration into a Starship. With sufficient mass for the chemicals to supply the laser, the lasers could be used long enough to clear the coplanar orbit. Once the chemicals are depleted, the starship can be landed and the laser refuelled for another mission. Other types of lasers, which have been developed recently, are also candidates: Solid state lasers, fiber lasers, diode lasers - with these the mass capability of Starship would be used for large batteries and potentially hydrogen fuel cells to provide enough power for the application.&lt;/li&gt;
&lt;li&gt;Ability to choose orbit: Laser ablation is most effective when the platform if shooting from a “same altitude and coplanar” orbit. Different launches are then the most effective way of reaching these orbits to get maximal efficiency.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If the &lt;a href="https://www.fastcompany.com/90789865/orbits-act-what-to-know-about-congress-effort-to-clean-up-space-debris"&gt;US government decides to start paying for orbital clearing services&lt;/a&gt; with the Orbital Clearance with the Orbital Sustainability (ORBITS) Act, it would be a clear case to develop such a Starship variant. Without other incentives, it might still be profitable by selling “deorbiting and clearance services” to other satellite providers. This remains to be seen how much “good citizenship” is to be expected from satellite constructors, launchers and operators.&lt;/p&gt;
&lt;h2 id="references-to-laser-ablation-papers"&gt;References to laser ablation papers&lt;/h2&gt;
&lt;p&gt;Space based -
&lt;a href="https://conference.sdo.esoc.esa.int/proceedings/sdc8/paper/43/SDC8-paper43.pdf"&gt;https://conference.sdo.esoc.esa.int/proceedings/sdc8/paper/43/SDC8-paper43.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ground based -
&lt;a href="https://conference.sdo.esoc.esa.int/proceedings/sdc6/paper/29/SDC6-paper29.pdf"&gt;https://conference.sdo.esoc.esa.int/proceedings/sdc6/paper/29/SDC6-paper29.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Air based Anti ballistic missile chemical laser - a lower powered could be used in Starsjip for de-orbiting, but this shows the feasibility.
&lt;a href="https://minutemanmissile.com/abl.html"&gt;https://minutemanmissile.com/abl.html&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="discussion"&gt;Discussion&lt;/h2&gt;
&lt;p&gt;A &lt;a href="https://www.reddit.com/r/SpaceXLounge/comments/xj8bjh/the_case_for_a_spacex_starship_laser_ablation/"&gt;discussion thread on reddit about this post&lt;/a&gt; has provided some feedback and the post has been updated.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;SpaceX’s Starship program as a platform for specialised load-outs has many potential applications: Tanker variants for orbital refuelling, Crew variant for Dear Moon mission and dedicated satellite launcher for the Starlink satellite constellation deployment being the variants we already know about.&lt;/p&gt;</summary>
	</entry>
</feed>