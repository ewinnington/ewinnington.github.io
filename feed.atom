<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
	<id>http://ewinnington.github.io/</id>
	<title>Eric Winnington</title>
	<link rel="self" href="http://ewinnington.github.io/" />
	<rights>2025</rights>
	<updated>2025-10-16T19:59:08Z</updated>
	<subtitle>A collection of thoughts, code and snippets.</subtitle>
	<entry>
		<id>http://ewinnington.github.io/posts/omarchy-hdmi-audio</id>
		<title>Omarchy - Multi-monitor HDMI and Audio</title>
		<link href="http://ewinnington.github.io/posts/omarchy-hdmi-audio" />
		<updated>2025-10-16T00:00:00Z</updated>
		<content>&lt;h1 id="configuring-multi-monitor-and-choosing-hdmi-sound-output-in-omarchy"&gt;Configuring multi-monitor and choosing HDMI sound output in Omarchy&lt;/h1&gt;
&lt;p&gt;I added a vertical monitor to my Omarchy desk. I have my BenQ horitontal and have added a Samsung as a vertical screen on the side.&lt;/p&gt;
&lt;h2 id="video-setup"&gt;Video setup&lt;/h2&gt;
&lt;p&gt;Listing my monitors gives me the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;hyprctl monitors

Monitor HDMI-A-1 (ID 0):
	3840x2160&amp;#64;60.00000 at 0x0
	description: Samsung Electric Company LS27A800U HNMW500048
	make: Samsung Electric Company
	model: LS27A800U
	physical size (mm): 600x340
	serial: -----------
	active workspace: 1 (1)
	special workspace: 0 ()
	reserved: 0 26 0 0
	scale: 1.50
	transform: 1
	focused: yes
	dpmsStatus: 1
	vrr: false
	solitary: 0
	solitaryBlockedBy: windowed mode,missing candidate
	activelyTearing: false
	tearingBlockedBy: next frame is not torn,user settings,missing candidate
	directScanoutTo: 0
	directScanoutBlockedBy: user settings,missing candidate
	disabled: false
	currentFormat: XRGB8888
	mirrorOf: none
	availableModes: 3840x2160&amp;#64;60.00Hz ...

Monitor HDMI-A-2 (ID 1):
	3840x2160&amp;#64;60.00100 at -2560x0
	description: BNQ BenQ EL2870U TBK01026SL0
	make: BNQ
	model: BenQ EL2870U
	physical size (mm): 620x340
	serial: -----------
	active workspace: 2 (2)
	special workspace: 0 ()
	reserved: 0 26 0 0
	scale: 1.50
	transform: 0
	focused: no
	dpmsStatus: 1
	vrr: false
	solitary: 0
	solitaryBlockedBy: not opaque
	activelyTearin#pactl list short sinks
#pactl list cardsg: false
	tearingBlockedBy: next frame is not torn,user settings,missing candidate
	directScanoutTo: 0
	directScanoutBlockedBy: user settings,missing candidate
	disabled: false
	currentFormat: XRGB8888
	mirrorOf: none
	availableModes: 3840x2160&amp;#64;60.00Hz ...

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I edited the monitors file to have &lt;code&gt;~/.config/hypr/monitors.conf&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;monitor=HDMI-A-2,preferred,auto-left,1.5
monitor=HDMI-A-1,preferred,auto-right,1.5, transform, 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;so that the A-1 (Samsung) has the vertical transform. I might switch it to -1 and turn the monitor the other way, because I have less of a bezel on the top of the monitor, but this is the current setup. I'm still not sure which auto- command I need at minimum, but with these two commands - the screens align the way I want.&lt;/p&gt;
&lt;h2 id="audio-setup"&gt;Audio setup&lt;/h2&gt;
&lt;p&gt;The next issue I have is that the audio output is connected from the back of the BenQ monitor to my speaker system, taking the audio along the HDMI channel. Sometimes, depending on which order the screens wake up, the audio output  switches to the Samsung. For the moment, I haven't yet fully figured out how to lock the audio output to a single HDMI output, so I have two scripts to run - checking which outputs where. I'll appreciate the help if someone has suggestions.&lt;/p&gt;
&lt;p&gt;I used these commands to list the cards and outputs:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pactl list short sinks
pactl list cards
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And depending on which screen woke up first, it's either running this one:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pactl set-card-profile alsa_card.pci-0000_00_1f.3 output:hdmi-stereo-extra1
pactl set-default-sink alsa_output.pci-0000_00_1f.3.hdmi-stereo-extra1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or this one:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pactl set-card-profile alsa_card.pci-0000_00_1f.3 output:hdmi-stereo
pactl set-default-sink alsa_output.pci-0000_00_1f.3.hdmi-stereo
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I'm hoping to figure out how to pin the audio to the BenQ monitor soon.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;I added a vertical monitor to my Omarchy desk. I have my BenQ horitontal and have added a Samsung as a vertical screen on the side.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://ewinnington.github.io/posts/omarchy-brightness2</id>
		<title>Omarchy - Integrating screen brightness via key binds</title>
		<link href="http://ewinnington.github.io/posts/omarchy-brightness2" />
		<updated>2025-09-10T00:00:00Z</updated>
		<content>&lt;h1 id="controlling-external-monitor-brightness-ddcci-in-hyprland-with-a-real-osd"&gt;Controlling External Monitor Brightness (DDC/CI) in Hyprland with a Real OSD&lt;/h1&gt;
&lt;p&gt;As a follow-up to the previous entry on how to get the &lt;a href="https://ewinnington.github.io/posts/omarchy-brightness"&gt;brightness adapted by ddcutil&lt;/a&gt;, I actually asked &lt;a href="https://openai.com/codex/"&gt;OpenAI's Codex&lt;/a&gt; to wire it up in my keyboard bindings. Once it succeeded and get the osd wired up, I asked it to document the process. Here is the Codex generated documentation.&lt;/p&gt;
&lt;p&gt;As a side note I created a small script to increase or decrease the brightness on the command line, that is executable.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;#!/bin/bash
# brightness up/down script using ddcutil
STEP=10
case &amp;quot;$1&amp;quot; in
  up)   ddcutil setvcp 10 +$STEP ;;
  down) ddcutil setvcp 10 -$STEP ;;
  get)  ddcutil getvcp 10 ;;
  *)    echo &amp;quot;Usage: $0 {up|down|get}&amp;quot; ;;
esac
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id="openai-codex-steps-and-explanations"&gt;OpenAI Codex steps and explanations&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Environment: Hyprland (Omarchy on Arch), SwayOSD, ddcutil&lt;/li&gt;
&lt;li&gt;Goal: Make hardware brightness keys and Alt+F1/F2 control HDMI monitor brightness via DDC/CI, with a correct on-screen display (OSD).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="the-problem"&gt;The Problem&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Omarchy’s default media bindings show the OSD and call brightnessctl, which targets laptop backlights—not external HDMI displays.&lt;/li&gt;
&lt;li&gt;My script ~/bin/hdmi-brightness already adjusts HDMI brightness using ddcutil, but Hyprland wasn’t calling it from brightness keys.&lt;/li&gt;
&lt;li&gt;Bonus ask: show an OSD reflecting the real HDMI brightness level.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="solution-summary"&gt;Solution Summary&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Unbind default brightness keys.&lt;/li&gt;
&lt;li&gt;Bind brightness keys and Alt+F1/F2 to the hdmi-brightness script.&lt;/li&gt;
&lt;li&gt;After each adjustment, read the real brightness via ddcutil getvcp 10 and display an OSD using SwayOSD’s custom-progress mode.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="keybindings"&gt;Keybindings&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;File: ~/.config/hypr/bindings.conf&lt;/li&gt;
&lt;li&gt;Unbind defaults:
&lt;ul&gt;
&lt;li&gt;unbind = , XF86MonBrightnessUp&lt;/li&gt;
&lt;li&gt;unbind = , XF86MonBrightnessDown&lt;/li&gt;
&lt;li&gt;unbind = ALT, XF86MonBrightnessUp&lt;/li&gt;
&lt;li&gt;unbind = ALT, XF86MonBrightnessDown&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Bind to DDC/CI script + OSD:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;bindeld = , XF86MonBrightnessUp, HDMI Brightness up, exec, bash -lc &amp;quot;~/bin/hdmi-brightness raise; read P R &amp;lt;&amp;lt;&amp;lt; $(ddcutil getvcp 10 2&amp;gt;/dev/null | awk 'BEGIN{FS=\&amp;quot;[=,]\&amp;quot;} /current value/ {cv=$2+0; mv=$4+0; if(mv&amp;lt;=0){p=0;r=0}else{p=int((cv*100+mv/2)/mv); r=cv/mv}; if(r&amp;lt;0) r=0; if(r&amp;gt;1) r=1; printf(\&amp;quot;%d %.2f\&amp;quot;, p, r)}'); $osdclient --custom-icon display-brightness --custom-progress-text \&amp;quot;Brightness: $P%\&amp;quot; --custom-progress \&amp;quot;$R\&amp;quot;&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bindeld = , XF86MonBrightnessDown, HDMI Brightness down, exec, bash -lc &amp;quot;~/bin/hdmi-brightness lower; read P R &amp;lt;&amp;lt;&amp;lt; $(ddcutil getvcp 10 2&amp;gt;/dev/null | awk 'BEGIN{FS=\&amp;quot;[=,]\&amp;quot;} /current value/ {cv=$2+0; mv=$4+0; if(mv&amp;lt;=0){p=0;r=0}else{p=int((cv*100+mv/2)/mv); r=cv/mv}; if(r&amp;lt;0) r=0; if(r&amp;gt;1) r=1; printf(\&amp;quot;%d %.2f\&amp;quot;, p, r)}'); $osdclient --custom-icon display-brightness --custom-progress-text \&amp;quot;Brightness: $P%\&amp;quot; --custom-progress \&amp;quot;$R\&amp;quot;&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bindeld = ALT, F2, HDMI Brightness up, exec, [same as above with raise]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bindeld = ALT, F1, HDMI Brightness down, exec, [same as above with lower]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$osdclient comes from Omarchy’s defaults and shows OSD on the focused monitor:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;swayosd-client --monitor &amp;quot;$(hyprctl monitors -j | jq -r '.[] | select(.focused == true).name')&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="osd-integration"&gt;OSD Integration&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ddcutil getvcp 10 returns a line containing “current value” and “maximum value”.&lt;/li&gt;
&lt;li&gt;We parse it to get:
&lt;ul&gt;
&lt;li&gt;P (percent, e.g., 72)&lt;/li&gt;
&lt;li&gt;R (ratio 0.00–1.00 for a progress bar)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;SwayOSD’s --custom-progress shows a clean bar without changing any backlight devices:
&lt;ul&gt;
&lt;li&gt;--custom-progress &amp;quot;$R&amp;quot;&lt;/li&gt;
&lt;li&gt;--custom-progress-text &amp;quot;Brightness: $P%&amp;quot;&lt;/li&gt;
&lt;li&gt;--custom-icon display-brightness&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="testing"&gt;Testing&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Ensure the script is executable: chmod +x ~/bin/hdmi-brightness&lt;/li&gt;
&lt;li&gt;Reload Hyprland: hyprctl reload&lt;/li&gt;
&lt;li&gt;Press brightness keys and Alt+F1/F2:
&lt;ul&gt;
&lt;li&gt;External monitor brightness changes (DDC/CI)&lt;/li&gt;
&lt;li&gt;OSD shows an accurate bar and percentage&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;If no OSD appears:
&lt;ul&gt;
&lt;li&gt;systemctl --user enable --now swayosd&lt;/li&gt;
&lt;li&gt;Keep a window on the monitor you want the OSD (defaults to focused)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="troubleshooting"&gt;Troubleshooting&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;No brightness change: confirm ddcutil works (ddcutil detect, ddcutil getvcp 10), user in i2c group, and i2c-dev is loaded.&lt;/li&gt;
&lt;li&gt;Different keycodes: use wev to check actual keysyms and update the binds accordingly.&lt;/li&gt;
&lt;li&gt;OSD on the wrong output: we can pin $osdclient to a specific monitor name (e.g., --monitor &amp;quot;HDMI-A-1&amp;quot;).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="why-this-works"&gt;Why This Works&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;It replaces backlight-centric controls with DDC/CI, which external monitors use.&lt;/li&gt;
&lt;li&gt;The OSD is decoupled from any system backlight and directly reflects DDC/CI state, so it’s always accurate.&lt;/li&gt;
&lt;/ul&gt;
</content>
		<summary>&lt;p&gt;As a follow-up to the previous entry on how to get the &lt;a href="https://ewinnington.github.io/posts/omarchy-brightness"&gt;brightness adapted by ddcutil&lt;/a&gt;, I actually asked &lt;a href="https://openai.com/codex/"&gt;OpenAI's Codex&lt;/a&gt; to wire it up in my keyboard bindings. Once it succeeded and get the osd wired up, I asked it to document the process. Here is the Codex generated documentation.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://ewinnington.github.io/posts/omarchy-brightness</id>
		<title>Omarchy - Setting screen brightness over HDMI by terminal</title>
		<link href="http://ewinnington.github.io/posts/omarchy-brightness" />
		<updated>2025-09-10T00:00:00Z</updated>
		<content>&lt;h1 id="omarchy"&gt;Omarchy&lt;/h1&gt;
&lt;p&gt;With the coming arrival of the end of Windows10, I installed &lt;a href="https://omarchy.org/"&gt;Omarchy&lt;/a&gt; on one my &lt;a href="https://www.bee-link.com/products/beelink-mini-s12-n95"&gt;Beelink MiniS12 N95&lt;/a&gt;, fully expecting just to play with it and revert back to Windows11 on the machine. Win11 was slow on the machine, but a decent cheap desktop to have connected to a screen. &lt;em&gt;Omarchy&lt;/em&gt; on the Beelink, even on the tiny hardware, has been absolutely flying. To the point that it's now my main desktop for everything right now.&lt;/p&gt;
&lt;p&gt;One thing I needed to do is to handle the brightness of the screen, from the command line, so I could toggle the screen brightness from the command line. I discovered I could use &lt;code&gt;ddcutil&lt;/code&gt; which I installed using the package manager on Omarchy.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo ddcutil detect
Display 1
   I2C bus:  /dev/i2c-0
   DRM_connector:           card1-HDMI-A-2
   EDID synopsis:
      Mfg id:               BNQ - UNK
      Model:                BenQ EL2870U
      Product code:         31049  (0x7949)
      Serial number:        58M02252SL0
      Binary serial number: 21573 (0x00005445)
      Manufacture year:     2021,  Week: 34
   VCP version:         2.2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;My screen is connected by HDMI, do I was able to get the information on it.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;~ ❯ sudo usermod -aG i2c $USER
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So I didn't want to sudo all the time for my screen display information, I added my user the control of the i2c bus. This could be a security weakening, there's other ways to do it, but for my case it's fine.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;~ ❯ ddcutil detect
Display 1
   I2C bus:  /dev/i2c-0
   DRM_connector:           card1-HDMI-A-2
   EDID synopsis:
      Mfg id:               BNQ - UNK
      Model:                BenQ EL2870U
      Product code:         31049  (0x7949)
      Serial number:        58M02252SL0
      Binary serial number: 21573 (0x00005445)
      Manufacture year:     2021,  Week: 34
   VCP version:         2.2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then getting and setting the brightness was done with the following commands: &lt;code&gt;ddcutil getvcp 10&lt;/code&gt; and &lt;code&gt;ddcutil setvcp 10 20&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;~ ❯ ddcutil getvcp 10
VCP code 0x10 (Brightness                    ): current value =    40, max value =   100

~ ❯ ddcutil setvcp 10 20
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To get a listing of what codes the screen supports, you can use &lt;code&gt;ddcutil capabilities&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ddcutil capabilities
Model: EL2870U
MCCS version: 2.2
Commands:
   Op Code: 01 (VCP Request)
   Op Code: 02 (VCP Response)
   Op Code: 03 (VCP Set)
   Op Code: 07 (Timing Request)
   Op Code: 0C (Save Settings)
   Op Code: E3 (Capabilities Reply)
   Op Code: F3 (Capabilities Request)
VCP Features:
   Feature: 02 (New control value)
   Feature: 04 (Restore factory defaults)
   Feature: 05 (Restore factory brightness/contrast defaults)
   Feature: 08 (Restore color defaults)
   Feature: 0B (Color temperature increment)
   Feature: 0C (Color temperature request)
   Feature: 10 (Brightness)
   Feature: 12 (Contrast)
   Feature: 14 (Select color preset)
      Values:
         04: 5000 K
         05: 6500 K
         08: 9300 K
         0b: User 1
   Feature: 16 (Video gain: Red)
   Feature: 18 (Video gain: Green)
   Feature: 1A (Video gain: Blue)
   Feature: 52 (Active control)
   Feature: 60 (Input Source)
      Values:
         0f: DisplayPort-1
         11: HDMI-1
         12: HDMI-2
   Feature: 62 (Audio speaker volume)
   Feature: 72 (Gamma)
      Invalid gamma descriptor: 50 64 78 8c a0
   Feature: 7D (Unrecognized feature)
      Values: 00 01 02 (interpretation unavailable)
   Feature: 7E (Trapezoid)
      Values: 03 0F 10 11 12 (interpretation unavailable)
   Feature: 7F (Unrecognized feature)
   Feature: 80 (Keystone)
      Values: 01 02 03 (interpretation unavailable)
   Feature: 86 (Display Scaling)
      Values:
         01: No scaling
         02: Max image, no aspect ration distortion
         05: Max vertical image with aspect ratio distortion
         0c: Unrecognized value
         10: Unrecognized value
         11: Unrecognized value
         13: Unrecognized value
         14: Unrecognized value
         15: Unrecognized value
         16: Unrecognized value
         17: Unrecognized value
   Feature: 87 (Sharpness)
   Feature: 8D (Audio mute/Screen blank)
      Values: 01 02 (interpretation unavailable)
   Feature: AC (Horizontal frequency)
   Feature: AE (Vertical frequency)
   Feature: B2 (Flat panel sub-pixel layout)
   Feature: B6 (Display technology type)
   Feature: C0 (Display usage time)
   Feature: C6 (Application enable key)
   Feature: C8 (Display controller type)
   Feature: C9 (Display firmware level)
   Feature: CA (OSD/Button Control)
      Values:
         01: OSD disabled, button events enabled
         02: OSD enabled, button events enabled
   Feature: CC (OSD Language)
      Values:
         01: Chinese (traditional, Hantai)
         02: English
         03: French
         04: German
         05: Italian
         06: Japanese
         07: Korean
         09: Russian
         0a: Spanish
         0b: Swedish
         0d: Chinese (simplified / Kantai)
         0e: Portuguese (Brazil)
         0f: Arabic
         12: Czech
         14: Dutch
         1a: Hungarian
         1e: Polish
         1f: Romanian
   Feature: D6 (Power mode)
      Values:
         01: DPM: On,  DPMS: Off
         05: Write only value to turn off display
   Feature: DA (Scan mode)
      Values:
         00: Normal operation
         02: Overscan
   Feature: DC (Display Mode)
      Values:
         04: User defined
         05: Games
         0b: Unrecognized value
         0c: Unrecognized value
         0e: Unrecognized value
         0f: Unrecognized value
         12: Unrecognized value
         13: Unrecognized value
         21: Unrecognized value
   Feature: DF (VCP Version)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This was what my BenQ exposes.&lt;/p&gt;
&lt;h2 id="ease-of-use"&gt;Ease of use&lt;/h2&gt;
&lt;p&gt;Discovered that ddcutil allows relative up downs:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;~ ❯ ddcutil setvcp 10 + 10
~ ❯ ddcutil setvcp 10 - 10
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="next-steps"&gt;Next steps&lt;/h2&gt;
&lt;p&gt;I need to figure out how to wire up these commands to the brightness up / down key commands on Omarchy - so I can control the brightness on the keyboard. Still not sure how to get that configuration working, since it doesn't work out of the box with my screen with the default tooling.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;With the coming arrival of the end of Windows10, I installed &lt;a href="https://omarchy.org/"&gt;Omarchy&lt;/a&gt; on one my &lt;a href="https://www.bee-link.com/products/beelink-mini-s12-n95"&gt;Beelink MiniS12 N95&lt;/a&gt;, fully expecting just to play with it and revert back to Windows11 on the machine. Win11 was slow on the machine, but a decent cheap desktop to have connected to a screen. &lt;em&gt;Omarchy&lt;/em&gt; on the Beelink, even on the tiny hardware, has been absolutely flying. To the point that it's now my main desktop for everything right now.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://ewinnington.github.io/posts/MIQP</id>
		<title>It’s a proportional allocation - how hard can it be? Going from Water filling to QP</title>
		<link href="http://ewinnington.github.io/posts/MIQP" />
		<updated>2025-04-16T10:00:00Z</updated>
		<content>&lt;h1 id="its-a-proportional-allocation-how-hard-can-it-be-miqp"&gt;It’s a proportional allocation - how hard can it be? MIQP&lt;/h1&gt;
&lt;p&gt;Alice, Bob and Charlie buy a pizza and they each put down a part of the price, respectively 50%, 30% and 20%. Pizza arrives and they slice it up and eat it. But Alice gets full after eating 40% of her 50% slice, how do we allocate the remaining 10% slice to Bob and Charlie? Her 10% can be cut up into 2% slices and we proportionally give 3 to Bob (total 36%) and 2 to Charlie (total 24%).&lt;/p&gt;
&lt;p&gt;We have a total allocation of 100 MW of power to allocate to three products, in the ideal allocation with (0.5, 0.3, 0.2) ratio and each product has a maximum of 40MW.&lt;/p&gt;
&lt;p&gt;These two problems are the same.&lt;/p&gt;
&lt;h2 id="waterfilling-algorithm"&gt;Waterfilling algorithm&lt;/h2&gt;
&lt;p&gt;There exists a well known algorithm for solving this problem, the water filling algorithm.&lt;/p&gt;
&lt;p&gt;In essence, we are looking at finding the level of water across three containers that is flat along the allocation amount.&lt;/p&gt;
&lt;h3 id="definitions"&gt;definitions:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(w_i\)&lt;/span&gt; is the weight from product i&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(w'_i\)&lt;/span&gt; is the updated weight when we have fewer products due to saturation&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(M_i\)&lt;/span&gt; is the maximum of product i&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(x'_i\)&lt;/span&gt; is the initial / ideal allocation in case the products are not saturated&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(x_i\)&lt;/span&gt; is the current allocation to the product&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(T\)&lt;/span&gt; is total allocation amount to spread over the products&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(T_r\)&lt;/span&gt; is the remaining allocation to spread over products after the saturated products are removed from T&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(saturated\)&lt;/span&gt; : means that the allocation is &amp;gt;= max on the product.&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(unsaturated\)&lt;/span&gt; : means that the allocation is &amp;lt; max on the product&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="water-filling-iterative-algorithm"&gt;Water-Filling (Iterative) Algorithm:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Step 1: Compute the ideal allocations &lt;span class="math"&gt;\(x'_i = w_i \cdot T\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Step 2: For any product i for which &lt;span class="math"&gt;\(x'_i &amp;gt;= M_i\)&lt;/span&gt; (saturated), set &lt;span class="math"&gt;\(x_i =M_i\)&lt;/span&gt;, otherwise &lt;span class="math"&gt;\(x_i = x'_i\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Step 3: Compute the remaining capacity by removing the capacity of saturated &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; : &lt;span class="math"&gt;\(T_r = T − \sum_{i_{\text{saturated}}} M_i\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Step 4: For the remaining (unsaturated) products, redistribute &lt;span class="math"&gt;\(T_r\)&lt;/span&gt; proportionally based on their weights normalized over the unsaturated set &lt;span class="math"&gt;\(x_i = w^{\prime}_i \cdot T_r\)&lt;/span&gt; where &lt;span class="math"&gt;\(w^{\prime}_i = \frac{w_i}{\sum_j w_j}\)&lt;/span&gt; with &lt;span class="math"&gt;\(j\)&lt;/span&gt; representing the unsaturated products&lt;/li&gt;
&lt;li&gt;Step 5: Repeat the process if additional products get saturated during the redistribution, ie. from Step 2.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The reason this terminates is that because we remove saturated products from the list, the next products get allocated and the set gets reduced, either a new product is saturated and the cycle continues or the final allocation is done and terminates.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import numpy as np

def iterative_waterfilling(T, weights, max_allocations):
    n = len(weights)
    allocations = np.zeros(n)
    unsaturated = np.array([True] * n)
    remaining_T = T

    while True:
        # Calculate proportional weights for unsaturated products
        current_weights = np.array(weights) * unsaturated
        total_current_weight = np.sum(current_weights)
        
        # Calculate ideal allocation for unsaturated products
        ideal_allocations = (current_weights / total_current_weight) * remaining_T

        # Check for saturation
        newly_saturated = ideal_allocations &amp;gt;= max_allocations
        
        # Update allocations and saturation status
        if not np.any(newly_saturated &amp;amp; unsaturated):
            allocations[unsaturated] = ideal_allocations[unsaturated]
            break
        
        for i in range(n):
            if unsaturated[i] and newly_saturated[i]:
                allocations[i] = max_allocations[i]
                unsaturated[i] = False
                remaining_T -= allocations[i]

    return allocations

T = 100
weights = [0.5, 0.3, 0.2]
max_allocations = [40, 40, 40]

allocations_result = iterative_waterfilling(T, weights, max_allocations)
print(&amp;quot;Iterative Waterfilling Result:&amp;quot;, allocations_result)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="examples"&gt;Examples&lt;/h3&gt;
&lt;p&gt;Example 1 - total 100, allocation (0.5,0.3,0.2), maximum (40,40,40), result (40,36,24)&lt;/p&gt;
&lt;p&gt;Example 2 - total 100, allocation (0.5,0.3,0.2), maximum (40,34,40), result (40,34,26)&lt;/p&gt;
&lt;p&gt;The problem of this algorithm is that while it works wonderfully for simple proportional problems, as soon as you start adding more constraints (minimums) and relations between the allocations, this iterative algorithm doesn’t work that great.&lt;/p&gt;
&lt;p&gt;So how do we solve this as an optimization problem? We want to find a solution that when unconstrained (unsaturated) falls back to the proportional allocation and when constrained (saturated maximum) falls back to the waterfilling algorithm.&lt;/p&gt;
&lt;h2 id="from-waterfilling-to-qp-mixed-integer-quadratic-programming"&gt;From Waterfilling to QP : Mixed integer quadratic programming&lt;/h2&gt;
&lt;p&gt;While the iterative waterfilling algorithm effectively solves basic proportional allocation problems, it struggles under more complex scenarios involving additional constraints, such as minimum allocation limits or relational constraints between allocations. To robustly handle these real-world complexities, we leverage Mixed Integer Quadratic Programming (MIQP). MIQP elegantly generalizes the waterfilling logic into an optimization framework, allowing precise specification of constraints and objectives. By translating allocation decisions into a mathematical optimization problem, we ensure optimal, constraint-respecting allocations, making it suitable for applications demanding reliability and flexibility.&lt;/p&gt;
&lt;h3 id="definitions-1"&gt;Definitions:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(T\)&lt;/span&gt;: total&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(w_i\)&lt;/span&gt;: weight&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(M_i\)&lt;/span&gt;: Maximum of allocation&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(d_i\)&lt;/span&gt;: product saturated marker ($\in \mathbb, binary \in \lbrace 0,1 \rbrace $), 0 unsaturated, 1 saturated&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(x_i\)&lt;/span&gt;: allocation amount&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(v_i\)&lt;/span&gt;: target water level for unsaturated product (shared identity)&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(U\)&lt;/span&gt;: upper large bound&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="objective"&gt;Objective:&lt;/h3&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\min \sum_i (x_i - w_i T)^2\)&lt;/span&gt;&lt;/p&gt;
&lt;h3 id="constraints"&gt;Constraints:&lt;/h3&gt;
&lt;h4 id="i-basic-allocation-limits"&gt;(I) Basic allocation limits&lt;/h4&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\forall i \quad x_i \geq 0 \quad (a)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\forall i \quad x_i \leq M_i \quad (b)\)&lt;/span&gt;&lt;/p&gt;
&lt;h4 id="ii-total-allocation-constraint"&gt;(II) Total allocation constraint&lt;/h4&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\sum_i x_i \leq T\)&lt;/span&gt;&lt;/p&gt;
&lt;h4 id="iii-saturation-constraints"&gt;(III) Saturation constraints &lt;span class="math"&gt;\(\forall i\)&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;&lt;span class="math"&gt;\(x_i = w_i \cdot v_i + M_i \cdot d_i \quad (a)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(v_i \leq U \cdot (1 - d_i) \quad (b) \quad \text{with } v_i = 0 \text{ when saturated}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(v_i \geq 0 \quad (c)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(v_i \leq \frac{M_i}{w_i} \cdot (1 - d_i) + U \cdot d_i \quad (d)\)&lt;/span&gt;&lt;/p&gt;
&lt;h4 id="iv-water-level-equality-constraints-across-unsaturated-products"&gt;(IV) Water level equality constraints across unsaturated products&lt;/h4&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\forall i (1 \rightarrow n), \quad \forall j (i+1 \rightarrow n)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(v_i - v_j \leq U \cdot (d_i + d_j)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(v_j - v_i \leq U \cdot (d_i + d_j)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;If both are unsaturated, it means &lt;span class="math"&gt;\(d_i = d_j = 0\)&lt;/span&gt;, thus forcing &lt;span class="math"&gt;\(v_i = v_j\)&lt;/span&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;h3 id="explanation-of"&gt;Explanation of &lt;span class="math"&gt;\(v_i\)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;This set of equations sets up &lt;span class="math"&gt;\(v_i\)&lt;/span&gt; as a shared value &lt;span class="math"&gt;\(z\)&lt;/span&gt; across all unsaturated products.&lt;/p&gt;
&lt;p&gt;$$40 + (w_1 + w_2) \cdot z = 100$$&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\((0.3 + 0.2) z = 60\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(z = 120\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(x_1 = 0.3 \cdot 120 = 36\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(x_2 = 0.2 \cdot 120 = 24\)&lt;/span&gt;&lt;/p&gt;
&lt;h3 id="implementation-in-python"&gt;Implementation in Python&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;#pip install numpy
#pip install cvxpy
#pip install ecos

import cvxpy as cp
import numpy as np

# Problem parameters
T = 100.0
w = [0.5, 0.3, 0.2]          # target weights for products 1, 2, and 3
M_max = [40.0, 40.0, 40.0]     # maximum allocation for each product

# Number of products
n = len(w)

# A sufficiently large constant U (big-M) for enforcing water-level equality.
U = 500.0

# Decision variables:
# x: allocation amounts
x = cp.Variable(n)
# v: auxiliary &amp;quot;water-level&amp;quot; variables for unsaturated products
v = cp.Variable(n)
# delta: binary variables; delta[i] = 1 means product i is saturated (x[i] = M_max[i])
delta = cp.Variable(n, boolean=True)

constraints = []

# For each product, define the allocation as the sum of the unsaturated part and the saturation term.
for i in range(n):
    # If not saturated (delta[i] = 0) then x[i] = w[i]*v[i].
    # If saturated (delta[i] = 1) then x[i] = M_max[i].
    constraints.append(x[i] == w[i] * v[i] + M_max[i] * delta[i])
    
    # Force v[i] = 0 when saturated, by bounding v[i] to 0 when delta[i]=1.
    constraints.append(v[i] &amp;lt;= U * (1 - delta[i]))
    # Ensure nonnegativity of v.
    constraints.append(v[i] &amp;gt;= 0)
    # When unsaturated (delta[i]=0), we must have x[i] = w[i]*v[i] ≤ M_max[i]. 
    # Throught: Why not T here instead of M_max[i]? -&amp;gt; it would be correct, but M_max[i] is a more restrictive boundary so helps convergence
    constraints.append(v[i] &amp;lt;= (M_max[i] / w[i]) * (1 - delta[i]) + U * delta[i])

# Enforce that all unsaturated products share the same water-level.
# For every pair (i,j), if both are unsaturated (delta[i] = delta[j] = 0) then v[i] must equal v[j].
for i in range(n):
    for j in range(i+1, n):
        constraints.append(v[i] - v[j] &amp;lt;= U * (delta[i] + delta[j]))
        constraints.append(v[j] - v[i] &amp;lt;= U * (delta[i] + delta[j]))

# Total allocation constraint: the sum of all allocations must equal the available capacity.
constraints.append(cp.sum(x) == T)

# Ensure each x[i] does not exceed its maximum. (These could be built in via the definition of x.)
for i in range(n):
    constraints.append(x[i] &amp;gt;= 0)
    constraints.append(x[i] &amp;lt;= M_max[i])

# Define the target allocation for each product (unconstrained ideals)
target = np.array([w_i * T for w_i in w])

# Objective: minimize squared deviation from the ideal allocation.
objective = cp.Minimize(cp.sum_squares(x - target))

# Define and solve the MIQP.
# Use a MIQP-capable solver such as GUROBI, CPLEX, ECOS_BB, etc.
prob = cp.Problem(objective, constraints)
result = prob.solve(solver=cp.ECOS_BB)


print(&amp;quot;Status:&amp;quot;, prob.status)
print(&amp;quot;Optimal value:&amp;quot;, result)
print(&amp;quot;Optimal allocations:&amp;quot;)
for i in range(n):
    print(f&amp;quot;  x[{i+1}] = {x.value[i]:.4f}&amp;quot;)
print(&amp;quot;Water-level (v) values:&amp;quot;)
for i in range(n):
    print(f&amp;quot;  v[{i+1}] = {v.value[i]:.4f}&amp;quot;)
print(&amp;quot;Saturation indicators (delta) values:&amp;quot;)
for i in range(n):
    print(f&amp;quot;  delta[{i+1}] = {delta.value[i]:.4f}&amp;quot;)

# Expected behavior for this example:
# - For product 1, the unconstrained target is 50 but M_max[0]=40, so we expect it to be saturated (delta[0]=1, x[0]=40).
# - For products 2 and 3 (unsaturated, delta = 0), they share the same water-level z.
#   The total allocation constraint becomes: 40 + (w[1] + w[2]) * z = 100  --&amp;gt;  (0.3+0.2)*z = 60, so z = 120.
#   Hence, x[2] = 0.3 * 120 = 36 and x[3] = 0.2 * 120 = 24.
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="summary"&gt;Summary&lt;/h2&gt;
&lt;p&gt;In business applications, we often see allocations that should be &amp;quot;preference based&amp;quot; if unconstrained, but then with different constraints it becomes complicated to tease out the preferences in an optimal way. This QP application shows a way to have an allocation identical to the water filling, but with the flexibility of QP.&lt;/p&gt;
&lt;p&gt;If you were simplifying the QP constraints to remove the equations (IV), the output would be (40, 35, 25) since that minimises the objective function.&lt;/p&gt;
&lt;p&gt;This example here is taken out of an abstration of a practical problem of allocating ancillary service capacity to a series of contracts, with trader preferences. This chapter is part of my book &amp;quot;Energy - From Asset to Cashflow&amp;quot; (not yet published).&lt;/p&gt;
</content>
		<summary>&lt;p&gt;Alice, Bob and Charlie buy a pizza and they each put down a part of the price, respectively 50%, 30% and 20%. Pizza arrives and they slice it up and eat it. But Alice gets full after eating 40% of her 50% slice, how do we allocate the remaining 10% slice to Bob and Charlie? Her 10% can be cut up into 2% slices and we proportionally give 3 to Bob (total 36%) and 2 to Charlie (total 24%).&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://ewinnington.github.io/posts/SimplePerf3</id>
		<title>In-memory software design 2025 ? - from 40GB/s to 55GB/s</title>
		<link href="http://ewinnington.github.io/posts/SimplePerf3" />
		<updated>2025-02-16T08:00:00Z</updated>
		<content>&lt;h1 id="in-memory-software-design-in-2025-from-40gbs-to-55gbs"&gt;In-memory software design in 2025 - from 40GB/s to 55GB/s&lt;/h1&gt;
&lt;p&gt;In the last blog, we looked at techniques for improving when we were doing partial sums and to reduce the scanned trades we used in-memory indexes using sets.&lt;/p&gt;
&lt;p&gt;But what if we simply &lt;em&gt;want to go faster&lt;/em&gt; for the complete aggregation?&lt;/p&gt;
&lt;p&gt;Up to now, we've been programming &lt;strong&gt;for the programmer&lt;/strong&gt; making the program easy to write and understand, but what if we make the program &lt;strong&gt;easy to run for the cpu&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;How do we do that? By improving memory locality and structures.&lt;/p&gt;
&lt;h2 id="memory-locality"&gt;Memory locality&lt;/h2&gt;
&lt;p&gt;The initial implementation uses a &lt;code&gt;std::vector&amp;lt;Trade&amp;gt; trades;&lt;/code&gt; with each trade maintaining a &lt;code&gt;std::vector&amp;lt;DailyDelivery&amp;gt; dailyDeliveries;&lt;/code&gt; that contains the two vectors of power and value &lt;code&gt;std::array&amp;lt;int, 100&amp;gt; power; std::array&amp;lt;int, 100&amp;gt; value;&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/in-mem/TradeInMemory.png" class="img-fluid" width="80%" alt="TradeInMemory" /&gt;&lt;/p&gt;
&lt;p&gt;While &lt;code&gt;vector&lt;/code&gt; does a great job at trying to get the memory allocated to it to be continuous in cpp, when you nest vectors in vectors and you allocate the sub-vectors, you are creating fragmented memory. This means the CPU's memory controller has to jump around a lot to get the next bloc and there's a high probability that adjacent will not be in the cache for the other cores of the CPU to be able to use.&lt;/p&gt;
&lt;p&gt;The best architecture for locality will be always dependent on the access patters to the data. In our case here, we are going to optimize for maximum speed when doing complete aggregations.&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/in-mem/TradeCompressed.png" class="img-fluid" width="80%" alt="TradeCompressed" /&gt;&lt;/p&gt;
&lt;p&gt;This is done by creating an specific area for the Power and Value data of each delivery day is allocated to. Trades point to the beginning and end of this data. If you need to move inside this area, because you know exactly the start delivery date of the trade and the size of the delivery day vector (100 ints), you can immediately index into the large array.&lt;/p&gt;
&lt;p&gt;Now, if you are dealing with a small amount of trades (~50'000 with an average of 120 days of delivery ~~ 4.9 GB of RAM for 600 million quarter hours), you can get away with a single coherent bloc of RAM for your delivery vector. But see later for a more practical production discussion:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-cpp"&gt;    // Each daily delivery contributes 100 ints for power and 100 ints for value.
    std::vector&amp;lt;int&amp;gt; flatPower(totalDailyDeliveries * 100);
    std::vector&amp;lt;int&amp;gt; flatValue(totalDailyDeliveries * 100);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now with this information, we can directly index into the area based on the trades data:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-cpp"&gt;#ifdef _OPENMP
#pragma omp parallel for reduction(+: all_totalPower, all_totalValue, \
                                     traderX_totalPower, traderX_totalValue, \
                                     traderX_area1_totalPower, traderX_area1_totalValue, \
                                     traderX_area2_totalPower, traderX_area2_totalValue)
#endif
    for (std::size_t i = 0; i &amp;lt; flatTrades.size(); ++i)
    {
        const auto&amp;amp; ft = flatTrades[i];
        // Each trade's deliveries are stored in a contiguous block.
        size_t startIndex = ft.deliveriesOffset * 100;
        size_t numInts = ft.numDeliveries * 100;
        long long sumPower = 0;
        long long sumValue = 0;
        for (size_t j = 0; j &amp;lt; numInts; ++j)
        {
            sumPower += flatPower[startIndex + j];
            sumValue += flatValue[startIndex + j];
        }
        // (a) All trades
        all_totalPower += sumPower;
        all_totalValue += sumValue;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When running with this configuration, my time to aggregation on the laptop improves from &lt;strong&gt;241ms&lt;/strong&gt; to &lt;strong&gt;178ms&lt;/strong&gt; a 37% improvement in speed - which get us to &lt;strong&gt;55 GB/s&lt;/strong&gt; on a commodity laptop (using OpenMP of course).&lt;/p&gt;
&lt;h2 id="limits"&gt;Limits&lt;/h2&gt;
&lt;p&gt;But as you scale to larger and larger amounts of trades and delivery days, you will really find that the vector&lt;int&gt; allocator will not be able to handle that in a single bloc.&lt;/p&gt;
&lt;p&gt;At that point, we'll start running our own custom allocation, keeping our own block memory via a custom allocator. By creating blocks of 64 to 256MB that we allocate as needed and indexing our trade deliveries into it, we can then scale to more of the the entire memory of your machine.&lt;/p&gt;
&lt;p&gt;Two good references on that are &lt;a href="https://johnfarrier.com/custom-allocators-in-c-high-performance-memory-management/"&gt;Custom Allocators in C++: High Performance Memory Management&lt;/a&gt; and &lt;a href="https://www.youtube.com/watch?v=kSWfushlvB8"&gt;CppCon 2017: Bob Steagall “How to Write a Custom Allocator”&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="next-steps"&gt;Next steps?&lt;/h2&gt;
&lt;p&gt;Going from a simple data structure to a memory adapted structure allowed us to go from &lt;strong&gt;40GB/s&lt;/strong&gt; (42% of laptop bandwidth) to &lt;strong&gt;55GB/s&lt;/strong&gt; (57% of laptop's 96 GB/s bandwidth).&lt;/p&gt;
&lt;p&gt;If you still need more performance, you must further adapt your data structures and locality to the access patterns. Then start looking in depth at where the stalls are in the execution trace. There's other approaches such as looking at AVX instructions in more detail to find some perf, loop unrolling, and so on. Get a real cpp expert to consult on it.&lt;/p&gt;
&lt;p&gt;Or just get a machine with more bandwidth! An example of that is the M2, M4 Max and Ultra family of chips from Apple, with memory bandwidth of over 800GB/s - over 8x what my laptop has. Or just run on a server, as noted in the first article, Azure has now a machine with 6'900 GB/s of bandwidth.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;In the last blog, we looked at techniques for improving when we were doing partial sums and to reduce the scanned trades we used in-memory indexes using sets.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://ewinnington.github.io/posts/SimplePerf2</id>
		<title>In-memory software design 2025 - Applied to energy</title>
		<link href="http://ewinnington.github.io/posts/SimplePerf2" />
		<updated>2025-02-15T08:00:00Z</updated>
		<content>&lt;h1 id="in-memory-software-design"&gt;In-memory software design&lt;/h1&gt;
&lt;p&gt;In the last blog, &lt;a href="https://ewinnington.github.io/posts/SimplePerf"&gt;In-memory performance in 2025&lt;/a&gt; we looked at a simple design for an energy aggregation system to aggregate 9.8 GB of 100'000 trades rolled out and saw we got about 40 GB/s performance.&lt;/p&gt;
&lt;p&gt;I upped the numbers of trades to 500'000 trades (rolled out over quarter hours, with average trade length of 120 days in quarter hours) and validated that on a ~50 GB dataset, we are running at ~1200 ms for the complete aggregation, confirming a linear scaling of the compute time.&lt;/p&gt;
&lt;p&gt;But this brings us to a point where on commodity hardware, we are running our aggregations over 1 second in duration. So how can we improve this? This time, instead of trying to brute force, let's bring in some other techniques from databases: Indexes!&lt;/p&gt;
&lt;h2 id="simple-indexes-for-in-memory-aggregations"&gt;Simple indexes for in-memory aggregations&lt;/h2&gt;
&lt;p&gt;For our use case, we are going to look at two meta-data properties on our trades, Trader and Delivery area, but the concept scales efficiently to large collections of meta-data, because we are dealing with such a small amounts of Trades (500k).&lt;/p&gt;
&lt;h3 id="building-the-index"&gt;Building the index&lt;/h3&gt;
&lt;p&gt;We can generate at insertion a set for each meta-data, trader and delivery area.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-cpp"&gt;    // =================
    // 2) Build Indexes
    // =================
    // We'll do single-attribute indexes: trader -&amp;gt; set of tradeIDs, area -&amp;gt; set of tradeIDs
    // For large data, consider using std::vector&amp;lt;size_t&amp;gt; sorted, or some other structure.

    std::unordered_map&amp;lt;std::string, std::unordered_set&amp;lt;size_t&amp;gt;&amp;gt; traderIndex;
    traderIndex.reserve(10);  // if you know approx how many traders you have to avoid resizing continuously

    std::unordered_map&amp;lt;std::string, std::unordered_set&amp;lt;size_t&amp;gt;&amp;gt; areaIndex;
    areaIndex.reserve(10);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I'm using an unordered set, but could also use an ordered set, this might be more efficient.&lt;/p&gt;
&lt;p&gt;As keys, I'm using the string data of the meta-data on the trade - in production this would probably be integer keys, but for this use case is sufficient.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-cpp"&gt;    for (size_t i = 0; i &amp;lt; trades.size(); ++i)
    {
        const auto&amp;amp; t = trades[i];
        traderIndex[t.trader].trades(i);
        areaIndex[t.deliveryArea].trades(i);
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we insert a set of trades, we can add them to the set index.&lt;/p&gt;
&lt;p&gt;As a linear pass, it's extremely efficient to build this index and cheap to keep it updated as we insert new trades.&lt;/p&gt;
&lt;h3 id="using-the-index"&gt;Using the index&lt;/h3&gt;
&lt;p&gt;If we have a search query on a single attribute, we can now use the simple index and have directly the result.&lt;/p&gt;
&lt;p&gt;But if we are doing a query on two (or more), we are going to take use the smallest index first and match with the largest index. The unordered_set gives an acceptable performance for &lt;code&gt;bigger.find(id)&lt;/code&gt; but you can probably do even better with a set structure that is optimized for intersections. You can benchmark using &lt;code&gt;std::set_intersection&lt;/code&gt; against my simple implementation if you are using sorted sets.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-cpp"&gt;// ===================================
    // 3) Use the indexes for filtering
    // ===================================
    // For instance, let's do a query: TraderX, Area2
    // We'll find the intersection of (all trades for TraderX) and (all trades for Area2).

    std::string queryTrader = &amp;quot;TraderX&amp;quot;;
    std::string queryArea   = &amp;quot;Area2&amp;quot;;

    // Get sets from the index
    // (handle the case if the key doesn't exist -&amp;gt; empty set)
    auto itT = traderIndex.find(queryTrader);
    auto itA = areaIndex.find(queryArea);

    if (itT == traderIndex.end() || itA == areaIndex.end()) {
        std::cout &amp;lt;&amp;lt; &amp;quot;No trades found for &amp;quot; &amp;lt;&amp;lt; queryTrader &amp;lt;&amp;lt; &amp;quot; AND &amp;quot; &amp;lt;&amp;lt; queryArea &amp;lt;&amp;lt; &amp;quot;\n&amp;quot;;
        return 0;
    }

    const auto&amp;amp; traderSet = itT-&amp;gt;second;
    const auto&amp;amp; areaSet   = itA-&amp;gt;second;

    // Intersection
    // We'll create a vector of trade IDs that are in both sets
    // For speed, we can iterate over the smaller set and check membership in the larger set.
    const auto&amp;amp; smaller = (traderSet.size() &amp;lt; areaSet.size()) ? traderSet : areaSet;
    const auto&amp;amp; bigger  = (traderSet.size() &amp;lt; areaSet.size()) ? areaSet   : traderSet;

    std::vector&amp;lt;size_t&amp;gt; intersection;
    intersection.reserve(smaller.size());  // a safe upper bound if all the smaller set is selected, to avoid resizing

    for (auto id : smaller)
    {
        if (bigger.find(id) != bigger.end())
        {
            intersection.push_back(id);
        }
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This gives us the intersection and we iterate it to do the summation - again, here we are summing up over the entire year into a single value, but we could just as easily be doing daily, weekly or monthly sums.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-cpp"&gt;    // ====================================
    // 4) Aggregate deliveries for matches
    // ====================================
    // Let's sum up total power / total value for the intersection set.

    long long totalPower = 0;
    long long totalValue = 0;

    #ifdef _OPENMP
    #pragma omp parallel for num_threads(4) reduction(+:totalPower, totalValue)
    #endif
    for (auto id : intersection)
    {
        const Trade&amp;amp; t = trades[id];
        // sum up all deliveries
        for (const auto&amp;amp; dd : t.dailyDeliveries)
        {
            for (int slot = 0; slot &amp;lt; 100; ++slot)
            {
                totalPower += dd.power[slot];
                totalValue += dd.value[slot];
            }
        }
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This addition here is single threaded, but you can also use OpenMP to accelerate it - this will require some tuning, you don't want to use too many threads for these smaller aggregations, the &lt;code&gt;omp parallel for num_threads(4)&lt;/code&gt; can be added to example limit to 4 threads. (note: I added the openMP in the code above).&lt;/p&gt;
&lt;p&gt;Generally you get a 10x or more acceleration in single core, depending on how selective the indexes you are using are- In parallel, I'm getting 40-50x acceleration in multi-core with a num_threads to 4.&lt;/p&gt;
&lt;h2 id="why-arrayint100"&gt;Why array&amp;lt;int,100&amp;gt; ?&lt;/h2&gt;
&lt;p&gt;In my last post, I used an array of 100 points in a day. I'm using it because it's simplest to have all days have the same &amp;quot;size&amp;quot; for memory alignment, therefore if I am calculating days in local time I have a 25h and 23h hour day once a year. I would generally prefer to work in UTC and have constant 24h days -- but for some reason humans prefer local time so it aligns with expectations.&lt;/p&gt;
&lt;p&gt;Just be clear with the developer if you use an internal UTC or Local time representation. If using local make sure to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Properly sanitize your inputs, check trades fill the 96 quarter hours only for all days apart from the short (92) and long day (100) and zero fill the remainder.&lt;/li&gt;
&lt;li&gt;Keep summing on all 100 hours for summations, the 4% extra index length is not worth an if statement in the inner loop of the code - try to keep loops jump free.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="how-to-deal-with-canceled-amended-recalled-trades"&gt;How to deal with Canceled / Amended / Recalled trades ?&lt;/h2&gt;
&lt;p&gt;My first answer is don't! Let me explain: Normal trades should represent the 99.9% or 99.99% of your deals - unless there's something you haven't told me about the way you are trading!&lt;/p&gt;
&lt;p&gt;We can design this by having a tradeStatus on the trade.&lt;/p&gt;
&lt;p&gt;int TradeStatus:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;0 : trade is valid&lt;/li&gt;
&lt;li&gt;1 : trade is canceled&lt;/li&gt;
&lt;li&gt;2 : trade is amended (ie. replaced by a new one)&lt;/li&gt;
&lt;li&gt;... : any other status necessary&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When a trade is canceled, we leave it in the trade vector, but simply set the tradeStatus to a non-zero value, and skip it with a test at the beginning of the aggregation.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-cpp"&gt;    #ifdef _OPENMP
    #pragma omp parallel for num_threads(4) reduction(+:totalPower, totalValue)
    #endif
    for (auto id : intersection)
    {
        const Trade&amp;amp; t = trades[id];
        if(t.tradeStatus != 0) continue;  // skip canceled/amended trades
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If the trade is amended, same thing, we add a new trade to our list of trades and set the previous one to amended status. Generally, the this is not using up much memory. If it ever becomes a problem, we could:&lt;/p&gt;
&lt;ol type="a"&gt;
&lt;li&gt;have a &amp;quot;trade compression&amp;quot; which removes all non-zero trade status from the vector.&lt;/li&gt;
&lt;li&gt;flush the entire trade vector and reload the whole set.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Depending on your implementation, the flushing and reloading might be just as fast - not every program needs to stay resident in memory all the time.&lt;/p&gt;
&lt;h2 id="snapshot-state-to-disk-for-recovery-or-storage-fork-as-in-redis"&gt;Snapshot state to disk for recovery or storage - Fork() as in Redis&lt;/h2&gt;
&lt;p&gt;If we want to take snapshots of the state,  we can get inspired from Redis' famous &lt;a href="https://architecturenotes.co/i/143231289/forking"&gt;fork() snapshotting technique&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Use this when needing to snapshot a large data structure in RAM to disk (serialize the entire state), without blocking our main process from accepting new trades for the entire duration of the write.&lt;/p&gt;
&lt;h3 id="how-fork-helps"&gt;How fork() helps&lt;/h3&gt;
&lt;p&gt;On Linux, calling fork() creates a child process that initially shares the same physical memory pages as the parent.&lt;/p&gt;
&lt;p&gt;Copy-on-write (CoW): If either the parent or the child writes to a page after the fork, the kernel duplicates that page so each process sees consistent data.&lt;/p&gt;
&lt;p&gt;The child process can serialize the in-memory data (in a consistent state from the moment of forking) to disk, while the parent continues to run to accept new trades. New trades arriving in the parent process after the fork will not affect the child’s view of memory. The child effectively sees a snapshot as of the fork().&lt;/p&gt;
&lt;p&gt;You want the data structure to be in a consistent state at the instant of fork().
A brief lock (or pause writes) just before the fork() is triggered, ensuring no partial updates. Immediately after fork() returns, you can unlock, letting the parent continue. Meanwhile, the child proceeds to write out the data.&lt;/p&gt;
&lt;p&gt;We can store an atomic counter value in the program that represents the last tradeid inserted or a state version. This gives you a “version” or “stamp” number for the dataset.&lt;/p&gt;
&lt;p&gt;I won't put the full code for that here, since the design is a little more involved, but the basics are:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;static std::atomic&amp;lt;long&amp;gt; g_version{0}; //snapshot version id 
static std::mutex g_tradeMutex;  // protect g_trades from concurrent modification - lock on write to 

// ---------------------------------------------------
// fork() to create a child that writes the snapshot
// ---------------------------------------------------
int snapshotNow(const char* filename) {
    // 1) Acquire short lock to ensure no partial updates in progress
    g_tradeMutex.lock();
    long snapVer = g_version.load(std::memory_order_relaxed);

    // 2) Fork
    pid_t pid = fork();
    if(pid &amp;lt; 0) {
        // error
        std::cerr &amp;lt;&amp;lt; &amp;quot;fork() failed\n&amp;quot;;
        g_tradeMutex.unlock();
        return -1;
    }

    if(pid == 0) {
        // child
        // We have a consistent view of memory as of the fork.
        // release the lock in the child
        g_tradeMutex.unlock();

        // write the snapshot
        writeSnapshotToDisk(filename, snapVer);

        // exit child
        _exit(0);
    } else {
        // parent
        // release the lock and continue
        g_tradeMutex.unlock();
        std::cout &amp;lt;&amp;lt; &amp;quot;[Parent] Snapshot child pid=&amp;quot; &amp;lt;&amp;lt; pid 
                  &amp;lt;&amp;lt; &amp;quot;, version=&amp;quot; &amp;lt;&amp;lt; snapVer &amp;lt;&amp;lt; &amp;quot;\n&amp;quot;;
        return 0;
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In essence we have two processes continuing from the same command fork() return, each taking one branch.&lt;/p&gt;
&lt;h2 id="data-io"&gt;Data I/O&lt;/h2&gt;
&lt;p&gt;To integrate your cpp aggregation software into the rest of your stack depends on the software running around it.&lt;/p&gt;
&lt;p&gt;You can run the application as an on-demand aggregation, loading everything to memory, doing the aggregation and exiting - leaving the server to do something else - this can be worth it if you only do aggregations on-demand and can afford the load time of a second or two from your NVME storage.&lt;/p&gt;
&lt;p&gt;You can keep the cpp program running either :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;exposing an http RestAPI (&lt;a href="https://github.com/microsoft/cpprestsdk"&gt;https://github.com/microsoft/cpprestsdk&lt;/a&gt; is a good library for that, I've used it before).&lt;/li&gt;
&lt;li&gt;having a GRPC endpoint for performance&lt;/li&gt;
&lt;li&gt;receiving data from a Kafka stream - I'm sure Confluent can give you a good example of that.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="summary"&gt;Summary&lt;/h2&gt;
&lt;p&gt;In-memory data aggregation using cpp is relatively easy to write and maintain. Cpp is no longer the terrible monster it was - auto pointers help and using std:: components makes everything simple. OpenMP is an easy win to add to compute or memory intensive sections.&lt;/p&gt;
&lt;h4 id="sidenotes"&gt;Sidenotes&lt;/h4&gt;
&lt;p&gt;On Windows you can get everything you need to compile cpp by installing:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;winget install LLVM.LLVM
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On Linux (or wsl), you need to install the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo apt-get update
sudo apt-get install clang
sudo apt-get install libomp-dev 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then in both os, you can usually run a compilation on your source file (inmem_agg_omp.cpp) using:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;clang++ -O3 -march=native -flto -ffast-math -fopenmp -o inmem_agg_omp inmem_agg_omp.cpp
&lt;/code&gt;&lt;/pre&gt;
</content>
		<summary>&lt;p&gt;In the last blog, &lt;a href="https://ewinnington.github.io/posts/SimplePerf"&gt;In-memory performance in 2025&lt;/a&gt; we looked at a simple design for an energy aggregation system to aggregate 9.8 GB of 100'000 trades rolled out and saw we got about 40 GB/s performance.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://ewinnington.github.io/posts/SimplePerf</id>
		<title>Performance of in-memory in 2025</title>
		<link href="http://ewinnington.github.io/posts/SimplePerf" />
		<updated>2025-02-13T08:00:00Z</updated>
		<content>&lt;h1 id="performance-of-in-memory-software-in-2025"&gt;Performance of in-memory software in 2025&lt;/h1&gt;
&lt;p&gt;Yesterday, at an Energy panel in Essen, I mentioned that some heavy calculations should be done in-memory. It is something that people have a tendency to dismiss because generally they are not aware of the capability and speed of modern CPUs and RAM. A 32GB dataset in memory can now be processed every second by a commodity cpu in your laptop.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://tailscale.com/blog/living-in-the-future"&gt;Living in the Future, by the numbers&lt;/a&gt; is a great article on the progress we have had since 2004:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CPU Compute is 1000x faster&lt;/li&gt;
&lt;li&gt;Web servers are 100x faster&lt;/li&gt;
&lt;li&gt;Ram is 16x to 750x larger&lt;/li&gt;
&lt;li&gt;SSD can do 10'000x more transactions per second.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can also see this progression on &lt;a href="https://techcommunity.microsoft.com/blog/azurehighperformancecomputingblog/announcing-azure-hbv5-virtual-machines-a-breakthrough-in-memory-bandwidth-for-hp/4303504"&gt;Azure with the high-compute servers&lt;/a&gt;, Microsoft and AMD are packing so much more memory bandwidth in modern compute.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/b7561026-767e-4dc4-97a5-e5306c3fa36a" class="img-fluid" width="60%" alt="image" /&gt;&lt;/p&gt;
&lt;p&gt;We are going to have 7 TB/s of memory bandwidth!&lt;/p&gt;
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/f2c78f96-0eb0-4359-9f65-963b2b4b4f7b" class="img-fluid" width="60%" alt="image" /&gt;&lt;/p&gt;
&lt;p&gt;You can now run super-computer level problems on Azure!&lt;/p&gt;
&lt;p&gt;But what does that all mean? What can we do even on a commodity laptop? I have a Latitude 9440 laptop on my desk here, with a 13th gen i7-1365U with 32 GB of RAM.&lt;/p&gt;
&lt;h2 id="energy-trade-aggregation"&gt;Energy Trade aggregation&lt;/h2&gt;
&lt;p&gt;Let's start with a small calculation from the world of Energy. I have 100'000 trades, these trades affect one or multiple quarter hours of one year (8'784 hours =&amp;gt; 35'136 quarter hours).&lt;/p&gt;
&lt;p&gt;Pulling out a little C++, completely unoptimized, how long does it take to aggregate them and how much RAM is used?&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-cpp"&gt;// A struct to hold the daily delivery arrays (power, value).
struct DailyDelivery
{
    int dayOfYear;  // 1..365
    std::array&amp;lt;int, 100&amp;gt; power; 
    std::array&amp;lt;int, 100&amp;gt; value; 
};

// A struct to hold the trade metadata.
struct Trade
{
    int tradeId;
    std::string trader;        // e.g. &amp;quot;TraderX&amp;quot;
    std::string deliveryArea;  // e.g. &amp;quot;Area1&amp;quot;, &amp;quot;Area2&amp;quot;
    // You could store time points, but we'll just store day indexes for simplicity.
    // Real code might store start_delivery, end_delivery as std::chrono::system_clock::time_point.
    int startDay; // 1..365
    int endDay;   // 1..365

    std::vector&amp;lt;DailyDelivery&amp;gt; dailyDeliveries;
};
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Daily Delivery structure represents a day of delivery, with 100 slots (for the 25h day =&amp;gt; 100 quarter hours).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I'm storing the delivery of power in kW as an int32, meaning in a single trade I can do –2'147'483'648 kW to 2'147'483'647 kW.&lt;/li&gt;
&lt;li&gt;Same thing for the value, we store the individual value of the MW in milli values (decimal shift 3), so each MW could be priced at -2'147'483.648 € to 2'147'483.647 €.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The Trade stores:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;metadata: Trader and DeliveryArea. We could add as many metadata elements as we need, but for simplicity in the demo, I only use this&lt;/li&gt;
&lt;li&gt;a dailyDeliveries vector containing the array of all days affected by the trade.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now if we wanted to see what is the total sum of power of all trades, the sum of TraderX and the sum of TraderX's deals in Area1 and Area2, we can runn the aggregation over all the memory. This is completely straight forward code, no optimizations what so ever.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-cpp"&gt;    // --------------------------------------
    // 2) Run the aggregations (measure time)
    // --------------------------------------

    // The aggregates we want:
    // (a) All Trader total (yearly - all zones)
    // (b) Trader X total
    // (c) Trader X / Area1
    // (d) Trader X / Area2
    //
    // We'll assume we only have TraderX, so &amp;quot;All Trader&amp;quot; == &amp;quot;TraderX&amp;quot; in this simple version.
    //
    // But let's keep it generic. If you had multiple traders, you'd do some checks:
    //
    // For Weighted Average Cost = total_value / total_power (where total_power != 0)

    // We'll measure the time for a single pass that gathers all these sums.

    using Clock = std::chrono::steady_clock;
    auto startTime = Clock::now();

    long long all_totalPower = 0;
    long long all_totalValue = 0;

    long long traderX_totalPower = 0;
    long long traderX_totalValue = 0;

    long long traderX_area1_totalPower = 0;
    long long traderX_area1_totalValue = 0;

    long long traderX_area2_totalPower = 0;
    long long traderX_area2_totalValue = 0;

    for(const auto&amp;amp; trade : trades)
    {
        // (a) &amp;quot;All Trader&amp;quot; sums:
        //    Summation for all trades, all areas, all days
        //    Because this example is all TraderX, you might have to adapt if you had multiple traders
        for(const auto&amp;amp; dd : trade.dailyDeliveries)
        {
            for(int slot = 0; slot &amp;lt; 100; ++slot)
            {
                all_totalPower += dd.power[slot];
                all_totalValue += dd.value[slot];
            }
        }

        // (b) If trade.trader == &amp;quot;TraderX&amp;quot;
        if(trade.trader == &amp;quot;TraderX&amp;quot;)
        {
            for(const auto&amp;amp; dd : trade.dailyDeliveries)
            {
                for(int slot = 0; slot &amp;lt; 100; ++slot)
                {
                    traderX_totalPower += dd.power[slot];
                    traderX_totalValue += dd.value[slot];
                }
            }

            // (c) and (d) by area
            if(trade.deliveryArea == &amp;quot;Area1&amp;quot;)
            {
                for(const auto&amp;amp; dd : trade.dailyDeliveries)
                {
                    for(int slot = 0; slot &amp;lt; 100; ++slot)
                    {
                        traderX_area1_totalPower += dd.power[slot];
                        traderX_area1_totalValue += dd.value[slot];
                    }
                }
            }
            else if(trade.deliveryArea == &amp;quot;Area2&amp;quot;)
            {
                for(const auto&amp;amp; dd : trade.dailyDeliveries)
                {
                    for(int slot = 0; slot &amp;lt; 100; ++slot)
                    {
                        traderX_area2_totalPower += dd.power[slot];
                        traderX_area2_totalValue += dd.value[slot];
                    }
                }
            }
        }
    }

    auto endTime = Clock::now();
    auto durationMs = std::chrono::duration_cast&amp;lt;std::chrono::milliseconds&amp;gt;(endTime - startTime).count();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How long do you think that takes on a commodity laptop? It's 9.8 GB of RAM to scan and fully aggregate. This is also running inside a VM on my Windows WSL instance, with other software running at the same time.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Time for in-memory aggregation: 907 ms
--- Memory usage statistics (approx) ---
Total bytes allocated (cumulative): 9822202136 bytes
Peak bytes allocated (concurrent):  9822202136 bytes
Current bytes allocated:            9822202136 bytes
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="parallelization"&gt;Parallelization&lt;/h2&gt;
&lt;p&gt;Since we are running on a multicore CPU, we can use more than one core to do the aggregation. With OpenMP, it's extremely simple to setup some parallelization for the compute. At the beginning of the loop, we can define a parallel aggregation for reduction, meaning a final sum.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-cpp"&gt;    // Parallel over trades
    // The 'reduction(+: variableList)' tells OpenMP to create private copies of
    // these variables in each thread, accumulate them, and then combine them
    // at the end.
#ifdef _OPENMP
#pragma omp parallel for reduction(+ : all_totalPower, all_totalValue, \
                                       traderX_totalPower, traderX_totalValue, \
                                       traderX_area1_totalPower, traderX_area1_totalValue, \
                                       traderX_area2_totalPower, traderX_area2_totalValue)
#endif
    for (std::size_t i = 0; i &amp;lt; trades.size(); ++i)
    {
        const auto&amp;amp; trade = trades[i];
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With this, we improve the time to aggregate on the laptop to: 241ms. This means we can now do the &lt;strong&gt;complete aggregation on a laptop 4x per second&lt;/strong&gt; - even on a completely unoptimized, simplistic memory structure for trades.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Time for in-memory aggregation: 241 ms
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So when you are doing large numerical aggregations or calculations, ask yourself - can I do this in RAM? If so, you might be surprised at how quickly and efficiently you can do it with modern cpp.&lt;/p&gt;
&lt;h2 id="performance-vs-memory-bandwidth"&gt;Performance vs Memory bandwidth&lt;/h2&gt;
&lt;p&gt;This completely unoptimized implementation is running at :&lt;/p&gt;
&lt;p&gt;Data size (GB) / time (s) = bandwidth (GB/s) =&amp;gt; 9.8 GB / 0.241 s ≈ &lt;strong&gt;40.7 GB/s&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;My laptop has approx ~96 GB/s of memory bandwidth. I calculate it as follows: LPDDR5 at 6000 MT/s, 8 bytes, dual channel = 6000 * 8 * 2 =~ 96 GB/s. My laptop has less than half bandwidth of the HC family on Azure (AMD EPYC™ 7003-series CPU) using CPUs that were released in 2021. Still impressive for my laptop, but it shows you could do much better.&lt;/p&gt;
&lt;p&gt;If I really needed to optimize, I would re-organise the data structures to improve the memory aligment as an initial step. With that, we should get closer to the theoretical bandwidth of the machine.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;Yesterday, at an Energy panel in Essen, I mentioned that some heavy calculations should be done in-memory. It is something that people have a tendency to dismiss because generally they are not aware of the capability and speed of modern CPUs and RAM. A 32GB dataset in memory can now be processed every second by a commodity cpu in your laptop.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://ewinnington.github.io/posts/network-tailscale</id>
		<title>Network with Tailscale</title>
		<link href="http://ewinnington.github.io/posts/network-tailscale" />
		<updated>2025-01-13T10:40:00Z</updated>
		<content>&lt;p&gt;I updated my OpenVPN based network to use Tailscale instead in 2023 and it is game changing. I have used Tailscale ever since. I simply did not update my blog and network diagram.&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/network/network-update.png" class="img-fluid" width="100%" alt="Network" /&gt;&lt;/p&gt;
&lt;p&gt;With &lt;a href="https://tailscale.com/"&gt;Tailscale&lt;/a&gt;, all my machines appear seamlessly on a single control pane and I can reach any of them from any device.&lt;/p&gt;
&lt;h2 id="zone-z"&gt;Zone Z&lt;/h2&gt;
&lt;p&gt;Z has a single fiber connection via Swisscom to internet.&lt;/p&gt;
&lt;h3 id="inventory"&gt;Inventory&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;DeepThread is an AMD Threadripper 1920x running Windows 10.&lt;/li&gt;
&lt;li&gt;Minis4 is the Beelink MiniS12 N95s running Ubuntu Server 24.10.&lt;/li&gt;
&lt;li&gt;NAS is an an older QNAP TS-269L&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="zone-n"&gt;Zone N&lt;/h2&gt;
&lt;p&gt;N has two connections, a Starlink (v1 round) with only the powerbrick router and Sosh as a backup DSL provider (with an ADSL Router) both connected to a Ubiquity UDM-PRO-SE in Failover mode.&lt;/p&gt;
&lt;h3 id="inventory-1"&gt;Inventory&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Minis1 is the Beelink MiniS12 N95s running Windows 11, enjoying it VESA mounted behind a screen in the office currently. I originally thought I would also put Ubuntu, but a windows machine is useful.&lt;/li&gt;
&lt;li&gt;Minis2 and Minis3 are  the Beelink MiniS12 N95s running Ubuntu Server  24.10. Currently rackmounted with the UDM-PRO.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="vpn"&gt;VPN&lt;/h3&gt;
&lt;p&gt;On the UDM-PRO, a VPN is configured with Ubiquity and I can use the iOS application WifiMan to access the network. It's really a backup of a backup solution to have Wifiman.&lt;/p&gt;
&lt;p&gt;On Minis2 and minis4, a &lt;a href="https://github.com/cloudflare/cloudflared"&gt;cloudflared docker&lt;/a&gt; is running, reaching up to Cloudflare and providing an Zero trust tunnel to expose several dockerized websites hosted on it.&lt;/p&gt;
&lt;p&gt;I made a &lt;a href="https://suno.com/song/fb47c594-b22d-4504-83a1-75d8df705194"&gt;Suno song on how awesome&lt;/a&gt; it is.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;I updated my OpenVPN based network to use Tailscale instead in 2023 and it is game changing. I have used Tailscale ever since. I simply did not update my blog and network diagram.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://ewinnington.github.io/posts/2025-predictions</id>
		<title>On the Horizon - 2025 - My predictions</title>
		<link href="http://ewinnington.github.io/posts/2025-predictions" />
		<updated>2025-01-13T00:00:00Z</updated>
		<content>&lt;h1 id="on-the-horizon-2025-my-predictions"&gt;On the Horizon - 2025 - My predictions&lt;/h1&gt;
&lt;p&gt;In a way, predicting 2025 is somewhat harder and easier than 2024, a lot of what I see are the seeds of 2024 coming to bloom. But for what we will have by the end of the year is really unclear to me - but we will see some impact in research for sure - AI assisted research in fields will explode this year.&lt;/p&gt;
&lt;h2 id="predictions"&gt;Predictions:&lt;/h2&gt;
&lt;h3 id="ai-video-ai-audio"&gt;AI Video / AI Audio&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Cinema level visuals made on generative environments&lt;/strong&gt; - not only creating a video, but creating the entire space so that the camera can be then moved in post production - will become available to the high end customers. This approach is a complement to the diffusion models, which only generate a few frames of temporal consistency - using this method will allow much better time coherence and consistency / object permanence. Nvidia Cosmos is closest to this and I think the next version of it will satisfy this point. I expect many video models to actually start using this method with temporal control nets to avoid the inconsistency of object permanence.&lt;/li&gt;
&lt;li&gt;AI Chatbots will be allowed to sing, make music and emote more. While some LLMs are already capable of such things, they are generally removed in post-training but I think these restrictions will be removed this year.  Suno's lead on AI Music gets folded into a leading model, meaning you'll be able to ask a ChatGPT competitor &amp;quot;make me a song, with lyrics and background track&amp;quot;.&lt;/li&gt;
&lt;li&gt;Visual understanding models will be commonly deployed - Meaning point your camera and get full descriptions of what you see, It's nearly there anyway.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="ai-agents"&gt;AI Agents&lt;/h3&gt;
&lt;p&gt;By agent, I define as a &lt;strong&gt;application in which an AI model takes actions against external systems on behalf of a user in furtherance of a user's give task and goal&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A desktop based agent will become available to use on the computer. Interacting with your browser and mail client automatically. (Probably Claude's Anthropic will be there soonest) - doesn't mean the LLM has to run on the desktop.&lt;/li&gt;
&lt;li&gt;AI Agents included in softwares (Teams, Github, ...) will start to become available in preview at least before the end of the year.&lt;/li&gt;
&lt;li&gt;Programming Agents will start to be useful (see AI Devin in 2024 being still completely unusable) - but in 2025 AI coders will be the focus and ship mid year.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="ai-in-mathematics"&gt;AI in Mathematics&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;While the first theorem proof by an LLM has already been published, I expect 2025 to have a slew of progress on fundamental proofs rewritten by LLMs or LRMs, particularly towards automated proofing systems (Coq, ...) and several proofs generated by LLMs that humans did not independently derive.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="ai-in-medicine"&gt;AI in Medicine&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;One company will announce an AI diagnostics companion for health - that is certified as a support tool for doctors.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="ai-in-war"&gt;AI in War&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A semi-autonomous multi-agent AI will be used to control a tactical engagement in Ukraine. This has nearly happened due to the engagement of the first robot brigade (&lt;a href="https://www.forbes.com/sites/davidaxe/2024/12/21/ukraines-first-all-robot-assault-force-just-won-its-first-battle/"&gt;dec 2024 : Ukraine’s All-Robot Assault Force Just Won Its First Battle&lt;/a&gt; ) - but without AI - using tele-operation. I think AI will be used at more levels than just terminal guidance of the FPV drones.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="agi"&gt;AGI&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;One company claims their model has attained &lt;strong&gt;AGI&lt;/strong&gt; - defined as a model that is as good as a reasoning human - &lt;strong&gt;&lt;em&gt;in office work related tasks&lt;/em&gt;&lt;/strong&gt;. There is a lot of disagreement if this constitutes AGI or not.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="robotics"&gt;Robotics:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Humanoid robots become available in limited quantities to the general public, with at least 1 company shipping a home robot with AI onboard to do simple tasks (Unitree from 2024 does not count since the robots are &lt;strong&gt;only&lt;/strong&gt; remote controlled). These first robots will be sometimes teleoperated for specific tasks. Pricing will be lower than 50k$ per robot.&lt;/li&gt;
&lt;li&gt;One company announces wide scale drone deliveries in US cities: While drone delivery companies already exist (&lt;a href="https://builtin.com/articles/drone-delivery-companies"&gt;13 Drone Delivery Companies to Know | Built In&lt;/a&gt;), one of them is going to break out as an early leader this year.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="space"&gt;Space:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;SpaceX demonstrates first ship to ship refueling.&lt;/li&gt;
&lt;li&gt;Blue Origin gets to orbit with New Glenn and proves the landing system, but is not able to send a reused first stage to orbit yet.&lt;/li&gt;
&lt;li&gt;The first part of a new space station gets deployed, probably commercial.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="environment"&gt;Environment:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;2025 beats 2024 as hottest year.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="energy"&gt;Energy:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Record installation of Solar, Wind and renewables (beating 2024 worldwide - despite the US's drop).&lt;/li&gt;
&lt;li&gt;Price spikes on Gas and Petrol due to US actions and disruption of Russian production. Unsure if that will continue throughout the year but we will have shocks from policy changes.&lt;/li&gt;
&lt;/ul&gt;
</content>
		<summary>&lt;p&gt;In a way, predicting 2025 is somewhat harder and easier than 2024, a lot of what I see are the seeds of 2024 coming to bloom. But for what we will have by the end of the year is really unclear to me - but we will see some impact in research for sure - AI assisted research in fields will explode this year.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://ewinnington.github.io/posts/Disposable-Software</id>
		<title>Disposable software</title>
		<link href="http://ewinnington.github.io/posts/Disposable-Software" />
		<updated>2024-12-09T00:00:00Z</updated>
		<content>&lt;h1 id="the-era-of-throw-away-software-is-upon-us"&gt;The era of throw away software is upon us&lt;/h1&gt;
&lt;p&gt;With the advent of LLMs and their capability to create quick programs (&amp;quot;create me a flashcard app&amp;quot;, &amp;quot;I need a typing exercise software&amp;quot;, &amp;quot;make a dashboard to track my investments&amp;quot;), we might see a lot more software being written, used and then discarded since it's trivial for a LLM to re-write it next time it is needed.&lt;/p&gt;
&lt;p&gt;Where perfection is not required, just good enough, there will be a whole slew of applications, websites and programs that are used and put into production that are never even reviewed by a human programmer, just tested for their outputs and/or visually checked by a human. Even, we might see lots of code that is only ever read by a machine for bugs and issues, to then be corrected by a machine.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;With the advent of LLMs and their capability to create quick programs ("create me a flashcard app", "I need a typing exercise software", "make a dashboard to track my investments"), we might see a lot more software being written, used and then discarded since it's trivial for a LLM to re-write it next time it is needed.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://ewinnington.github.io/posts/Audit-Trail-Oracle</id>
		<title>Using an audit trail table on Oracle</title>
		<link href="http://ewinnington.github.io/posts/Audit-Trail-Oracle" />
		<updated>2024-10-05T08:00:00Z</updated>
		<content>&lt;h1 id="implementing-auditable-updates-in-a-relational-database"&gt;Implementing Auditable Updates in a Relational Database&lt;/h1&gt;
&lt;p&gt;In modern applications, maintaining an audit trail of changes to data is crucial for compliance, debugging, and data integrity. This blog post explores a straightforward approach to implementing auditable updates in a relational database system, specifically focusing on a project management scenario with hierarchical data.&lt;/p&gt;
&lt;h2 id="problem-description"&gt;Problem Description&lt;/h2&gt;
&lt;p&gt;We have a relational database containing &lt;code&gt;Projects&lt;/code&gt;, each of which includes &lt;code&gt;Instruments&lt;/code&gt;, &lt;code&gt;Markets&lt;/code&gt;, and &lt;code&gt;Valuations&lt;/code&gt;. These entities form a tree structure, adhering to the third normal form (3NF). Previously, any update to a project involved downloading the entire project tree, making changes, and uploading a new project under a new ID to ensure complete auditability.&lt;/p&gt;
&lt;p&gt;This approach is inefficient for small updates and doesn't allow for granular tracking of changes. The goal is to enable small, precise updates to projects while maintaining a comprehensive audit trail of all changes.&lt;/p&gt;
&lt;h2 id="solution-overview"&gt;Solution Overview&lt;/h2&gt;
&lt;p&gt;We introduce an audit table that records every change made to the database. The audit table will store serialized JSON representations of operations like &lt;code&gt;update&lt;/code&gt;, &lt;code&gt;insert&lt;/code&gt;, and &lt;code&gt;delete&lt;/code&gt;. We'll also provide C# code to apply and revert these changes, effectively creating an undo stack.&lt;/p&gt;
&lt;p&gt;Let's use the following DB Schema for illustration:&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/audit-trail/TableStructureBlog.png" class="img-fluid" width="60%" alt="TableSchema" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Primary Keys: Each table has a primary key (e.g., &lt;code&gt;ProjectID&lt;/code&gt;, &lt;code&gt;InstrumentID&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Foreign Keys: Child tables reference their parent via foreign keys (e.g., &lt;code&gt;instruments.ProjectID&lt;/code&gt; references &lt;code&gt;projects.ProjectID&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Audit Table: The &lt;code&gt;change_audit&lt;/code&gt; table records changes with fields like &lt;code&gt;ChangeAuditID&lt;/code&gt;, &lt;code&gt;TimeApplied&lt;/code&gt;, and &lt;code&gt;ImpactJson&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- 
```d2
projects: {
  shape: sql_table
  ProjectID: int {constraint: primary_key}
  Name: varchar(100)
  Description: text
  LastUpdated: timestamp with time zone
  VersionNumber: int
}

instruments: {
  shape: sql_table
  InstrumentID: int {constraint: primary_key}
  ProjectID: int {constraint: foreign_key}
  Name: varchar(100)
  Type: varchar(50)
  LastUpdated: timestamp with time zone
}

markets: {
  shape: sql_table
  MarketID: int {constraint: primary_key}
  ProjectID: int {constraint: foreign_key}
  Region: varchar(50)
  MarketType: varchar(50)
  LastUpdated: timestamp with time zone
}

valuations: {
  shape: sql_table
  ValuationID: int {constraint: primary_key}
  ProjectID: int {constraint: foreign_key}
  Value: decimal(10, 2)
  Currency: varchar(10)
  LastUpdated: timestamp with time zone
}

change_audit: {
  shape: sql_table
  ChangeAuditID: int {constraint: primary_key}
  TimeApplied: timestamp with time zone
  UserID: varchar(100)
  ImpactJson: jsonb
}

instruments.ProjectID -&gt; projects.ProjectID
markets.ProjectID -&gt; projects.ProjectID
valuations.ProjectID -&gt; projects.ProjectID

```
--&gt;
&lt;h2 id="implementing-change-auditing"&gt;Implementing Change Auditing&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;change_audit&lt;/code&gt; table is designed to store all changes in a JSON format for flexibility and ease of storage.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;CREATE TABLE change_audit (
  ChangeAuditID   NUMBER PRIMARY KEY,
  TimeApplied     TIMESTAMP,
  UserID          VARCHAR2(100),
  ImpactJson      CLOB
);
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="json-structure-for-changes"&gt;JSON Structure for Changes&lt;/h2&gt;
&lt;p&gt;Each change is recorded as a JSON object:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
  &amp;quot;Operation&amp;quot;: &amp;quot;update&amp;quot;,
  &amp;quot;impact&amp;quot;: [
    {
      &amp;quot;Table&amp;quot;: &amp;quot;Instruments&amp;quot;,
      &amp;quot;PrimaryKey&amp;quot;: {&amp;quot;ProjectID&amp;quot;: 4, &amp;quot;InstrumentID&amp;quot;: 2},
      &amp;quot;Column&amp;quot;: &amp;quot;Name&amp;quot;,
      &amp;quot;OldValue&amp;quot;: &amp;quot;Old Instrument Name&amp;quot;,
      &amp;quot;NewValue&amp;quot;: &amp;quot;Updated Instrument Name&amp;quot;
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="csharp-to-apply-changes-given-an-operation"&gt;CSharp to apply changes given an operation&lt;/h2&gt;
&lt;p&gt;To apply changes recorded in the JSON, we'll use C# code that parses the JSON and executes the corresponding SQL commands.&lt;/p&gt;
&lt;p&gt;I assume you have the &lt;code&gt;_connectionString&lt;/code&gt; available somewhere as a constant in the code.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;using Oracle.ManagedDataAccess.Client;
using Newtonsoft.Json.Linq;
using System;
using System.Collections.Generic;

public class ChangeApplier
{

    public void ApplyChanges(string jsonInput)
    {
        // Parse the JSON input
        var operation = JObject.Parse(jsonInput);
        string opType = operation[&amp;quot;Operation&amp;quot;].ToString();
        var impactList = (JArray)operation[&amp;quot;impact&amp;quot;];

        using (var conn = new OracleConnection(_connectionString))
        {
            conn.Open();
            using (var transaction = conn.BeginTransaction())
            {
                try
                {
                    foreach (var impact in impactList)
                    {
                        string table = impact[&amp;quot;Table&amp;quot;].ToString();
                        var primaryKey = (JObject)impact[&amp;quot;PrimaryKey&amp;quot;];
                        string column = impact[&amp;quot;Column&amp;quot;]?.ToString();
                        string newValue = impact[&amp;quot;NewValue&amp;quot;]?.ToString();

                        switch (opType)
                        {
                            case &amp;quot;update&amp;quot;:
                                ApplyUpdate(conn, table, primaryKey, column, newValue);
                                break;
                            case &amp;quot;insert&amp;quot;:
                                ApplyInsert(conn, table, impact);
                                break;
                            case &amp;quot;delete&amp;quot;:
                                ApplyDelete(conn, table, primaryKey);
                                break;
                        }
                    }

                    transaction.Commit();
                }
                catch (Exception ex)
                {
                    transaction.Rollback();
                    Console.WriteLine($&amp;quot;Error applying changes: {ex.Message}&amp;quot;);
                }
            }
        }
    }

    private void ApplyUpdate(OracleConnection conn, string table, JObject primaryKey, string column, string newValue)
    {
        var pkConditions = BuildPrimaryKeyCondition(primaryKey);
        var query = $&amp;quot;UPDATE {table} SET {column} = :newValue WHERE {pkConditions}&amp;quot;;

        using (var cmd = new OracleCommand(query, conn))
        {
            cmd.Parameters.Add(new OracleParameter(&amp;quot;newValue&amp;quot;, newValue));
            cmd.ExecuteNonQuery();
        }
    }

    private void ApplyInsert(OracleConnection conn, string table, JToken impact)
    {
        var primaryKey = (JObject)impact[&amp;quot;PrimaryKey&amp;quot;];
        var newValues = (JObject)impact[&amp;quot;NewValues&amp;quot;];
        var columns = new List&amp;lt;string&amp;gt;();
        var values = new List&amp;lt;string&amp;gt;();

        foreach (var property in primaryKey.Properties())
        {
            columns.Add(property.Name);
            values.Add($&amp;quot;:{property.Name}&amp;quot;);
        }

        foreach (var property in newValues.Properties())
        {
            columns.Add(property.Name);
            values.Add($&amp;quot;:{property.Name}&amp;quot;);
        }

        var query = $&amp;quot;INSERT INTO {table} ({string.Join(&amp;quot;, &amp;quot;, columns)}) VALUES ({string.Join(&amp;quot;, &amp;quot;, values)})&amp;quot;;

        using (var cmd = new OracleCommand(query, conn))
        {
            foreach (var property in primaryKey.Properties())
            {
                cmd.Parameters.Add(new OracleParameter(property.Name, property.Value.ToString()));
            }

            foreach (var property in newValues.Properties())
            {
                cmd.Parameters.Add(new OracleParameter(property.Name, property.Value.ToString()));
            }

            cmd.ExecuteNonQuery();
        }
    }

    private void ApplyDelete(OracleConnection conn, string table, JObject primaryKey)
    {
        var pkConditions = BuildPrimaryKeyCondition(primaryKey);
        var query = $&amp;quot;DELETE FROM {table} WHERE {pkConditions}&amp;quot;;

        using (var cmd = new OracleCommand(query, conn))
        {
            cmd.ExecuteNonQuery();
        }
    }

    private string BuildPrimaryKeyCondition(JObject primaryKey)
    {
        var conditions = new List&amp;lt;string&amp;gt;();
        foreach (var prop in primaryKey.Properties())
        {
            conditions.Add($&amp;quot;{prop.Name} = :{prop.Name}&amp;quot;);
        }
        return string.Join(&amp;quot; AND &amp;quot;, conditions);
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ApplyChanges&lt;/strong&gt;: Parses the JSON input and determines the operation type.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ApplyUpdate&lt;/strong&gt;: Executes an UPDATE SQL command using parameters to prevent SQL injection.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ApplyInsert&lt;/strong&gt;: Executes an INSERT SQL command, constructing columns and values from the JSON.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ApplyDelete&lt;/strong&gt;: Executes a DELETE SQL command based on the primary key.
BuildPrimaryKeyCondition: Constructs the WHERE clause for SQL commands.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A side note, for the insert, you'll have the challenge if you are using auto-incremented IDs, this will mean you don't know the new IDs until you have inserted the data, so you should make sure to capture the new IDs and then create the audit log. This is left as a simple exercise to the reader in case it is necessary.&lt;/p&gt;
&lt;h2 id="csharp-to-revert-changes"&gt;CSharp to revert changes&lt;/h2&gt;
&lt;p&gt;To revert changes (undo operations), we'll process the audit trail in reverse order. Here I give the processing of a list of operations as an example of unrolling. It is to note that the reverse delete does only one table, so if there was some connected information that was deleted via referential identity, it was the task of the audit table to keep that in the audit.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public class ChangeReverter
{
    public void RevertChanges(List&amp;lt;string&amp;gt; jsonOperations)
    {
        using (var conn = new OracleConnection(_connectionString))
        {
            conn.Open();
            using (var transaction = conn.BeginTransaction())
            {
                try
                {
                    jsonOperations.Reverse(); // note: you could also have provided sorted by last time from the audit table instead of reversing them

                    foreach (var operationJson in jsonOperations)
                    {
                        var operation = JObject.Parse(operationJson);
                        string opType = operation[&amp;quot;Operation&amp;quot;].ToString();
                        var impactList = (JArray)operation[&amp;quot;impact&amp;quot;];

                        foreach (var impact in impactList)
                        {
                            string table = impact[&amp;quot;Table&amp;quot;].ToString();
                            var primaryKey = (JObject)impact[&amp;quot;PrimaryKey&amp;quot;];
                            string column = impact[&amp;quot;Column&amp;quot;]?.ToString();
                            string oldValue = impact[&amp;quot;OldValue&amp;quot;]?.ToString();

                            switch (opType)
                            {
                                case &amp;quot;update&amp;quot;:
                                    RevertUpdate(conn, table, primaryKey, column, oldValue);
                                    break;
                                case &amp;quot;insert&amp;quot;:
                                    ApplyDelete(conn, table, primaryKey);
                                    break;
                                case &amp;quot;delete&amp;quot;:
                                    RevertDelete(conn, table, impact);
                                    break;
                            }
                        }
                    }

                    transaction.Commit();
                }
                catch (Exception ex)
                {
                    transaction.Rollback();
                    Console.WriteLine($&amp;quot;Error reverting changes: {ex.Message}&amp;quot;);
                }
            }
        }
    }

    private void RevertUpdate(OracleConnection conn, string table, JObject primaryKey, string column, string oldValue)
    {
        var pkConditions = BuildPrimaryKeyCondition(primaryKey);
        var query = $&amp;quot;UPDATE {table} SET {column} = :oldValue WHERE {pkConditions}&amp;quot;;

        using (var cmd = new OracleCommand(query, conn))
        {
            cmd.Parameters.Add(new OracleParameter(&amp;quot;oldValue&amp;quot;, oldValue));
            cmd.ExecuteNonQuery();
        }
    }

    private void RevertDelete(OracleConnection conn, string table, JToken impact)
    {
        var primaryKey = (JObject)impact[&amp;quot;PrimaryKey&amp;quot;];
        var oldValues = (JObject)impact[&amp;quot;OldValues&amp;quot;];
        var columns = new List&amp;lt;string&amp;gt;();
        var values = new List&amp;lt;string&amp;gt;();

        foreach (var property in primaryKey.Properties())
        {
            columns.Add(property.Name);
            values.Add($&amp;quot;:{property.Name}&amp;quot;);
        }

        foreach (var property in oldValues.Properties())
        {
            columns.Add(property.Name);
            values.Add($&amp;quot;:{property.Name}&amp;quot;);
        }

        var query = $&amp;quot;INSERT INTO {table} ({string.Join(&amp;quot;, &amp;quot;, columns)}) VALUES ({string.Join(&amp;quot;, &amp;quot;, values)})&amp;quot;;

        using (var cmd = new OracleCommand(query, conn))
        {
            foreach (var property in primaryKey.Properties())
            {
                cmd.Parameters.Add(new OracleParameter(property.Name, property.Value.ToString()));
            }

            foreach (var property in oldValues.Properties())
            {
                cmd.Parameters.Add(new OracleParameter(property.Name, property.Value.ToString()));
            }

            cmd.ExecuteNonQuery();
        }
    }

    // Reuse BuildPrimaryKeyCondition and ApplyDelete methods from ChangeApplier
}
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;RevertChanges&lt;/strong&gt;: Processes the list of JSON operations in reverse order to undo changes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RevertUpdate&lt;/strong&gt;: Sets the column back to its old value.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RevertDelete&lt;/strong&gt;: Re-inserts a deleted row using the old values stored in the audit trail.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ApplyDelete&lt;/strong&gt;: Deletes a row, used here to undo an insert operation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="json-schema"&gt;JSON schema&lt;/h2&gt;
&lt;p&gt;The reason that I prefer to use the Json directly in the C# code is that actually making up the C# classes for this schema is actually more work that processing the json directly in the code.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
  &amp;quot;$schema&amp;quot;: &amp;quot;http://json-schema.org/draft-07/schema#&amp;quot;,
  &amp;quot;title&amp;quot;: &amp;quot;ImpactJsonRoot&amp;quot;,
  &amp;quot;type&amp;quot;: &amp;quot;object&amp;quot;,
  &amp;quot;properties&amp;quot;: {
    &amp;quot;Operation&amp;quot;: {
      &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;,
      &amp;quot;enum&amp;quot;: [&amp;quot;update&amp;quot;, &amp;quot;insert&amp;quot;, &amp;quot;delete&amp;quot;],
      &amp;quot;description&amp;quot;: &amp;quot;Type of operation&amp;quot;
    },
    &amp;quot;Impact&amp;quot;: {
      &amp;quot;type&amp;quot;: &amp;quot;array&amp;quot;,
      &amp;quot;items&amp;quot;: {
        &amp;quot;type&amp;quot;: &amp;quot;object&amp;quot;,
        &amp;quot;properties&amp;quot;: {
          &amp;quot;Table&amp;quot;: {
            &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;,
            &amp;quot;description&amp;quot;: &amp;quot;Name of the table affected&amp;quot;
          },
          &amp;quot;PrimaryKey&amp;quot;: {
            &amp;quot;type&amp;quot;: &amp;quot;object&amp;quot;,
            &amp;quot;description&amp;quot;: &amp;quot;Primary key fields and their values&amp;quot;,
            &amp;quot;additionalProperties&amp;quot;: {
              &amp;quot;type&amp;quot;: [&amp;quot;number&amp;quot;, &amp;quot;null&amp;quot;]
            }
          },
          &amp;quot;Column&amp;quot;: {
            &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;,
            &amp;quot;description&amp;quot;: &amp;quot;Column affected (for updates)&amp;quot;
          },
          &amp;quot;OldValue&amp;quot;: {
            &amp;quot;type&amp;quot;: [&amp;quot;string&amp;quot;, &amp;quot;number&amp;quot;, &amp;quot;boolean&amp;quot;, &amp;quot;null&amp;quot;],
            &amp;quot;description&amp;quot;: &amp;quot;Previous value (for updates and deletes)&amp;quot;
          },
          &amp;quot;NewValue&amp;quot;: {
            &amp;quot;type&amp;quot;: [&amp;quot;string&amp;quot;, &amp;quot;number&amp;quot;, &amp;quot;boolean&amp;quot;, &amp;quot;null&amp;quot;],
            &amp;quot;description&amp;quot;: &amp;quot;New value (for updates and inserts)&amp;quot;
          },
          &amp;quot;OldValues&amp;quot;: {
            &amp;quot;type&amp;quot;: &amp;quot;object&amp;quot;,
            &amp;quot;description&amp;quot;: &amp;quot;All old values (for deletes)&amp;quot;,
            &amp;quot;additionalProperties&amp;quot;: {
              &amp;quot;type&amp;quot;: [&amp;quot;string&amp;quot;, &amp;quot;number&amp;quot;, &amp;quot;boolean&amp;quot;, &amp;quot;null&amp;quot;]
            }
          },
          &amp;quot;NewValues&amp;quot;: {
            &amp;quot;type&amp;quot;: &amp;quot;object&amp;quot;,
            &amp;quot;description&amp;quot;: &amp;quot;All new values (for inserts)&amp;quot;,
            &amp;quot;additionalProperties&amp;quot;: {
              &amp;quot;type&amp;quot;: [&amp;quot;string&amp;quot;, &amp;quot;number&amp;quot;, &amp;quot;boolean&amp;quot;, &amp;quot;null&amp;quot;]
            }
          }
        },
        &amp;quot;required&amp;quot;: [&amp;quot;Table&amp;quot;, &amp;quot;PrimaryKey&amp;quot;]
      }
    }
  },
  &amp;quot;required&amp;quot;: [&amp;quot;Operation&amp;quot;, &amp;quot;Impact&amp;quot;]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and here are examples of operations:&lt;/p&gt;
&lt;h3 id="update"&gt;update&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
  &amp;quot;Operation&amp;quot;: &amp;quot;update&amp;quot;,
  &amp;quot;Impact&amp;quot;: [
    {
      &amp;quot;Table&amp;quot;: &amp;quot;Instruments&amp;quot;,
      &amp;quot;PrimaryKey&amp;quot;: { &amp;quot;ProjectID&amp;quot;: 4, &amp;quot;InstrumentID&amp;quot;: 2 },
      &amp;quot;Column&amp;quot;: &amp;quot;Name&amp;quot;,
      &amp;quot;OldValue&amp;quot;: &amp;quot;Old Instrument Name&amp;quot;,
      &amp;quot;NewValue&amp;quot;: &amp;quot;Updated Instrument Name&amp;quot;
    }
  ]
}

&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="insert"&gt;insert&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
  &amp;quot;Operation&amp;quot;: &amp;quot;insert&amp;quot;,
  &amp;quot;Impact&amp;quot;: [
    {
      &amp;quot;Table&amp;quot;: &amp;quot;Instruments&amp;quot;,
      &amp;quot;PrimaryKey&amp;quot;: { &amp;quot;ProjectID&amp;quot;: 4, &amp;quot;InstrumentID&amp;quot;: 10 },
      &amp;quot;NewValues&amp;quot;: {
        &amp;quot;Name&amp;quot;: &amp;quot;New Instrument&amp;quot;,
        &amp;quot;Type&amp;quot;: &amp;quot;Flexible Asset&amp;quot;,
        &amp;quot;LastUpdated&amp;quot;: &amp;quot;2024-10-05T12:34:56Z&amp;quot;
      }
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="delete"&gt;delete&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
  &amp;quot;Operation&amp;quot;: &amp;quot;delete&amp;quot;,
  &amp;quot;Impact&amp;quot;: [
    {
      &amp;quot;Table&amp;quot;: &amp;quot;Instruments&amp;quot;,
      &amp;quot;PrimaryKey&amp;quot;: { &amp;quot;ProjectID&amp;quot;: 4, &amp;quot;InstrumentID&amp;quot;: 5 },
      &amp;quot;OldValues&amp;quot;: {
        &amp;quot;Name&amp;quot;: &amp;quot;Obsolete Instrument&amp;quot;,
        &amp;quot;Type&amp;quot;: &amp;quot;Flexible Asset&amp;quot;,
        &amp;quot;LastUpdated&amp;quot;: &amp;quot;2024-10-01T09:15:00Z&amp;quot;
      }
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note: OpenAI's &lt;code&gt;o1-preview&lt;/code&gt; was used to assist in the creation of the post.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;In modern applications, maintaining an audit trail of changes to data is crucial for compliance, debugging, and data integrity. This blog post explores a straightforward approach to implementing auditable updates in a relational database system, specifically focusing on a project management scenario with hierarchical data.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://ewinnington.github.io/posts/snippets-in-vscode</id>
		<title>VSCode Snippets</title>
		<link href="http://ewinnington.github.io/posts/snippets-in-vscode" />
		<updated>2024-07-18T11:00:00Z</updated>
		<content>&lt;p&gt;I use Visual Studio Code to write markdown notes of my meetings. I datetime every meeting to be able to find it afterwards. One of the shorthands I use is 'dt'. I always forget how to set it up when I reinstall my vscode, so here's the instructions from github copilot chat, which explained how to set it up again!&lt;/p&gt;
&lt;p&gt;So now I know I can ask Copilot how to do it, but I still want to document it, so here it is:&lt;/p&gt;
&lt;h3 id="github-copilot-instructions"&gt;Github Copilot Instructions&lt;/h3&gt;
&lt;p&gt;To recreate the shorthand command in Visual Studio Code for inserting the current date and time in a Markdown file using &lt;code&gt;ctrl-space&lt;/code&gt;, you can use a snippet. Here's how to set it up:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Open the Command Palette with &lt;code&gt;Ctrl+Shift+P&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Type &lt;code&gt;Configure User Snippets&lt;/code&gt; and select it.&lt;/li&gt;
&lt;li&gt;Choose &lt;code&gt;markdown.json&lt;/code&gt; for Markdown files.&lt;/li&gt;
&lt;li&gt;Insert the following snippet configuration:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;&amp;quot;Insert Current Date and Time&amp;quot;: {
    &amp;quot;prefix&amp;quot;: &amp;quot;dt&amp;quot;,
    &amp;quot;body&amp;quot;: [
        &amp;quot;### $CURRENT_YEAR.$CURRENT_MONTH.$CURRENT_DATE $CURRENT_HOUR:$CURRENT_MINUTE:$CURRENT_SECOND&amp;quot;
    ],
    &amp;quot;description&amp;quot;: &amp;quot;Inserts the current date and time&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start="5"&gt;
&lt;li&gt;Save the &lt;code&gt;markdown.json&lt;/code&gt; file.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now, when you type &lt;code&gt;dt&lt;/code&gt; in a Markdown file and press &lt;code&gt;Ctrl+Space&lt;/code&gt;, it should suggest your snippet. Selecting it will insert the current date and time formatted as you described.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;I use Visual Studio Code to write markdown notes of my meetings. I datetime every meeting to be able to find it afterwards. One of the shorthands I use is 'dt'. I always forget how to set it up when I reinstall my vscode, so here's the instructions from github copilot chat, which explained how to set it up again!&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://ewinnington.github.io/posts/HowToUpdateWritebook</id>
		<title>Adding LaTeX Maths to Writebook</title>
		<link href="http://ewinnington.github.io/posts/HowToUpdateWritebook" />
		<updated>2024-07-13T15:00:00Z</updated>
		<content>&lt;h1 id="how-to-add-latex-maths-to-writebook"&gt;How to add LaTeX maths to Writebook&lt;/h1&gt;
&lt;p&gt;I've started using ONCE &lt;a href="https://once.com/writebook"&gt;Writebook&lt;/a&gt; to host some markdown documents, including my book on Optimization applied to energy products. But that book contains tons of Markdown LaTeX formatted mathematics, which Writebook does not support at this time.&lt;/p&gt;
&lt;p&gt;So I patched support for it into the docker image.&lt;/p&gt;
&lt;h2 id="copy-out-the-app-layout-file"&gt;Copy out the app layout file&lt;/h2&gt;
&lt;p&gt;Assuming your docker image is named writebook.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo docker cp writebook:/rails/app/views/layouts/application.html.erb .
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="modify-the-application-erb-to-add-support"&gt;Modify the application erb to add support&lt;/h2&gt;
&lt;p&gt;In the Head Section add&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; &amp;lt;script type=&amp;quot;text/javascript&amp;quot;&amp;gt;
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$']],
          displayMath: [['$$', '$$']],
          processEscapes: true
        }
      };
    &amp;lt;/script&amp;gt;
      
    &amp;lt;script src=&amp;quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thing is, it doesn't render the first time round, so I have to refresh the page to render, but it's not too much of an issue right now, I actually like to see the LaTeX maths code before I see the rendered version.&lt;/p&gt;
&lt;!--
Not working right now ! 

## How to add a render on end of page load to ensure it typesets first time

Above the ```&lt;/Body&gt;``` tag at the bottom of the page, add
```
    &lt;script type="text/javascript"&gt;
    document.addEventListener("DOMContentLoaded", function() {
      MathJax.typeset();
    });
  &lt;/script&gt;
```

--&gt;
&lt;h2 id="save-and-copy-the-file-back"&gt;Save and copy the file back&lt;/h2&gt;
&lt;p&gt;Then finally copy the file back into the docker image and restart the image&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo docker cp application.html.erb writebook:/rails/app/views/layouts/application.html.erb
sudo docker restart writebook
&lt;/code&gt;&lt;/pre&gt;
</content>
		<summary>&lt;p&gt;I've started using ONCE &lt;a href="https://once.com/writebook"&gt;Writebook&lt;/a&gt; to host some markdown documents, including my book on Optimization applied to energy products. But that book contains tons of Markdown LaTeX formatted mathematics, which Writebook does not support at this time.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://ewinnington.github.io/posts/HttpClientCompression</id>
		<title>Receiving compressed data from an http(s) endpoint</title>
		<link href="http://ewinnington.github.io/posts/HttpClientCompression" />
		<updated>2024-03-20T11:00:00Z</updated>
		<content>&lt;p&gt;With the amount of data that we are passing around across services, it is often beneficial to use compression on the data to reduce the transmission time. Modern platforms and algorithms are now very efficient at compressing regular data, particularly if that data is text or json data. &lt;/p&gt;
&lt;p&gt;If the developer of the endpoint has prepared their service for compression, the client must still indicate that they are ready to receive the compressed data. Luckily, most implementations of modern http clients in R, Python, JavaScript and Dotnet support compression / decompression and are seamless for the client. This means that you can set the compression headers on and simply benefit from compressed data being received. &lt;/p&gt;
&lt;p&gt;We can also check in the Content-Encoding header which compression was used. I've found that example.com is sending responses compressed with gzip.&lt;/p&gt;
&lt;h2 id="python"&gt;Python&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import requests

url = &amp;quot;http://example.com&amp;quot;  # Replace with the actual URL you want to request

# Specify the accepted encoding methods in the headers
headers = {
    'Accept-Encoding': 'gzip, br',
}

response = requests.get(url, headers=headers)
print(response.text)

# In case you want to see if it was compressed, you can check via the headers
#if 'Content-Encoding' in response.headers:
#    print(f&amp;quot;Response was compressed using: {response.headers['Content-Encoding']}&amp;quot;)
#else:
#    print(&amp;quot;Response was not compressed.&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="r"&gt;R&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-R"&gt;library(httr)

# The URL to which you're sending the request
url &amp;lt;- &amp;quot;http://example.com&amp;quot;

# Setting the Accept-Encoding header
response &amp;lt;- GET(url, add_headers(`Accept-Encoding` = 'gzip, br'))

# The content of the response will be automatically decompressed by httr, so you can access it directly.
content(response, &amp;quot;text&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="c"&gt;C#&lt;/h2&gt;
&lt;p&gt;In C#, for some ungodly strange reason, the standard HTTP endpoint doesn't decompress for you automatically unless you add a decompression handler - see handler &lt;a href="https://learn.microsoft.com/en-us/dotnet/api/system.net.http.httpclienthandler.automaticdecompression?view=net-8.0#system-net-http-httpclienthandler-automaticdecompression"&gt;HttpClientHandler.AutomaticDecompression&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-Csharp"&gt;using System;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Threading.Tasks;
using System.Text;

class Program
{
    static async Task Main(string[] args)
    {
        HttpClientHandler handler = new HttpClientHandler();
        handler.AutomaticDecompression = System.Net.DecompressionMethods.GZip; //Adding automatic Decompression means that the accept headers are added automatically

        using (var client = new HttpClient(handler))
        {
            string url = &amp;quot;http://example.com&amp;quot;;
            HttpResponseMessage response = await client.GetAsync(url);
            response.EnsureSuccessStatusCode();

            Console.WriteLine(await response.Content.ReadAsStringAsync());
		}
	}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="javascript"&gt;JavaScript&lt;/h2&gt;
&lt;p&gt;it is so easy that you don't even need to do anything else than setting the gzip: true for the support&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-JS"&gt;const request = require('request');
const requestOptions = {
  url: 'http://example.com',
  gzip: true, // This is all that is required
};
request(requestOptions, (error, response, body) =&amp;gt; {
  // Handle the response here
});
&lt;/code&gt;&lt;/pre&gt;
</content>
		<summary>&lt;p&gt;With the amount of data that we are passing around across services, it is often beneficial to use compression on the data to reduce the transmission time. Modern platforms and algorithms are now very efficient at compressing regular data, particularly if that data is text or json data.&amp;nbsp;&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://ewinnington.github.io/posts/devcontainers</id>
		<title>DevContainers - The future of developer environments</title>
		<link href="http://ewinnington.github.io/posts/devcontainers" />
		<updated>2023-07-24T00:00:00Z</updated>
		<content>&lt;h2 id="history"&gt;History&lt;/h2&gt;
&lt;p&gt;It's been years now that we've had Infrastructure as Code (IaC), Containers and Desired state Configuration (DsC) tools to do our deployments. But these have been mostly focused on the deployment side of things, with fewer tools on the developer side. On the dev machine, installing and maintaining the development tools and package dependencies has been in flux, both in windows where finally tools like Ninite, Chocolatey and Winget allow management of dev tools, and on the linux side, which was always quite well served with apt - but has also gained Snap, Flatpack and other package management tools. The thing is, sometimes you need more that one version of a particular tool, Python3.10 and Python3.11, Java9 and Java17, Dotnet 4.8 and Dotnet 6, to work on the various projects you have during the day. Sometimes, they work side by side very well and sometimes they don't. And when they don't, it can be a long process to figure out why and also very difficult to get help without resorting to having a clean image refresh and starting again to install your dependencies.&lt;/p&gt;
&lt;p&gt;Since the end of the 2010s and the early 2020s, with the rise of web hosted IDEs, there has been a need to define ways to have a base image that contained the environment and tools needed to work. I remember running some in the mid 2010s - Nitrous.IO (2013-16) - that allowed you to use a base container and configure it to do remote development.&lt;/p&gt;
&lt;h2 id="devcontainers"&gt;DevContainers&lt;/h2&gt;
&lt;p&gt;With the arrival of Docker on every desktop, Github's Cloudspaces and Visual Studio Code, there's been a new interest in this type of desired state environments with developer tooling. Microsoft published the &lt;a href="https://containers.dev/"&gt;DevContainer specification&lt;/a&gt; in early 2022 to formalize the language.&lt;/p&gt;
&lt;p&gt;So how does it help us? Well, with a DevContainer, we can setup a new development environment on Premise (in VSCode), on the cloud VM (Azure+VM) or on a Codespace environment with a single file that ensures that we always have the tools we want and need installed. Starting to work is as easy as openining the connection and cloning the repo we need if the .devcontainer file is located inside.&lt;/p&gt;
&lt;h2 id="devcontainer-example"&gt;DevContainer example&lt;/h2&gt;
&lt;p&gt;You can find below my &lt;a href="https://github.com/ewinnington/DevContainerTemplate/blob/master/.devcontainer/devcontainer.json"&gt;personal DevContainer&lt;/a&gt;, it is setup with Git, Node, AzureCLI, Docker control of hose, Dotnet, Terraform, Java with Maven, Python3 and Postgresql. I also have the VSCode extensions directly configured so I can directly start using them when I connect. I also use the &amp;quot;postStartCommand&amp;quot;: &amp;quot;nohup bash -c 'postgres &amp;amp;'&amp;quot; to run an instance of Postgresql directly inside the development container, so I can a directly have a DB to run requests against. And yes, this is a bit of a kitchen sink DevContainer, they can be smaller and more tailored to a project with only one or two of these features included, but here I use a generic one add added everything I use apart from the c++ and fortran compilers.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{
    &amp;quot;name&amp;quot;: &amp;quot;Erics-base-dev-container&amp;quot;,
    &amp;quot;image&amp;quot;: &amp;quot;mcr.microsoft.com/devcontainers/base:debian&amp;quot;,
 
    &amp;quot;features&amp;quot;: {
        &amp;quot;ghcr.io/devcontainers/features/git:1&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/node:1&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/azure-cli:1&amp;quot;: {}, //azure-cli,
        &amp;quot;ghcr.io/devcontainers/features/docker-outside-of-docker:1&amp;quot;: {}, //docker on host
        &amp;quot;ghcr.io/devcontainers/features/dotnet:1&amp;quot;: {}, //dotnet installed
        &amp;quot;ghcr.io/devcontainers/features/terraform:1&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/java:1&amp;quot;: { &amp;quot;installMaven&amp;quot; : true },
        &amp;quot;ghcr.io/devcontainers-contrib/features/postgres-asdf:1&amp;quot;: {}
    },
 
    // Configure tool-specific properties.
    &amp;quot;customizations&amp;quot;: {
        // Configure properties specific to VS Code.
        &amp;quot;vscode&amp;quot;: {
            &amp;quot;settings&amp;quot;: {},
            &amp;quot;extensions&amp;quot;: [
                &amp;quot;streetsidesoftware.code-spell-checker&amp;quot;,
                &amp;quot;ms-azuretools.vscode-docker&amp;quot;,
                &amp;quot;ms-dotnettools.csharp&amp;quot;,
                &amp;quot;HashiCorp.terraform&amp;quot;,
                &amp;quot;ms-azuretools.vscode-azureterraform&amp;quot;,
                &amp;quot;GitHub.copilot&amp;quot;,
                &amp;quot;GitHub.copilot-chat&amp;quot;,
                &amp;quot;vscjava.vscode-java-pack&amp;quot;,
                &amp;quot;ms-python.python&amp;quot;
            ]
        }
    },
 
    // Use 'forwardPorts' to make a list of ports inside the container available locally.
    // &amp;quot;forwardPorts&amp;quot;: [3000],
 
    // Use 'portsAttributes' to set default properties for specific forwarded ports.
    // More info: https://containers.dev/implementors/json_reference/#port-attributes
    &amp;quot;portsAttributes&amp;quot;: {
        &amp;quot;3000&amp;quot;: {
            &amp;quot;label&amp;quot;: &amp;quot;Hello Remote World&amp;quot;,
            &amp;quot;onAutoForward&amp;quot;: &amp;quot;notify&amp;quot;
        }
    },
 
    // Use 'postCreateCommand' to run commands after the container is created.
    &amp;quot;postCreateCommand&amp;quot;: &amp;quot;&amp;quot;,
 
    &amp;quot;postStartCommand&amp;quot;: &amp;quot;nohup bash -c 'postgres &amp;amp;'&amp;quot;
 
    // Uncomment to connect as root instead. More info: https://aka.ms/dev-containers-non-root.
    // &amp;quot;remoteUser&amp;quot;: &amp;quot;root&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="so-how-do-you-start-with-devcontainers"&gt;So how do you start with DevContainers?&lt;/h2&gt;
&lt;p&gt;There are 2 easy ways:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;(remote) Github Codespaces
By going to my repo, you can click &amp;quot;Create Codespace on Master&amp;quot; and get a running VSCode in the cloud with all those tools setup instantly.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;(at first build, the image might take time)&lt;/p&gt;
&lt;ol start="2"&gt;
&lt;li&gt;(local) Docker + VS Code
Ensure you have the &lt;a href="https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers"&gt;ms-vscode-remote.remote-containers&lt;/a&gt; extension installed in VS Code and Docker installed.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Clone the repo &lt;a href="https://github.com/ewinnington/DevContainerTemplate.git"&gt;https://github.com/ewinnington/DevContainerTemplate.git&lt;/a&gt;, then open it with VSCode. It should automatically detect the .devContainer and offer to rebuild the container image and open it up in the IDE for you.&lt;/p&gt;
&lt;p&gt;Once that is done, you should have access to a complete environment at the state you specified.&lt;/p&gt;
&lt;h2 id="whats-the-use-for-developers-at-corporations-where-computers-are-locked-down"&gt;What's the use for Developers at corporations where computers are locked down?&lt;/h2&gt;
&lt;p&gt;I think that providing developer windows machine with Git, Docker, WSL2 installed and using VS Code or another IDE that supports DevContainers is an excellent way forwards in providing a good fast and stable environment for developers to work faster and more efficiently. Using this configuration, any person showing up to a Hackathon would be able to start working in minutes after cloning a repository. It would really simplify daily operations, since every repo can provide the correct .DevContainer configuration, or teams can share a DevContainer basic configuration.&lt;/p&gt;
&lt;p&gt;This all simplifies operations, makes developer experience more consistent and increases productivity since you can move faster from one development environment to another in minutes. OnPrem → Remote VM → Cloudspace and back in minutes, without any friction.&lt;/p&gt;
&lt;p&gt;All in all, I'm convinced it is a tool that both IT support must understand and master how to best provide access to, and for developers to understand the devContainer to benefit from it.&lt;/p&gt;
&lt;p&gt;Have you used DevContainers? What is your experience?&lt;/p&gt;
</content>
		<summary>&lt;p&gt;It's been years now that we've had Infrastructure as Code (IaC), Containers and Desired state Configuration (DsC) tools to do our deployments. But these have been mostly focused on the deployment side of things, with fewer tools on the developer side. On the dev machine, installing and maintaining the development tools and package dependencies has been in flux, both in windows where finally tools like Ninite, Chocolatey and Winget allow management of dev tools, and on the linux side, which was always quite well served with apt - but has also gained Snap, Flatpack and other package management tools. The thing is, sometimes you need more that one version of a particular tool, Python3.10 and Python3.11, Java9 and Java17, Dotnet 4.8 and Dotnet 6, to work on the various projects you have during the day. Sometimes, they work side by side very well and sometimes they don't. And when they don't, it can be a long process to figure out why and also very difficult to get help without resorting to having a clean image refresh and starting again to install your dependencies.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://ewinnington.github.io/posts/Proxmox</id>
		<title>Proxmox 8 on sub $200 mini PCs</title>
		<link href="http://ewinnington.github.io/posts/Proxmox" />
		<updated>2023-07-01T22:00:00Z</updated>
		<content>&lt;h1 id="installing-proxmox-tailscale-win11-vms-and-automation"&gt;Installing Proxmox, Tailscale, Win11 VMs and Automation&lt;/h1&gt;
&lt;p&gt;This is a Beelink MiniS12 with an Intel N95s.&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/minipc.jpg" class="img-fluid" width="80%" alt="" /&gt;
(coffee cup for scale)&lt;/p&gt;
&lt;p&gt;Up until Proxmox 8 dropped about a week ago, I was unable to install Proxmox due to compatibility with the graphics driver. Now in 8, that was fixed, so I've been able to install Proxmox on several of my machines.&lt;/p&gt;
&lt;p&gt;The procedure is trivial: write the proxmox iso to a usb key via a software that will make the iso bootable. Boot the machine with the usb key inserted and select the correct boot drive, then follow the proxmox installation prompts.&lt;/p&gt;
&lt;p&gt;It all worked out of the box.&lt;/p&gt;
&lt;h2 id="clustering"&gt;Clustering&lt;/h2&gt;
&lt;p&gt;I was able to connect to my proxmox installed machine on the port :8006 via a browser on my home network. Next step was enabling the management of multiple machines via a single UI. So as soon as I had two machines with Proxmox installer, I was able to go to my primary, click on Create cluster, confirm. Get the Join token, connect to the socond machine and paste the join token into the &amp;quot;Join Cluster&amp;quot;. Worked out of the box.&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/proxmox/proxmox-clustering.png" class="img-fluid" width="80%" alt="" /&gt;&lt;/p&gt;
&lt;h2 id="removing-the-update-repositories-to-work-on-the-free-version-of-proxmox"&gt;Removing the Update Repositories to work on the Free version of Proxmox&lt;/h2&gt;
&lt;p&gt;To stay within the Free licensing of Proxmox and be able to do apt-get, remember to go for each machine and to remove the non-free repos in the repository list.&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/proxmox/proxmox-remove-repos" class="img-fluid" width="80%" alt="" /&gt;&lt;/p&gt;
&lt;h2 id="installing-tailscale"&gt;Installing Tailscale&lt;/h2&gt;
&lt;p&gt;I use &lt;a href="https://tailscale.com/"&gt;Tailscale&lt;/a&gt; at home to connect across multiple locations and roaming devices. Every time I add a Tailscale device, I am amazed at how easy it is.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -fsSL https://tailscale.com/install.sh | sh
tailscale up
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Two lines, one URL to visit and the machines were enrolled.&lt;/p&gt;
&lt;h2 id="vms-and-lxcs"&gt;VMs and LXCs&lt;/h2&gt;
&lt;p&gt;To create VMs and LXCs, you need to add iso images or Templates to your local storage:&lt;/p&gt;
&lt;p&gt;Container templates, you can create your own and upload them or simply click on &amp;quot;Templates&amp;quot; and get a couple of ready made ones for use in your containers.&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/proxmox/container-templates.png" class="img-fluid" width="80%" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;You can now get the official windows ISO from the microsoft website. So download it to your local machine and then Upload it to the ISO images.&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/proxmox/windows-iso.png" class="img-fluid" width="80%" alt="" /&gt;&lt;/p&gt;
&lt;h3 id="installing-windows-11-pro-and-activating-with-the-hardware-license"&gt;Installing Windows 11 Pro and activating with the Hardware license&lt;/h3&gt;
&lt;p&gt;I had previously logged in and linked to my microsoft account on the windows 11 pro licensed version of the OS for each of the machines. This meant that when installing the Windows11 Pro from the ISO onto a VM running on the machines, I was able to activate the machine by referring to the previous activation. #Windows11Pro allows itself to be reactivated with the license that came with the hardware, inside a proxmox8 VM of the same machine as long as you pass the host-cpu - or so it seems to me.&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/proxmox/activated.jpg" class="img-fluid" width="80%" alt="Proxmox-shell-mode-issue" /&gt;&lt;/p&gt;
&lt;h2 id="issues-i-had-and-solutions"&gt;Issues I had and solutions&lt;/h2&gt;
&lt;h3 id="console-not-connecting-to-lxc-containers"&gt;Console not connecting to LXC containers&lt;/h3&gt;
&lt;p&gt;Several times, either while connecting to a container in Proxmox directly, or after a containter migation, I was not able to use the integrated shell. I therefore had to change the Container &amp;quot;Options-&amp;gt;Console mode&amp;quot; to &amp;quot;shell&amp;quot; to make it connect every time.
&lt;img src="/posts/images/proxmox/Proxmox-Shell-mode.png" class="img-fluid" width="80%" alt="Proxmox-shell-mode-issue" /&gt;&lt;/p&gt;
&lt;h3 id="apt-get-issue-in-proxmox-containers"&gt;apt-get issue in proxmox containers&lt;/h3&gt;
&lt;p&gt;The first thing I do upon entering a container is nearly always apt-get update. And sometimes it breaks. I couldn't update or install. Here is my checklist:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Check you gave and ip address to the container in the &amp;quot;Network&amp;quot; section, either a dhcp or a static address.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Check your DNS servers: I had not noticed that after installing Tailscale, my DNS records were only pointing to the talescale DNS resolver. Adding back google (8.8.8.8) and cloudflare (1.1.1.1) to my proxmox hosts helped.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src="/posts/images/proxmox/Tailscale-dns-issue-proxmox.png" class="img-fluid" width="80%" alt="Tailscale-dns-issue-proxmox" /&gt;&lt;/p&gt;
&lt;p&gt;By fixing both of these I was able to get the apt-get running correctly.&lt;/p&gt;
&lt;h2 id="automation"&gt;Automation&lt;/h2&gt;
&lt;h3 id="installing-on-a-client-machine-the-proxmox-cli-tools"&gt;Installing on a client machine the Proxmox CLI Tools&lt;/h3&gt;
&lt;p&gt;I'm planning on checking out automation of deployment on proxmox. Making a note here of the command line installation of the tools&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo pip3 install pve-cli
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I'll also look into Terraform + Ansible for a proxmox deployment, or the Packer LXC to make container templates, but that is for next time.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;This is a Beelink MiniS12 with an Intel N95s.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://ewinnington.github.io/posts/network-vpn</id>
		<title>Network planning and VPN</title>
		<link href="http://ewinnington.github.io/posts/network-vpn" />
		<updated>2023-04-20T18:40:00Z</updated>
		<content>&lt;p&gt;I am in the process of setting up my homelab network between my two locations.&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/network/network.png" class="img-fluid" width="100%" alt="Network" /&gt;&lt;/p&gt;
&lt;h2 id="zone-z"&gt;Zone Z&lt;/h2&gt;
&lt;p&gt;Z has a single fiber connection via Swisscom to internet.&lt;/p&gt;
&lt;h3 id="inventory"&gt;Inventory&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;DeepThread is an AMD Threadripper 1920x running Windows 10.&lt;/li&gt;
&lt;li&gt;Minis3 and Minis4 are the Beelink MiniS12 N95s running Ubuntu Server 23.04.&lt;/li&gt;
&lt;li&gt;NAS is an an older QNAP TS-269L&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="vpn"&gt;VPN&lt;/h3&gt;
&lt;p&gt;With a L2TP VPN connection configured to allow remote access onto the network, so I can get Red (Surface Laptop) and Xr (iPhone) onto the network in case.&lt;/p&gt;
&lt;h2 id="zone-n"&gt;Zone N&lt;/h2&gt;
&lt;p&gt;N has two connections, a Starlink (v1 round) with only the powerbrick router and Sosh as a backup DSL provider (with an ADSL Router) both connected to a Ubiquity UDM-PRO-SE in Failover mode. Getting a VPN to N is a little more involved, since the UDM is behind a separate router on each WAN.&lt;/p&gt;
&lt;h3 id="inventory-1"&gt;Inventory&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Minis1 is the Beelink MiniS12 N95s running Windows 11. Planned to Switch to Ubuntu 23.04, but enjoying it VESA mounted behind a screen in the office currently.&lt;/li&gt;
&lt;li&gt;Minis2 is the Beelink MiniS12 N95s running Ubuntu Server 23.04. Currently rackmounted with the UDM-PRO.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="vpn-1"&gt;VPN&lt;/h3&gt;
&lt;p&gt;On the UDM-PRO, a VPN is configured with Ubiquity and I can use the iOS application WifiMan to access the network.
On Minis2, a &lt;a href="https://github.com/cloudflare/cloudflared"&gt;cloudflared docker&lt;/a&gt; is running, reaching up to Cloudflare and providing an Zero trust tunnel to expose several dockerized websites hosted on it.&lt;/p&gt;
&lt;h1 id="the-issue-at-hand"&gt;The issue at hand&lt;/h1&gt;
&lt;p&gt;I would like the N Minis1 &amp;amp; Minis2 to be able to access the Z NAS, ideally with a relatively simple connection that I can leave running all the time, to be able to pull files from the NAS and ideally also access the NAS's front-end application from inside my N location. I could connect to the SwisscomVPN every time I do something that requires connectivity to the NAS, but I would really ideally like a more permanent solution where I make the Z NAS &amp;quot;visible&amp;quot; in the N network. Or go full and establish a site-to-site VPN and simply make the two areas N and Z communicate seamlessly while still having local connectivity.&lt;/p&gt;
&lt;p&gt;Do you have any suggestions as to how best to accomplish this?&lt;/p&gt;
&lt;h2 id="section"&gt;22.04.2023&lt;/h2&gt;
&lt;p&gt;I now put a OpenVPN on the Qnap NAS to act as a S2S VPN. Not sure that will be the solution I keep for the long term but it works for now.&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/network/network2.drawio.svg" class="img-fluid" width="100%" alt="Network" /&gt;&lt;/p&gt;
</content>
		<summary>&lt;p&gt;I am in the process of setting up my homelab network between my two locations.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://ewinnington.github.io/posts/mini-pc</id>
		<title>The era of the sub $200 PC</title>
		<link href="http://ewinnington.github.io/posts/mini-pc" />
		<updated>2023-04-05T00:00:00Z</updated>
		<content>&lt;p&gt;I recently purchased (arrived yesterday!) two mini PCs, for 170.- CHF each (~$187). They sport the latest low power CPU from Intel, the N95, which is amazingly efficient, powerful and cheap.&lt;/p&gt;
&lt;p&gt;The Beelink Mini S12 with the N95 intel CPU 4c, 16 GB Ram, 512 GB NVME storage drive, with space for an additional 2.5&amp;quot; SSD inside too. Amazingly small and light to carry around.&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/minipc.jpg" class="img-fluid" width="80%" alt="" /&gt;
(coffee cup for scale)&lt;/p&gt;
&lt;p&gt;I just got two of these to put in a Kubernetes cluster, and I've been playing around with them and they are super impressive. They pack a punch and sip power at 9-20W. The internal video card does 4k youtube playback without any issue.&lt;/p&gt;
&lt;p&gt;Windows 11 comes included with the machine for the price. I've already put ubuntu server on one of them. These machines are powerful enough for family members to do all their online activities, zoom (with a webcam), or use as a media machine.&lt;/p&gt;
&lt;p&gt;Over the next weeks, I'll be documenting configuring them and setting them up to serve as highly available servers from my home, using a reverse VPN tunneling.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;I recently purchased (arrived yesterday!) two mini PCs, for 170.- CHF each (~$187). They sport the latest low power CPU from Intel, the N95, which is amazingly efficient, powerful and cheap.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://ewinnington.github.io/posts/learn-from-chatgpt-Crdt-OT</id>
		<title>Learning concepts from chatGPT - Operational Transform and Conflict-free Replicated Data Types</title>
		<link href="http://ewinnington.github.io/posts/learn-from-chatgpt-Crdt-OT" />
		<updated>2022-12-11T00:20:00Z</updated>
		<content>&lt;p&gt;I have been enjoying using &lt;a href="https://chat.openai.com/chat"&gt;chatGPT&lt;/a&gt; as a private tutor / reference on topics. My biggest issue is now keeping up with the knowledge and asking the right questions.&lt;/p&gt;
&lt;p&gt;But, remember that chatGPT is not all knowing and can get things wrong - even fabulating, so you have to double check the information. I did my best to validate the information with &lt;a href="https://mwhittaker.github.io/consistency_in_distributed_systems/3_crdt.html"&gt;Conflict-Free Replicated Data Types&lt;/a&gt;, &lt;a href="https://svn.apache.org/repos/asf/incubator/wave/whitepapers/operational-transform/operational-transform.html"&gt;Google's OT papers&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Operational_transformation"&gt;Wikipedia on Operational Transformation&lt;/a&gt;. I did not find any glaring errors in the text. For future crawlers, &lt;strong&gt;the following text was generated by chatGPT on 2022-12-11T01:12:45+0000&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;bold&lt;/strong&gt; is used for the prompts I send into chatGPT.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;italics&lt;/em&gt; is for my comments.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;can you explain CRDT's in data structures?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/gpt/gpt-logo.png" class="img-fluid" width="5%" alt="gpt-logo" /&gt;
Sure! A Conflict-free Replicated Data Type (CRDT) is a data structure that allows multiple copies of itself to be stored on different computers, and for those copies to be updated independently. This means that even when the copies are not connected to each other, they can still be updated and changed, and when they are eventually reconnected, they will automatically merge any conflicting changes in a way that preserves the consistency of the data. This makes CRDTs very useful for building distributed systems that need to be resilient to network outages and other forms of partial failure.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Is this similar to Google Wave's Operational Transform?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/gpt/gpt-logo.png" class="img-fluid" width="5%" alt="gpt-logo" /&gt;
Yes, Operational Transform (OT) is a technique that is related to CRDTs, and it is often used to solve the same kinds of problems. Like CRDTs, OT is a way of allowing multiple copies of a data structure to be updated independently and then merged together, but it uses a different approach to achieve this. OT works by applying a series of &amp;quot;operations&amp;quot; to the data, rather than directly modifying the data itself. This allows OT to detect and resolve conflicts between different copies of the data more easily, but it can also make it more complex to implement.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Can you give me an example of a CRDT design?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/gpt/gpt-logo.png" class="img-fluid" width="5%" alt="gpt-logo" /&gt;
Sure! One example of a CRDT is a &amp;quot;g-counter,&amp;quot; which is a data structure that stores a count. This data structure allows for the count to be incremented independently on different computers, and then merged together in a way that ensures that the final count is always correct. Here is one way that a g-counter could be implemented:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;class GCounter {
  Map&amp;lt;String, Integer&amp;gt; counts;

  void increment(String nodeId) {
    counts.put(nodeId, counts.getOrDefault(nodeId, 0) + 1);
  }

  void merge(GCounter other) {
    for (Map.Entry&amp;lt;String, Integer&amp;gt; entry : other.counts.entrySet()) {
      String nodeId = entry.getKey();
      int count = entry.getValue();
      counts.put(nodeId, Math.max(counts.getOrDefault(nodeId, 0), count));
    }
  }

  int getCount() {
    return counts.values().stream().mapToInt(x -&amp;gt; x).sum();
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;I never specified a programming language, so assume this is a kind of pseudocode&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In this implementation, the increment() method is used to increment the count on a particular node, and the merge() method is used to merge the counts from two different g-counters together. The getCount() method returns the current value of the count.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;And contrast with the design of an OT?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/gpt/gpt-logo.png" class="img-fluid" width="5%" alt="gpt-logo" /&gt;
Here is an example of how an Operational Transform (OT) might be implemented for a simple text editing application:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;class TextBuffer {
  StringBuilder text;

  void applyOperation(Operation op) {
    int start = op.getStartIndex();
    int end = op.getEndIndex();
    String newText = op.getText();
    text.replace(start, end, newText);
  }

  String getText() {
    return text.toString();
  }
}

class Operation {
  int startIndex;
  int endIndex;
  String text;

  int getStartIndex() {
    return startIndex;
  }

  int getEndIndex() {
    return endIndex;
  }

  String getText() {
    return text;
  }

  Operation transform(Operation other) {
    // Compute the transformed version of this operation, given the other operation
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;I never specified a programming language, so assume this is a kind of pseudocode&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In this example, the TextBuffer class represents the text being edited, and the Operation class represents a single edit operation (e.g. inserting or deleting some text). The applyOperation() method is used to apply an operation to the text buffer, and the getText() method is used to retrieve the current contents of the buffer. The transform() method is used to compute the transformed version of an operation, given another operation. This is used to ensure that concurrent operations on the same text buffer can be merged together consistently.&lt;/p&gt;
&lt;p&gt;One key difference between this approach and the g-counter example is that OT explicitly tracks the operations that are applied to the data, rather than directly modifying the data itself. This makes it easier to detect and resolve conflicts between concurrent operations, but it also makes the implementation more complex.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Can you list known CRDTs and their usage in a table?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/gpt/gpt-logo.png" class="img-fluid" width="5%" alt="gpt-logo" /&gt;
Here is a list of some common Conflict-free Replicated Data Types (CRDTs), along with a brief description of each one and some potential use cases:&lt;/p&gt;
&lt;table class="table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;CRDT&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Use Cases&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;G-Counter&lt;/td&gt;
&lt;td&gt;Stores a count that can be incremented independently on different nodes, and then merged together consistently.&lt;/td&gt;
&lt;td&gt;- Versioning systems that need to keep track of the number of times a file has been modified.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;PN-Counter&lt;/td&gt;
&lt;td&gt;Stores a count that can be incremented or decremented independently on different nodes, and then merged together consistently.&lt;/td&gt;
&lt;td&gt;- Tracking the number of likes or dislikes on a social media post.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Two-Phase Set&lt;/td&gt;
&lt;td&gt;Stores a set of unique values that can be added or removed independently on different nodes, and then merged together consistently.&lt;/td&gt;
&lt;td&gt;- Collaborative editing of a list of items (e.g. a to-do list).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;OR-Set&lt;/td&gt;
&lt;td&gt;Stores a set of values that can be added independently on different nodes, and then merged together consistently.&lt;/td&gt;
&lt;td&gt;- Storing the list of friends that a user has on a social network.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;LWW-Element-Set&lt;/td&gt;
&lt;td&gt;Stores a set of values, where each value has a timestamp associated with it. The value with the most recent timestamp wins in case of conflicts.&lt;/td&gt;
&lt;td&gt;- Storing the latest version of a user's profile on a social network.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Observed-Remove Set&lt;/td&gt;
&lt;td&gt;Stores a set of values that can be added or removed independently on different nodes, and then merged together consistently.&lt;/td&gt;
&lt;td&gt;- Storing the list of items that a user has added to their shopping cart on an e-commerce website.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;These are just a few examples of CRDTs, and there are many other types of CRDTs that have been developed for different types of data and applications. In general, CRDTs can be useful whenever you need to store and update data in a distributed system, and you want to ensure that the data remains consistent even in the face of network partitions and other forms of partial failure.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Can you make a similar table for OTs?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/gpt/gpt-logo.png" class="img-fluid" width="5%" alt="gpt-logo" /&gt;
Sure! Here is a table that lists some common Operational Transform (OT) techniques, along with a brief description of each one and some potential use cases:&lt;/p&gt;
&lt;table class="table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;OT Technique&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Use Cases&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Insert-Delete OT&lt;/td&gt;
&lt;td&gt;Allows for the insertion and deletion of text, and can detect and resolve conflicts between concurrent insertions and deletions.&lt;/td&gt;
&lt;td&gt;- Collaborative text editing in a real-time editor (e.g. Google Docs).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Block-Based OT&lt;/td&gt;
&lt;td&gt;Allows for the insertion, deletion, and movement of blocks of text, and can detect and resolve conflicts between concurrent insertions, deletions, and movements.&lt;/td&gt;
&lt;td&gt;- Collaborative editing of structured documents (e.g. a spreadsheet or a presentation).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Change-Based OT&lt;/td&gt;
&lt;td&gt;Allows for arbitrary changes to be made to a document, and can detect and resolve conflicts between concurrent changes.&lt;/td&gt;
&lt;td&gt;- Collaborative editing of free-form documents (e.g. a word processor or a code editor).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;State-Based OT&lt;/td&gt;
&lt;td&gt;Stores the complete state of the document at each point in time, and can detect and resolve conflicts between concurrent changes by comparing the state of the document before and after each change.&lt;/td&gt;
&lt;td&gt;- Collaborative editing of complex data structures (e.g. a database or a network topology diagram).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Hybrid OT&lt;/td&gt;
&lt;td&gt;Combines multiple OT techniques in order to support a wider range of operations and conflicts.&lt;/td&gt;
&lt;td&gt;- Collaborative editing of documents that contain both structured and unstructured data (e.g. a document that contains both text and images).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Composable OT&lt;/td&gt;
&lt;td&gt;Allows for OTs to be composed together in order to create more complex data structures and operations.&lt;/td&gt;
&lt;td&gt;- Building collaborative editing systems that support multiple types of data and operations (e.g. a system that supports text, images, and geometry).&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Like CRDTs, there are many different OT techniques that have been developed for different types of data and applications, and this table only covers a few of the most common ones. In general, OTs can be useful whenever you need to support real-time collaboration on data, and you want to ensure that concurrent changes can be merged together consistently.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;is there a use case to use both CRDTs and OTs together?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/gpt/gpt-logo.png" class="img-fluid" width="5%" alt="gpt-logo" /&gt;
It is possible to use Conflict-free Replicated Data Types (CRDTs) and Operational Transform (OT) techniques together in some cases, although this is not always necessary or desirable. Here are a few examples of situations where using both CRDTs and OTs together might be useful:&lt;/p&gt;
&lt;p&gt;If you have a data structure that is very complex and cannot be easily represented using a single CRDT, you could use a combination of CRDTs and OTs to break the data structure down into smaller, simpler pieces that can be managed independently. For example, you could use a CRDT to store the overall structure of the data, and then use OTs to handle the details of how the individual pieces of the data are updated and changed.&lt;/p&gt;
&lt;p&gt;If you have a data structure that is updated very frequently, and you need to ensure that updates can be applied and merged together quickly, you might use a CRDT to handle the &amp;quot;big picture&amp;quot; view of the data, and then use OTs to handle the finer-grained details of how the data is changed. This can help to reduce the amount of work that needs to be done in order to merge updates together, which can improve the overall performance of the system.&lt;/p&gt;
&lt;p&gt;If you have a data structure that needs to support both offline and online updates, you might use a CRDT to handle the offline updates, and then use OTs to handle the online updates. This can allow the system to continue functioning even when the network is unavailable, and then automatically merge any offline updates with the online updates when the network becomes available again.&lt;/p&gt;
&lt;p&gt;Overall, whether or not it makes sense to use CRDTs and OTs together in a particular system will depend on the specific requirements and constraints of the system, as well as the trade-offs between the different approaches.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Are there any other similar types of data structures that also cover this space?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/gpt/gpt-logo.png" class="img-fluid" width="5%" alt="gpt-logo" /&gt;
There are many different types of data structures and algorithms that can be used to solve similar problems to those addressed by Conflict-free Replicated Data Types (CRDTs) and Operational Transform (OT) techniques. Some examples of other approaches that are related to CRDTs and OTs include:&lt;/p&gt;
&lt;p&gt;Replication protocols: These are algorithms that are used to manage the process of replicating data between different nodes in a distributed system. Replication protocols can be used to ensure that multiple copies of the data are kept consistent with each other, and that conflicts between different copies of the data are resolved in a way that preserves the integrity of the data.&lt;/p&gt;
&lt;p&gt;Consensus algorithms: These are algorithms that are used to reach agreement on the state of a distributed system, even in the presence of faulty or malicious nodes. Consensus algorithms can be used to ensure that different nodes in the system have a consistent view of the data, and that updates to the data are applied in a way that is consistent with the consensus.&lt;/p&gt;
&lt;p&gt;Distributed hash tables (DHTs): These are data structures that are used to store and retrieve data in a distributed system. DHTs use a hash function to map data to different nodes in the system, and they can be used to ensure that data is stored and retrieved in a consistent and efficient manner.&lt;/p&gt;
&lt;p&gt;Distributed databases: These are databases that are designed to be used in a distributed system, and that provide mechanisms for storing, querying, and updating data in a way that is consistent and scalable. Distributed databases can use a variety of techniques, including CRDTs and OTs, to ensure that the data remains consistent and available even in the face of network partitions and other forms of partial failure.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Give me a example of a consensus algorithm that computes the final state of a text document that is edited in realtime in multiple distributed computers. I'm looking at a peer to peer network without central authority.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/gpt/gpt-logo.png" class="img-fluid" width="5%" alt="gpt-logo" /&gt;
One example of a consensus algorithm that could be used to compute the final state of a text document that is edited in realtime in multiple distributed computers is called the &amp;quot;Paxos&amp;quot; algorithm. This algorithm is a well-known method for reaching consensus in a distributed system, and it has been used in many different types of applications, including real-time collaborative text editing.&lt;/p&gt;
&lt;p&gt;Here is a high-level overview of how the Paxos algorithm might be used to compute the final state of a text document that is being edited in realtime by multiple users:&lt;/p&gt;
&lt;p&gt;Each user maintains a local copy of the text document, and as they make changes to the document, they also generate a sequence of &amp;quot;operations&amp;quot; that represent the changes they are making. These operations are sent to the other users in the network.&lt;/p&gt;
&lt;p&gt;When a user receives an operation from another user, they apply the operation to their local copy of the document. This may result in conflicts with other operations that have already been applied to the document, in which case the user will need to resolve the conflicts in some way (e.g. by using Operational Transform (OT) techniques).&lt;/p&gt;
&lt;p&gt;As the users continue to edit the document and generate new operations, they also participate in the Paxos consensus protocol. This involves sending messages to each other in order to agree on the sequence of operations that should be applied to the document. The messages are sent in a specific order, and they are used to ensure that all of the users eventually agree on the same sequence of operations.&lt;/p&gt;
&lt;p&gt;Once all of the users have agreed on the sequence of operations, they can apply those operations to their local copies of the document in order to compute the final state of the document. This final state will be the same for all of the users, and it will reflect all of the changes that have been made to the document by any of the users.&lt;/p&gt;
&lt;p&gt;The Paxos algorithm is a complex and subtle algorithm, and this is just a very high-level overview of how it might be used in this context. There are many different variations of the Paxos algorithm, and the specific details of how it is implemented will depend on the specific requirements and constraints of the system.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;I have been enjoying using &lt;a href="https://chat.openai.com/chat"&gt;chatGPT&lt;/a&gt; as a private tutor / reference on topics. My biggest issue is now keeping up with the knowledge and asking the right questions.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://ewinnington.github.io/posts/7Reasons-no-cache</id>
		<title>7 reasons to not use caching</title>
		<link href="http://ewinnington.github.io/posts/7Reasons-no-cache" />
		<updated>2022-12-04T16:30:00Z</updated>
		<content>&lt;p&gt;Inspired by &lt;a href="https://twitter.com/mjovanovictech"&gt;Milan Jovanović&lt;/a&gt; tweet on &lt;a href="https://twitter.com/mjovanovictech/status/1599124855542411264"&gt;5 reasons to use Redis for caching&lt;/a&gt;,&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/caching/5reasonsCaching.png" class="img-fluid" width="80%" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;and &lt;a href="https://twitter.com/danielmarbach"&gt;Daniel Marbach's&lt;/a&gt; response &amp;quot;&lt;a href="https://twitter.com/danielmarbach/status/1599352526888849408"&gt;Now I want to see five reasons to avoid caching ✋😂&lt;/a&gt;&amp;quot;&lt;/p&gt;
&lt;p&gt;&lt;img src="/posts/images/caching/5reasonsNoCaching.png" class="img-fluid" width="80%" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;I found &lt;a href="https://twitter.com/ThrowATwit/status/1599356806874427392"&gt;seven reasons to not introduce caching&lt;/a&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Caching can increase complexity in your application, as you need to manage the cached data and ensure it remains consistent with the underlying data store.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Caching can increase latency, as the cache itself introduces an additional lookup step.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Caching can be expensive, both in terms of the additional hardware and storage required for the cache, and the overhead of managing the cache itself.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Caching can be unreliable, as cached data can become stale or inconsistent if it is not adequately managed or invalidated.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Caching can be a security risk, as sensitive data that is stored in the cache may be vulnerable to unauthorized access or exposure. It takes additional effort to ensure that the correct authorizations are applied to cached data, increasing application complexity.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Caching can be harder to debug. To determine why a piece of data is not being retrieved from the cache or is being retrieved from the underlying data store instead is difficult. This can make it challenging to diagnose and fix performance issues related to caching.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Caching can create additional maintenance overhead, as you need to monitor the cache and ensure it is working properly. Monitoring cache hit and miss rates, ensuring that the cache is not getting too full, and periodically purging expired or stale data from the cache.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;and a bonus &lt;a href="https://mobile.twitter.com/joslat/status/1599518029649678336"&gt;8.&lt;/a&gt; from &lt;a href="https://mobile.twitter.com/joslat"&gt;Jose Luis Latorre&lt;/a&gt;
&amp;quot;8. It should be also properly tested, and stress tested... without mention the security testing as well should include a check on this layer too... which would bring us to point 3. More expensive ;)&amp;quot;&lt;/p&gt;
&lt;p&gt;Introducing Caching into any architecture is a decision that must be made with care. We have to ask if it helps us fulfill a business requirement (latency requirements), and improves quality or responsiveness for the end user. And we must ensure the solution is appropriate in terms of cost of operation and cost of monitoring and support. Additionally, the security aspects of a cache should be considered in the solution design.&lt;/p&gt;
&lt;p&gt;In software architecture, there are very few single answers, everything is a compromise. Caching is a great hammer and use it when it is appropriate, but remember not every problem is a nail.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;Inspired by &lt;a href="https://twitter.com/mjovanovictech"&gt;Milan Jovanović&lt;/a&gt; tweet on &lt;a href="https://twitter.com/mjovanovictech/status/1599124855542411264"&gt;5 reasons to use Redis for caching&lt;/a&gt;,&lt;/p&gt;</summary>
	</entry>
</feed>