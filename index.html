
<!DOCTYPE html>
<!--
	Phantom by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html lang="en">

	<head>
		<title>Eric Winnington</title>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1" />		
        <link href="/assets/css/highlight.css" rel="stylesheet">
		<!--[if lte IE 8]><script src="/assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="/assets/css/main.css" />
		<!--[if lte IE 9]><link rel="stylesheet" href="/assets/css/ie9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="/assets/css/ie8.css" /><![endif]-->
        <link href="/assets/css/override.css" rel="stylesheet" />

		<meta name="description" content="A collection of thoughts, code and snippets." />
		<link type="application/rss+xml" rel="alternate" title="Eric Winnington" href="/feed.rss" />
				<link type="application/atom+xml" rel="alternate" title="Eric Winnington" href="/feed.atom" />
		<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
		<link rel="icon" href="/favicon.ico" type="image/x-icon">

		<meta name="application-name" content="Eric Winnington" />
		<meta name="msapplication-tooltip" content="Eric Winnington" />
		<meta name="msapplication-starturl" content="/" />

		<meta property="og:title" content="Eric Winnington" /> 
		<meta property="og:type" content="website" />
		<meta property="og:url" content="http://ewinnington.github.io/" />
		<!-- TODO: More social graph meta tags -->

        <script src="/assets/js/highlight.pack.js"></script>   
		
        <meta name="twitter:card" content="summary" />
<meta name="twitter:site" content="@ewinnington" />
<meta name="twitter:title" content="Eric Winnington" />
<meta name="twitter:description" content="A collection of thoughts, code and snippets." />

<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
		
	</head>

	<body>
		<!-- Wrapper -->
		<div id="wrapper">

			<!-- Header -->
			<header id="header">
				<div class="inner">

					<!-- Logo -->
					<a href="/" class="logo">
						<span class="title">Eric Winnington</span>
					</a>

					<!-- Nav -->
					<nav>
						<ul>
							<li><a href="#menu">Menu</a></li>
						</ul>
					</nav>

				</div>
			</header>

			<!-- Menu -->
			<nav id="menu">
				<h2>Menu</h2>
				<ul>
					        <li><a href="/posts">Archive</a></li>
        <li><a href="/tags">Tags</a></li>
        <li><a href="/about">About Me</a></li>
        <li><a href="/videos">Videos on programming</a></li>

				</ul>
			</nav>

			<!-- Main -->
			<div id="main">
				<div class="inner">

    <header>
        <h1>A collection of thoughts, code and snippets.</h1>
    </header>

					
					<div id="content">
						
<div class="row">
    <div class="8u 12u$(medium)">
                <div>
                    <a href="/posts/SimplePerf2">
                        <h2>In-memory software design 2025 - Applied to energy</h2>
                    </a>
                    <p>Posted on Saturday, 15 February 2025</p>
<h1 id="in-memory-software-design">In-memory software design</h1>
<p>In the last blog, <a href="https://ewinnington.github.io/posts/SimplePerf">In-memory performance in 2025</a> we looked at a simple design for an energy aggregation system to aggregate 9.8 GB of 100'000 trades rolled out and saw we got about 40 GB/s performance.</p>
<p>I upped the numbers of trades to 500'000 trades (rolled out over quarter hours, with average trade length of 120 days in quarter hours) and validated that on a ~50 GB dataset, we are running at ~1200 ms for the complete aggregation, confirming a linear scaling of the compute time.</p>
<p>But this brings us to a point where on commodity hardware, we are running our aggregations over 1 second in duration. So how can we improve this? This time, instead of trying to brute force, let's bring in some other techniques from databases: Indexes!</p>
<h2 id="simple-indexes-for-in-memory-aggregations">Simple indexes for in-memory aggregations</h2>
<p>For our use case, we are going to look at two meta-data properties on our trades, Trader and Delivery area, but the concept scales efficiently to large collections of meta-data, because we are dealing with such a small amounts of Trades (500k).</p>
<h3 id="building-the-index">Building the index</h3>
<p>We can generate at insertion a set for each meta-data, trader and delivery area.</p>
<pre><code class="language-cpp">    // =================
    // 2) Build Indexes
    // =================
    // We'll do single-attribute indexes: trader -&gt; set of tradeIDs, area -&gt; set of tradeIDs
    // For large data, consider using std::vector&lt;size_t&gt; sorted, or some other structure.

    std::unordered_map&lt;std::string, std::unordered_set&lt;size_t&gt;&gt; traderIndex;
    traderIndex.reserve(10);  // if you know approx how many traders you have to avoid resizing continuously

    std::unordered_map&lt;std::string, std::unordered_set&lt;size_t&gt;&gt; areaIndex;
    areaIndex.reserve(10);
</code></pre>
<p>I'm using an unordered set, but could also use an ordered set, this might be more efficient.</p>
<p>As keys, I'm using the string data of the meta-data on the trade - in production this would probably be integer keys, but for this use case is sufficient.</p>
<pre><code class="language-cpp">    for (size_t i = 0; i &lt; trades.size(); ++i)
    {
        const auto&amp; t = trades[i];
        traderIndex[t.trader].trades(i);
        areaIndex[t.deliveryArea].trades(i);
    }
</code></pre>
<p>When we insert a set of trades, we can add them to the set index.</p>
<p>As a linear pass, it's extremely efficient to build this index and cheap to keep it updated as we insert new trades.</p>
<h3 id="using-the-index">Using the index</h3>
<p>If we have a search query on a single attribute, we can now use the simple index and have directly the result.</p>
<p>But if we are doing a query on two (or more), we are going to take use the smallest index first and match with the largest index. The unordered_set gives an acceptable performance for <code>bigger.find(id)</code> but you can probably do even better with a set structure that is optimized for intersections. You can benchmark using <code>std::set_intersection</code> against my simple implementation if you are using sorted sets.</p>
<pre><code class="language-cpp">// ===================================
    // 3) Use the indexes for filtering
    // ===================================
    // For instance, let's do a query: TraderX, Area2
    // We'll find the intersection of (all trades for TraderX) and (all trades for Area2).

    std::string queryTrader = &quot;TraderX&quot;;
    std::string queryArea   = &quot;Area2&quot;;

    // Get sets from the index
    // (handle the case if the key doesn't exist -&gt; empty set)
    auto itT = traderIndex.find(queryTrader);
    auto itA = areaIndex.find(queryArea);

    if (itT == traderIndex.end() || itA == areaIndex.end()) {
        std::cout &lt;&lt; &quot;No trades found for &quot; &lt;&lt; queryTrader &lt;&lt; &quot; AND &quot; &lt;&lt; queryArea &lt;&lt; &quot;\n&quot;;
        return 0;
    }

    const auto&amp; traderSet = itT-&gt;second;
    const auto&amp; areaSet   = itA-&gt;second;

    // Intersection
    // We'll create a vector of trade IDs that are in both sets
    // For speed, we can iterate over the smaller set and check membership in the larger set.
    const auto&amp; smaller = (traderSet.size() &lt; areaSet.size()) ? traderSet : areaSet;
    const auto&amp; bigger  = (traderSet.size() &lt; areaSet.size()) ? areaSet   : traderSet;

    std::vector&lt;size_t&gt; intersection;
    intersection.reserve(smaller.size());  // a safe upper bound if all the smaller set is selected, to avoid resizing

    for (auto id : smaller)
    {
        if (bigger.find(id) != bigger.end())
        {
            intersection.push_back(id);
        }
    }
</code></pre>
<p>This gives us the intersection and we iterate it to do the summation - again, here we are summing up over the entire year into a single value, but we could just as easily be doing daily, weekly or monthly sums.</p>
<pre><code class="language-cpp">    // ====================================
    // 4) Aggregate deliveries for matches
    // ====================================
    // Let's sum up total power / total value for the intersection set.

    long long totalPower = 0;
    long long totalValue = 0;

    #ifdef _OPENMP
    #pragma omp parallel for num_threads(4) reduction(+:totalPower, totalValue)
    #endif
    for (auto id : intersection)
    {
        const Trade&amp; t = trades[id];
        // sum up all deliveries
        for (const auto&amp; dd : t.dailyDeliveries)
        {
            for (int slot = 0; slot &lt; 100; ++slot)
            {
                totalPower += dd.power[slot];
                totalValue += dd.value[slot];
            }
        }
    }
</code></pre>
<p>This addition here is single threaded, but you can also use OpenMP to accelerate it - this will require some tuning, you don't want to use too many threads for these smaller aggregations, the <code>omp parallel for num_threads(4)</code> can be added to example limit to 4 threads. (note: I added the openMP in the code above).</p>
<p>Generally you get a 10x or more acceleration in single core, depending on how selective the indexes you are using are- In parallel, I'm getting 40-50x acceleration in multi-core with a num_threads to 4.</p>
<h2 id="why-arrayint100">Why array&lt;int,100&gt; ?</h2>
<p>In my last post, I used an array of 100 points in a day. I'm using it because it's simplest to have all days have the same &quot;size&quot; for memory alignment, therefore if I am calculating days in local time I have a 25h and 23h hour day once a year. I would generally prefer to work in UTC and have constant 24h days -- but for some reason humans prefer local time so it aligns with expectations.</p>
<p>Just be clear with the developer if you use an internal UTC or Local time representation. If using local make sure to:</p>
<ul>
<li>Properly sanitize your inputs, check trades fill the 96 quarter hours only for all days apart from the short (92) and long day (100) and zero fill the remainder.</li>
<li>Keep summing on all 100 hours for summations, the 4% extra index length is not worth an if statement in the inner loop of the code - try to keep loops jump free.</li>
</ul>
<h2 id="how-to-deal-with-canceled-amended-recalled-trades">How to deal with Canceled / Amended / Recalled trades ?</h2>
<p>My first answer is don't! Let me explain: Normal trades should represent the 99.9% or 99.99% of your deals - unless there's something you haven't told me about the way you are trading!</p>
<p>We can design this by having a tradeStatus on the trade.</p>
<p>int TradeStatus:</p>
<ul>
<li>0 : trade is valid</li>
<li>1 : trade is canceled</li>
<li>2 : trade is amended (ie. replaced by a new one)</li>
<li>... : any other status necessary</li>
</ul>
<p>When a trade is canceled, we leave it in the trade vector, but simply set the tradeStatus to a non-zero value, and skip it with a test at the beginning of the aggregation.</p>
<pre><code class="language-cpp">    #ifdef _OPENMP
    #pragma omp parallel for num_threads(4) reduction(+:totalPower, totalValue)
    #endif
    for (auto id : intersection)
    {
        const Trade&amp; t = trades[id];
        if(t.tradeStatus != 0) continue;  // skip canceled/amended trades
</code></pre>
<p>If the trade is amended, same thing, we add a new trade to our list of trades and set the previous one to amended status. Generally, the this is not using up much memory. If it ever becomes a problem, we could:</p>
<ol type="a">
<li>have a &quot;trade compression&quot; which removes all non-zero trade status from the vector.</li>
<li>flush the entire trade vector and reload the whole set.</li>
</ol>
<p>Depending on your implementation, the flushing and reloading might be just as fast - not every program needs to stay resident in memory all the time.</p>
<h2 id="snapshot-state-to-disk-for-recovery-or-storage-fork-as-in-redis">Snapshot state to disk for recovery or storage - Fork() as in Redis</h2>
<p>If we want to take snapshots of the state,  we can get inspired from Redis' famous <a href="https://architecturenotes.co/i/143231289/forking">fork() snapshotting technique</a>.</p>
<p>Use this when needing to snapshot a large data structure in RAM to disk (serialize the entire state), without blocking our main process from accepting new trades for the entire duration of the write.</p>
<h3 id="how-fork-helps">How fork() helps</h3>
<p>On Linux, calling fork() creates a child process that initially shares the same physical memory pages as the parent.</p>
<p>Copy-on-write (CoW): If either the parent or the child writes to a page after the fork, the kernel duplicates that page so each process sees consistent data.</p>
<p>The child process can serialize the in-memory data (in a consistent state from the moment of forking) to disk, while the parent continues to run to accept new trades. New trades arriving in the parent process after the fork will not affect the child’s view of memory. The child effectively sees a snapshot as of the fork().</p>
<p>You want the data structure to be in a consistent state at the instant of fork().
A brief lock (or pause writes) just before the fork() is triggered, ensuring no partial updates. Immediately after fork() returns, you can unlock, letting the parent continue. Meanwhile, the child proceeds to write out the data.</p>
<p>We can store an atomic counter value in the program that represents the last tradeid inserted or a state version. This gives you a “version” or “stamp” number for the dataset.</p>
<p>I won't put the full code for that here, since the design is a little more involved, but the basics are:</p>
<pre><code>static std::atomic&lt;long&gt; g_version{0}; //snapshot version id 
static std::mutex g_tradeMutex;  // protect g_trades from concurrent modification - lock on write to 

// ---------------------------------------------------
// fork() to create a child that writes the snapshot
// ---------------------------------------------------
int snapshotNow(const char* filename) {
    // 1) Acquire short lock to ensure no partial updates in progress
    g_tradeMutex.lock();
    long snapVer = g_version.load(std::memory_order_relaxed);

    // 2) Fork
    pid_t pid = fork();
    if(pid &lt; 0) {
        // error
        std::cerr &lt;&lt; &quot;fork() failed\n&quot;;
        g_tradeMutex.unlock();
        return -1;
    }

    if(pid == 0) {
        // child
        // We have a consistent view of memory as of the fork.
        // release the lock in the child
        g_tradeMutex.unlock();

        // write the snapshot
        writeSnapshotToDisk(filename, snapVer);

        // exit child
        _exit(0);
    } else {
        // parent
        // release the lock and continue
        g_tradeMutex.unlock();
        std::cout &lt;&lt; &quot;[Parent] Snapshot child pid=&quot; &lt;&lt; pid 
                  &lt;&lt; &quot;, version=&quot; &lt;&lt; snapVer &lt;&lt; &quot;\n&quot;;
        return 0;
    }
}
</code></pre>
<p>In essence we have two processes continuing from the same command fork() return, each taking one branch.</p>
<h2 id="data-io">Data I/O</h2>
<p>To integrate your cpp aggregation software into the rest of your stack depends on the software running around it.</p>
<p>You can run the application as an on-demand aggregation, loading everything to memory, doing the aggregation and exiting - leaving the server to do something else - this can be worth it if you only do aggregations on-demand and can afford the load time of a second or two from your NVME storage.</p>
<p>You can keep the cpp program running either :</p>
<ul>
<li>exposing an http RestAPI (<a href="https://github.com/microsoft/cpprestsdk">https://github.com/microsoft/cpprestsdk</a> is a good library for that, I've used it before).</li>
<li>having a GRPC endpoint for performance</li>
<li>receiving data from a Kafka stream - I'm sure Confluent can give you a good example of that.</li>
</ul>
<h2 id="summary">Summary</h2>
<p>In-memory data aggregation using cpp is relatively easy to write and maintain. Cpp is no longer the terrible monster it was - auto pointers help and using std:: components makes everything simple. OpenMP is an easy win to add to compute or memory intensive sections.</p>
<h4 id="sidenotes">Sidenotes</h4>
<p>On Windows you can get everything you need to compile cpp by installing:</p>
<pre><code>winget install LLVM.LLVM
</code></pre>
<p>On Linux (or wsl), you need to install the following:</p>
<pre><code>sudo apt-get update
sudo apt-get install clang
sudo apt-get install libomp-dev 
</code></pre>
<p>Then in both os, you can usually run a compilation on your source file (inmem_agg_omp.cpp) using:</p>
<pre><code>clang++ -O3 -march=native -flto -ffast-math -fopenmp -o inmem_agg_omp inmem_agg_omp.cpp
</code></pre>
						<hr>
                </div>       
                <div>
                    <a href="/posts/SimplePerf">
                        <h2>Performance of in-memory in 2025</h2>
                    </a>
                    <p>Posted on Thursday, 13 February 2025</p>
<h1 id="performance-of-in-memory-software-in-2025">Performance of in-memory software in 2025</h1>
<p>Yesterday, at an Energy panel in Essen, I mentioned that some heavy calculations should be done in-memory. It is something that people have a tendency to dismiss because generally they are not aware of the capability and speed of modern CPUs and RAM. A 32GB dataset in memory can now be processed every second by a commodity cpu in your laptop.</p>
<p><a href="https://tailscale.com/blog/living-in-the-future">Living in the Future, by the numbers</a> is a great article on the progress we have had since 2004:</p>
<ul>
<li>CPU Compute is 1000x faster</li>
<li>Web servers are 100x faster</li>
<li>Ram is 16x to 750x larger</li>
<li>SSD can do 10'000x more transactions per second.</li>
</ul>
<p>You can also see this progression on <a href="https://techcommunity.microsoft.com/blog/azurehighperformancecomputingblog/announcing-azure-hbv5-virtual-machines-a-breakthrough-in-memory-bandwidth-for-hp/4303504">Azure with the high-compute servers</a>, Microsoft and AMD are packing so much more memory bandwidth in modern compute.</p>
<p><img src="https://github.com/user-attachments/assets/b7561026-767e-4dc4-97a5-e5306c3fa36a" class="img-fluid" width="60%" alt="image" /></p>
<p>We are going to have 7 TB/s of memory bandwidth!</p>
<p><img src="https://github.com/user-attachments/assets/f2c78f96-0eb0-4359-9f65-963b2b4b4f7b" class="img-fluid" width="60%" alt="image" /></p>
<p>You can now run super-computer level problems on Azure!</p>
<p>But what does that all mean? What can we do even on a commodity laptop? I have a Latitude 9440 laptop on my desk here, with a 13th gen i7-1365U with 32 GB of RAM.</p>
<h2 id="energy-trade-aggregation">Energy Trade aggregation</h2>
<p>Let's start with a small calculation from the world of Energy. I have 100'000 trades, these trades affect one or multiple quarter hours of one year (8'784 hours =&gt; 35'136 quarter hours).</p>
<p>Pulling out a little C++, completely unoptimized, how long does it take to aggregate them and how much RAM is used?</p>
<pre><code class="language-cpp">// A struct to hold the daily delivery arrays (power, value).
struct DailyDelivery
{
    int dayOfYear;  // 1..365
    std::array&lt;int, 100&gt; power; 
    std::array&lt;int, 100&gt; value; 
};

// A struct to hold the trade metadata.
struct Trade
{
    int tradeId;
    std::string trader;        // e.g. &quot;TraderX&quot;
    std::string deliveryArea;  // e.g. &quot;Area1&quot;, &quot;Area2&quot;
    // You could store time points, but we'll just store day indexes for simplicity.
    // Real code might store start_delivery, end_delivery as std::chrono::system_clock::time_point.
    int startDay; // 1..365
    int endDay;   // 1..365

    std::vector&lt;DailyDelivery&gt; dailyDeliveries;
};
</code></pre>
<p>The Daily Delivery structure represents a day of delivery, with 100 slots (for the 25h day =&gt; 100 quarter hours).</p>
<ul>
<li>I'm storing the delivery of power in kW as an int32, meaning in a single trade I can do –2'147'483'648 kW to 2'147'483'647 kW.</li>
<li>Same thing for the value, we store the individual value of the MW in milli values (decimal shift 3), so each MW could be priced at -2'147'483.648 € to 2'147'483.647 €.</li>
</ul>
<p>The Trade stores:</p>
<ul>
<li>metadata: Trader and DeliveryArea. We could add as many metadata elements as we need, but for simplicity in the demo, I only use this</li>
<li>a dailyDeliveries vector containing the array of all days affected by the trade.</li>
</ul>
<p>Now if we wanted to see what is the total sum of power of all trades, the sum of TraderX and the sum of TraderX's deals in Area1 and Area2, we can runn the aggregation over all the memory. This is completely straight forward code, no optimizations what so ever.</p>
<pre><code class="language-cpp">    // --------------------------------------
    // 2) Run the aggregations (measure time)
    // --------------------------------------

    // The aggregates we want:
    // (a) All Trader total (yearly - all zones)
    // (b) Trader X total
    // (c) Trader X / Area1
    // (d) Trader X / Area2
    //
    // We'll assume we only have TraderX, so &quot;All Trader&quot; == &quot;TraderX&quot; in this simple version.
    //
    // But let's keep it generic. If you had multiple traders, you'd do some checks:
    //
    // For Weighted Average Cost = total_value / total_power (where total_power != 0)

    // We'll measure the time for a single pass that gathers all these sums.

    using Clock = std::chrono::steady_clock;
    auto startTime = Clock::now();

    long long all_totalPower = 0;
    long long all_totalValue = 0;

    long long traderX_totalPower = 0;
    long long traderX_totalValue = 0;

    long long traderX_area1_totalPower = 0;
    long long traderX_area1_totalValue = 0;

    long long traderX_area2_totalPower = 0;
    long long traderX_area2_totalValue = 0;

    for(const auto&amp; trade : trades)
    {
        // (a) &quot;All Trader&quot; sums:
        //    Summation for all trades, all areas, all days
        //    Because this example is all TraderX, you might have to adapt if you had multiple traders
        for(const auto&amp; dd : trade.dailyDeliveries)
        {
            for(int slot = 0; slot &lt; 100; ++slot)
            {
                all_totalPower += dd.power[slot];
                all_totalValue += dd.value[slot];
            }
        }

        // (b) If trade.trader == &quot;TraderX&quot;
        if(trade.trader == &quot;TraderX&quot;)
        {
            for(const auto&amp; dd : trade.dailyDeliveries)
            {
                for(int slot = 0; slot &lt; 100; ++slot)
                {
                    traderX_totalPower += dd.power[slot];
                    traderX_totalValue += dd.value[slot];
                }
            }

            // (c) and (d) by area
            if(trade.deliveryArea == &quot;Area1&quot;)
            {
                for(const auto&amp; dd : trade.dailyDeliveries)
                {
                    for(int slot = 0; slot &lt; 100; ++slot)
                    {
                        traderX_area1_totalPower += dd.power[slot];
                        traderX_area1_totalValue += dd.value[slot];
                    }
                }
            }
            else if(trade.deliveryArea == &quot;Area2&quot;)
            {
                for(const auto&amp; dd : trade.dailyDeliveries)
                {
                    for(int slot = 0; slot &lt; 100; ++slot)
                    {
                        traderX_area2_totalPower += dd.power[slot];
                        traderX_area2_totalValue += dd.value[slot];
                    }
                }
            }
        }
    }

    auto endTime = Clock::now();
    auto durationMs = std::chrono::duration_cast&lt;std::chrono::milliseconds&gt;(endTime - startTime).count();
</code></pre>
<p>How long do you think that takes on a commodity laptop? It's 9.8 GB of RAM to scan and fully aggregate. This is also running inside a VM on my Windows WSL instance, with other software running at the same time.</p>
<pre><code>Time for in-memory aggregation: 907 ms
--- Memory usage statistics (approx) ---
Total bytes allocated (cumulative): 9822202136 bytes
Peak bytes allocated (concurrent):  9822202136 bytes
Current bytes allocated:            9822202136 bytes
</code></pre>
<h2 id="parallelization">Parallelization</h2>
<p>Since we are running on a multicore CPU, we can use more than one core to do the aggregation. With OpenMP, it's extremely simple to setup some parallelization for the compute. At the beginning of the loop, we can define a parallel aggregation for reduction, meaning a final sum.</p>
<pre><code class="language-cpp">    // Parallel over trades
    // The 'reduction(+: variableList)' tells OpenMP to create private copies of
    // these variables in each thread, accumulate them, and then combine them
    // at the end.
#ifdef _OPENMP
#pragma omp parallel for reduction(+ : all_totalPower, all_totalValue, \
                                       traderX_totalPower, traderX_totalValue, \
                                       traderX_area1_totalPower, traderX_area1_totalValue, \
                                       traderX_area2_totalPower, traderX_area2_totalValue)
#endif
    for (std::size_t i = 0; i &lt; trades.size(); ++i)
    {
        const auto&amp; trade = trades[i];
</code></pre>
<p>With this, we improve the time to aggregate on the laptop to: 241ms. This means we can now do the <strong>complete aggregation on a laptop 4x per second</strong> - even on a completely unoptimized, simplistic memory structure for trades.</p>
<pre><code>Time for in-memory aggregation: 241 ms
</code></pre>
<p>So when you are doing large numerical aggregations or calculations, ask yourself - can I do this in RAM? If so, you might be surprised at how quickly and efficiently you can do it with modern cpp.</p>
<h2 id="performance-vs-memory-bandwidth">Performance vs Memory bandwidth</h2>
<p>This completely unoptimized implementation is running at :</p>
<p>Data size (GB) / time (s) = bandwidth (GB/s) =&gt; 9.8 GB / 0.241 s ≈ <strong>40.7 GB/s</strong></p>
<p>My laptop has approx ~96 GB/s of memory bandwidth. I calculate it as follows: LPDDR5 at 6000 MT/s, 8 bytes, dual channel = 6000 * 8 * 2 =~ 96 GB/s. My laptop has less than half bandwidth of the HC family on Azure (AMD EPYC™ 7003-series CPU) using CPUs that were released in 2021. Still impressive for my laptop, but it shows you could do much better.</p>
<p>If I really needed to optimize, I would re-organise the data structures to improve the memory aligment as an initial step. With that, we should get closer to the theoretical bandwidth of the machine.</p>
						<hr>
                </div>       
                <div>
                    <a href="/posts/network-tailscale">
                        <h2>Network with Tailscale</h2>
                    </a>
                    <p>Posted on Monday, 13 January 2025</p>
<p>I updated my OpenVPN based network to use Tailscale instead in 2023 and it is game changing. I have used Tailscale ever since. I simply did not update my blog and network diagram.</p>
<p><img src="/posts/images/network/network-update.png" class="img-fluid" width="100%" alt="Network" /></p>
<p>With <a href="https://tailscale.com/">Tailscale</a>, all my machines appear seamlessly on a single control pane and I can reach any of them from any device.</p>
<h2 id="zone-z">Zone Z</h2>
<p>Z has a single fiber connection via Swisscom to internet.</p>
<h3 id="inventory">Inventory</h3>
<ul>
<li>DeepThread is an AMD Threadripper 1920x running Windows 10.</li>
<li>Minis4 is the Beelink MiniS12 N95s running Ubuntu Server 24.10.</li>
<li>NAS is an an older QNAP TS-269L</li>
</ul>
<h2 id="zone-n">Zone N</h2>
<p>N has two connections, a Starlink (v1 round) with only the powerbrick router and Sosh as a backup DSL provider (with an ADSL Router) both connected to a Ubiquity UDM-PRO-SE in Failover mode.</p>
<h3 id="inventory-1">Inventory</h3>
<ul>
<li>Minis1 is the Beelink MiniS12 N95s running Windows 11, enjoying it VESA mounted behind a screen in the office currently. I originally thought I would also put Ubuntu, but a windows machine is useful.</li>
<li>Minis2 and Minis3 are  the Beelink MiniS12 N95s running Ubuntu Server  24.10. Currently rackmounted with the UDM-PRO.</li>
</ul>
<h3 id="vpn">VPN</h3>
<p>On the UDM-PRO, a VPN is configured with Ubiquity and I can use the iOS application WifiMan to access the network. It's really a backup of a backup solution to have Wifiman.</p>
<p>On Minis2 and minis4, a <a href="https://github.com/cloudflare/cloudflared">cloudflared docker</a> is running, reaching up to Cloudflare and providing an Zero trust tunnel to expose several dockerized websites hosted on it.</p>
<p>I made a <a href="https://suno.com/song/fb47c594-b22d-4504-83a1-75d8df705194">Suno song on how awesome</a> it is.</p>
						<hr>
                </div>       
                <div>
                    <a href="/posts/2025-predictions">
                        <h2>On the Horizon - 2025 - My predictions</h2>
                    </a>
                    <p>Posted on Monday, 13 January 2025</p>
<h1 id="on-the-horizon-2025-my-predictions">On the Horizon - 2025 - My predictions</h1>
<p>In a way, predicting 2025 is somewhat harder and easier than 2024, a lot of what I see are the seeds of 2024 coming to bloom. But for what we will have by the end of the year is really unclear to me - but we will see some impact in research for sure - AI assisted research in fields will explode this year.</p>
<h2 id="predictions">Predictions:</h2>
<h3 id="ai-video-ai-audio">AI Video / AI Audio</h3>
<ul>
<li><strong>Cinema level visuals made on generative environments</strong> - not only creating a video, but creating the entire space so that the camera can be then moved in post production - will become available to the high end customers. This approach is a complement to the diffusion models, which only generate a few frames of temporal consistency - using this method will allow much better time coherence and consistency / object permanence. Nvidia Cosmos is closest to this and I think the next version of it will satisfy this point. I expect many video models to actually start using this method with temporal control nets to avoid the inconsistency of object permanence.</li>
<li>AI Chatbots will be allowed to sing, make music and emote more. While some LLMs are already capable of such things, they are generally removed in post-training but I think these restrictions will be removed this year.  Suno's lead on AI Music gets folded into a leading model, meaning you'll be able to ask a ChatGPT competitor &quot;make me a song, with lyrics and background track&quot;.</li>
<li>Visual understanding models will be commonly deployed - Meaning point your camera and get full descriptions of what you see, It's nearly there anyway.</li>
</ul>
<h3 id="ai-agents">AI Agents</h3>
<p>By agent, I define as a <strong>application in which an AI model takes actions against external systems on behalf of a user in furtherance of a user's give task and goal</strong>.</p>
<ul>
<li>A desktop based agent will become available to use on the computer. Interacting with your browser and mail client automatically. (Probably Claude's Anthropic will be there soonest) - doesn't mean the LLM has to run on the desktop.</li>
<li>AI Agents included in softwares (Teams, Github, ...) will start to become available in preview at least before the end of the year.</li>
<li>Programming Agents will start to be useful (see AI Devin in 2024 being still completely unusable) - but in 2025 AI coders will be the focus and ship mid year.</li>
</ul>
<h3 id="ai-in-mathematics">AI in Mathematics</h3>
<ul>
<li>While the first theorem proof by an LLM has already been published, I expect 2025 to have a slew of progress on fundamental proofs rewritten by LLMs or LRMs, particularly towards automated proofing systems (Coq, ...) and several proofs generated by LLMs that humans did not independently derive.</li>
</ul>
<h3 id="ai-in-medicine">AI in Medicine</h3>
<ul>
<li>One company will announce an AI diagnostics companion for health - that is certified as a support tool for doctors.</li>
</ul>
<h3 id="ai-in-war">AI in War</h3>
<ul>
<li>A semi-autonomous multi-agent AI will be used to control a tactical engagement in Ukraine. This has nearly happened due to the engagement of the first robot brigade (<a href="https://www.forbes.com/sites/davidaxe/2024/12/21/ukraines-first-all-robot-assault-force-just-won-its-first-battle/">dec 2024 : Ukraine’s All-Robot Assault Force Just Won Its First Battle</a> ) - but without AI - using tele-operation. I think AI will be used at more levels than just terminal guidance of the FPV drones.</li>
</ul>
<h3 id="agi">AGI</h3>
<ul>
<li>One company claims their model has attained <strong>AGI</strong> - defined as a model that is as good as a reasoning human - <strong><em>in office work related tasks</em></strong>. There is a lot of disagreement if this constitutes AGI or not.</li>
</ul>
<h3 id="robotics">Robotics:</h3>
<ul>
<li>Humanoid robots become available in limited quantities to the general public, with at least 1 company shipping a home robot with AI onboard to do simple tasks (Unitree from 2024 does not count since the robots are <strong>only</strong> remote controlled). These first robots will be sometimes teleoperated for specific tasks. Pricing will be lower than 50k$ per robot.</li>
<li>One company announces wide scale drone deliveries in US cities: While drone delivery companies already exist (<a href="https://builtin.com/articles/drone-delivery-companies">13 Drone Delivery Companies to Know | Built In</a>), one of them is going to break out as an early leader this year.</li>
</ul>
<h3 id="space">Space:</h3>
<ul>
<li>SpaceX demonstrates first ship to ship refueling.</li>
<li>Blue Origin gets to orbit with New Glenn and proves the landing system, but is not able to send a reused first stage to orbit yet.</li>
<li>The first part of a new space station gets deployed, probably commercial.</li>
</ul>
<h3 id="environment">Environment:</h3>
<ul>
<li>2025 beats 2024 as hottest year.</li>
</ul>
<h3 id="energy">Energy:</h3>
<ul>
<li>Record installation of Solar, Wind and renewables (beating 2024 worldwide - despite the US's drop).</li>
<li>Price spikes on Gas and Petrol due to US actions and disruption of Russian production. Unsure if that will continue throughout the year but we will have shocks from policy changes.</li>
</ul>
						<hr>
                </div>       
        <ul class="pager">
            <li class="previous">                
            </li>
            <li class="next">
            </li>
        </ul>
    </div>
    <div class="4u 12u$(medium)">

            <h5>Tags</h5>
            <ul class="actions small">
                    <li><a role="button" href="/tags/Migrated" class="button small">Migrated (38)</a></li>
                    <li><a role="button" href="/tags/Thoughts" class="button small">Thoughts (12)</a></li>
                    <li><a role="button" href="/tags/Database" class="button small">Database (11)</a></li>
                    <li><a role="button" href="/tags/Architecture" class="button small">Architecture (11)</a></li>
                    <li><a role="button" href="/tags/CSharp" class="button small">CSharp (10)</a></li>
                    <li><a role="button" href="/tags/Dotnet-try" class="button small">Dotnet try (7)</a></li>
                    <li><a role="button" href="/tags/Oracle" class="button small">Oracle (7)</a></li>
                    <li><a role="button" href="/tags/Jupyter-notebook" class="button small">Jupyter notebook (7)</a></li>
                    <li><a role="button" href="/tags/R" class="button small">R (4)</a></li>
                    <li><a role="button" href="/tags/WPF" class="button small">WPF (4)</a></li>
            </ul>
            <ul class="actions small">
                <li><a href="/tags" class="button small">View All Tags &rarr;</a></li>
            </ul>    

            <h5>Older Posts</h5>
            <ul>
                    <li><a href="/posts/Disposable-Software">Disposable software</a></li>
                    <li><a href="/posts/Audit-Trail-Oracle">Using an audit trail table on Oracle</a></li>
                    <li><a href="/posts/snippets-in-vscode">VSCode Snippets</a></li>
                    <li><a href="/posts/HowToUpdateWritebook">Adding LaTeX Maths to Writebook</a></li>
            </ul>
                <ul class="actions small">
                    <li><a href="/posts" class="button small">Archive &rarr;</a></li>
                </ul>

        
    </div> 
</div>
					</div>
				</div>
			</div>

			<!-- Footer -->
			<footer id="footer">
				<div class="inner">
    <section>
        <h2>Feeds</h2>
        <ul class="actions small vertical">
            <li><a href="/feed.rss" class="button small"><i class="fa fa-rss"></i> RSS Feed</a></li>
                        <li><a href="/feed.atom" class="button small"><i class="fa fa-rss"></i> Atom Feed</a></li>
        </ul>
    </section>
    <section>
    </section>
    <ul class="copyright">
        <li>Copyright © 2025</li>
        <li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
        <li><a href="https://wyam.io">Generated by Wyam</a></li>
    </ul>
</div>

			</footer>

		</div>
		
		

		<!-- Scripts -->
		<script>hljs.initHighlightingOnLoad();</script>
		<script src="/assets/js/jquery.min.js"></script>
		<script src="/assets/js/skel.min.js"></script>
		<script src="/assets/js/util.js"></script>
		<!--[if lte IE 8]><script src="/assets/js/ie/respond.min.js"></script><![endif]-->
		<script src="/assets/js/main.js"></script>

	</body>

</html>
