
<!DOCTYPE html>
<!--
	Phantom by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html lang="en">

	<head>
		<title>Eric Winnington</title>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1" />		
        <link href="/assets/css/highlight.css" rel="stylesheet">
		<!--[if lte IE 8]><script src="/assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="/assets/css/main.css" />
		<!--[if lte IE 9]><link rel="stylesheet" href="/assets/css/ie9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="/assets/css/ie8.css" /><![endif]-->
        <link href="/assets/css/override.css" rel="stylesheet" />

		<meta name="description" content="A collection of thoughts, code and snippets." />
		<link type="application/rss+xml" rel="alternate" title="Eric Winnington" href="/feed.rss" />
				<link type="application/atom+xml" rel="alternate" title="Eric Winnington" href="/feed.atom" />
		<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
		<link rel="icon" href="/favicon.ico" type="image/x-icon">

		<meta name="application-name" content="Eric Winnington" />
		<meta name="msapplication-tooltip" content="Eric Winnington" />
		<meta name="msapplication-starturl" content="/" />

		<meta property="og:title" content="Eric Winnington" /> 
		<meta property="og:type" content="website" />
		<meta property="og:url" content="http://ewinnington.github.io/" />
		<!-- TODO: More social graph meta tags -->

        <script src="/assets/js/highlight.pack.js"></script>   
		
        <meta name="twitter:card" content="summary" />
<meta name="twitter:site" content="@ewinnington" />
<meta name="twitter:title" content="Eric Winnington" />
<meta name="twitter:description" content="A collection of thoughts, code and snippets." />

<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
		
	</head>

	<body>
		<!-- Wrapper -->
		<div id="wrapper">

			<!-- Header -->
			<header id="header">
				<div class="inner">

					<!-- Logo -->
					<a href="/" class="logo">
						<span class="title">Eric Winnington</span>
					</a>

					<!-- Nav -->
					<nav>
						<ul>
							<li><a href="#menu">Menu</a></li>
						</ul>
					</nav>

				</div>
			</header>

			<!-- Menu -->
			<nav id="menu">
				<h2>Menu</h2>
				<ul>
					        <li><a href="/posts">Archive</a></li>
        <li><a href="/tags">Tags</a></li>
        <li><a href="/about">About Me</a></li>
        <li><a href="/videos">Videos on programming</a></li>

				</ul>
			</nav>

			<!-- Main -->
			<div id="main">
				<div class="inner">

    <header>
        <h1>A collection of thoughts, code and snippets.</h1>
    </header>

					
					<div id="content">
						
<div class="row">
    <div class="8u 12u$(medium)">
                <div>
                    <a href="/posts/MIQP">
                        <h2>It&#x2019;s a proportional allocation - how hard can it be? Going from Water filling to QP</h2>
                    </a>
                    <p>Posted on Wednesday, 16 April 2025</p>
<h1 id="its-a-proportional-allocation-how-hard-can-it-be-miqp">It’s a proportional allocation - how hard can it be? MIQP</h1>
<p>Alice, Bob and Charlie buy a pizza and they each put down a part of the price, respectively 50%, 30% and 20%. Pizza arrives and they slice it up and eat it. But Alice gets full after eating 40% of her 50% slice, how do we allocate the remaining 10% slice to Bob and Charlie? Her 10% can be cut up into 2% slices and we proportionally give 3 to Bob (total 36%) and 2 to Charlie (total 24%).</p>
<p>We have a total allocation of 100 MW of power to allocate to three products, in the ideal allocation with (0.5, 0.3, 0.2) ratio and each product has a maximum of 40MW.</p>
<p>These two problems are the same.</p>
<h2 id="waterfilling-algorithm">Waterfilling algorithm</h2>
<p>There exists a well known algorithm for solving this problem, the water filling algorithm.</p>
<p>In essence, we are looking at finding the level of water across three containers that is flat along the allocation amount.</p>
<h3 id="definitions">definitions:</h3>
<ul>
<li><span class="math">\(w_i\)</span> is the weight from product i</li>
<li><span class="math">\(w'_i\)</span> is the updated weight when we have fewer products due to saturation</li>
<li><span class="math">\(M_i\)</span> is the maximum of product i</li>
<li><span class="math">\(x'_i\)</span> is the initial / ideal allocation in case the products are not saturated</li>
<li><span class="math">\(x_i\)</span> is the current allocation to the product</li>
<li><span class="math">\(T\)</span> is total allocation amount to spread over the products</li>
<li><span class="math">\(T_r\)</span> is the remaining allocation to spread over products after the saturated products are removed from T</li>
<li><span class="math">\(saturated\)</span> : means that the allocation is &gt;= max on the product.</li>
<li><span class="math">\(unsaturated\)</span> : means that the allocation is &lt; max on the product</li>
</ul>
<h3 id="water-filling-iterative-algorithm">Water-Filling (Iterative) Algorithm:</h3>
<ul>
<li>Step 1: Compute the ideal allocations <span class="math">\(x'_i = w_i \cdot T\)</span></li>
<li>Step 2: For any product i for which <span class="math">\(x'_i &gt;= M_i\)</span> (saturated), set <span class="math">\(x_i =M_i\)</span>, otherwise <span class="math">\(x_i = x'_i\)</span>.</li>
<li>Step 3: Compute the remaining capacity by removing the capacity of saturated <span class="math">\(x_i\)</span> : <span class="math">\(T_r = T − \sum_{i_{\text{saturated}}} M_i\)</span>.</li>
<li>Step 4: For the remaining (unsaturated) products, redistribute <span class="math">\(T_r\)</span> proportionally based on their weights normalized over the unsaturated set <span class="math">\(x_i = w^{\prime}_i \cdot T_r\)</span> where <span class="math">\(w^{\prime}_i = \frac{w_i}{\sum_j w_j}\)</span> with <span class="math">\(j\)</span> representing the unsaturated products</li>
<li>Step 5: Repeat the process if additional products get saturated during the redistribution, ie. from Step 2.</li>
</ul>
<p>The reason this terminates is that because we remove saturated products from the list, the next products get allocated and the set gets reduced, either a new product is saturated and the cycle continues or the final allocation is done and terminates.</p>
<pre><code class="language-python">import numpy as np

def iterative_waterfilling(T, weights, max_allocations):
    n = len(weights)
    allocations = np.zeros(n)
    unsaturated = np.array([True] * n)
    remaining_T = T

    while True:
        # Calculate proportional weights for unsaturated products
        current_weights = np.array(weights) * unsaturated
        total_current_weight = np.sum(current_weights)
        
        # Calculate ideal allocation for unsaturated products
        ideal_allocations = (current_weights / total_current_weight) * remaining_T

        # Check for saturation
        newly_saturated = ideal_allocations &gt;= max_allocations
        
        # Update allocations and saturation status
        if not np.any(newly_saturated &amp; unsaturated):
            allocations[unsaturated] = ideal_allocations[unsaturated]
            break
        
        for i in range(n):
            if unsaturated[i] and newly_saturated[i]:
                allocations[i] = max_allocations[i]
                unsaturated[i] = False
                remaining_T -= allocations[i]

    return allocations

T = 100
weights = [0.5, 0.3, 0.2]
max_allocations = [40, 40, 40]

allocations_result = iterative_waterfilling(T, weights, max_allocations)
print(&quot;Iterative Waterfilling Result:&quot;, allocations_result)
</code></pre>
<h3 id="examples">Examples</h3>
<p>Example 1 - total 100, allocation (0.5,0.3,0.2), maximum (40,40,40), result (40,36,24)</p>
<p>Example 2 - total 100, allocation (0.5,0.3,0.2), maximum (40,34,40), result (40,34,26)</p>
<p>The problem of this algorithm is that while it works wonderfully for simple proportional problems, as soon as you start adding more constraints (minimums) and relations between the allocations, this iterative algorithm doesn’t work that great.</p>
<p>So how do we solve this as an optimization problem? We want to find a solution that when unconstrained (unsaturated) falls back to the proportional allocation and when constrained (saturated maximum) falls back to the waterfilling algorithm.</p>
<h2 id="from-waterfilling-to-qp-mixed-integer-quadratic-programming">From Waterfilling to QP : Mixed integer quadratic programming</h2>
<p>While the iterative waterfilling algorithm effectively solves basic proportional allocation problems, it struggles under more complex scenarios involving additional constraints, such as minimum allocation limits or relational constraints between allocations. To robustly handle these real-world complexities, we leverage Mixed Integer Quadratic Programming (MIQP). MIQP elegantly generalizes the waterfilling logic into an optimization framework, allowing precise specification of constraints and objectives. By translating allocation decisions into a mathematical optimization problem, we ensure optimal, constraint-respecting allocations, making it suitable for applications demanding reliability and flexibility.</p>
<h3 id="definitions-1">Definitions:</h3>
<ul>
<li><span class="math">\(T\)</span>: total</li>
<li><span class="math">\(w_i\)</span>: weight</li>
<li><span class="math">\(M_i\)</span>: Maximum of allocation</li>
<li><span class="math">\(d_i\)</span>: product saturated marker ($\in \mathbb, binary \in \lbrace 0,1 \rbrace $), 0 unsaturated, 1 saturated</li>
<li><span class="math">\(x_i\)</span>: allocation amount</li>
<li><span class="math">\(v_i\)</span>: target water level for unsaturated product (shared identity)</li>
<li><span class="math">\(U\)</span>: upper large bound</li>
</ul>
<h3 id="objective">Objective:</h3>
<p><span class="math">\(\min \sum_i (x_i - w_i T)^2\)</span></p>
<h3 id="constraints">Constraints:</h3>
<h4 id="i-basic-allocation-limits">(I) Basic allocation limits</h4>
<p><span class="math">\(\forall i \quad x_i \geq 0 \quad (a)\)</span></p>
<p><span class="math">\(\forall i \quad x_i \leq M_i \quad (b)\)</span></p>
<h4 id="ii-total-allocation-constraint">(II) Total allocation constraint</h4>
<p><span class="math">\(\sum_i x_i \leq T\)</span></p>
<h4 id="iii-saturation-constraints">(III) Saturation constraints <span class="math">\(\forall i\)</span></h4>
<p><span class="math">\(x_i = w_i \cdot v_i + M_i \cdot d_i \quad (a)\)</span></p>
<p><span class="math">\(v_i \leq U \cdot (1 - d_i) \quad (b) \quad \text{with } v_i = 0 \text{ when saturated}\)</span></p>
<p><span class="math">\(v_i \geq 0 \quad (c)\)</span></p>
<p><span class="math">\(v_i \leq \frac{M_i}{w_i} \cdot (1 - d_i) + U \cdot d_i \quad (d)\)</span></p>
<h4 id="iv-water-level-equality-constraints-across-unsaturated-products">(IV) Water level equality constraints across unsaturated products</h4>
<p><span class="math">\(\forall i (1 \rightarrow n), \quad \forall j (i+1 \rightarrow n)\)</span></p>
<p><span class="math">\(v_i - v_j \leq U \cdot (d_i + d_j)\)</span></p>
<p><span class="math">\(v_j - v_i \leq U \cdot (d_i + d_j)\)</span></p>
<p><em>If both are unsaturated, it means <span class="math">\(d_i = d_j = 0\)</span>, thus forcing <span class="math">\(v_i = v_j\)</span>.</em></p>
<h3 id="explanation-of">Explanation of <span class="math">\(v_i\)</span></h3>
<p>This set of equations sets up <span class="math">\(v_i\)</span> as a shared value <span class="math">\(z\)</span> across all unsaturated products.</p>
<p>$$40 + (w_1 + w_2) \cdot z = 100$$</p>
<p><span class="math">\((0.3 + 0.2) z = 60\)</span></p>
<p><span class="math">\(z = 120\)</span></p>
<p><span class="math">\(x_1 = 0.3 \cdot 120 = 36\)</span></p>
<p><span class="math">\(x_2 = 0.2 \cdot 120 = 24\)</span></p>
<h3 id="implementation-in-python">Implementation in Python</h3>
<pre><code class="language-python">#pip install numpy
#pip install cvxpy
#pip install ecos

import cvxpy as cp
import numpy as np

# Problem parameters
T = 100.0
w = [0.5, 0.3, 0.2]          # target weights for products 1, 2, and 3
M_max = [40.0, 40.0, 40.0]     # maximum allocation for each product

# Number of products
n = len(w)

# A sufficiently large constant U (big-M) for enforcing water-level equality.
U = 500.0

# Decision variables:
# x: allocation amounts
x = cp.Variable(n)
# v: auxiliary &quot;water-level&quot; variables for unsaturated products
v = cp.Variable(n)
# delta: binary variables; delta[i] = 1 means product i is saturated (x[i] = M_max[i])
delta = cp.Variable(n, boolean=True)

constraints = []

# For each product, define the allocation as the sum of the unsaturated part and the saturation term.
for i in range(n):
    # If not saturated (delta[i] = 0) then x[i] = w[i]*v[i].
    # If saturated (delta[i] = 1) then x[i] = M_max[i].
    constraints.append(x[i] == w[i] * v[i] + M_max[i] * delta[i])
    
    # Force v[i] = 0 when saturated, by bounding v[i] to 0 when delta[i]=1.
    constraints.append(v[i] &lt;= U * (1 - delta[i]))
    # Ensure nonnegativity of v.
    constraints.append(v[i] &gt;= 0)
    # When unsaturated (delta[i]=0), we must have x[i] = w[i]*v[i] ≤ M_max[i]. 
    # Throught: Why not T here instead of M_max[i]? -&gt; it would be correct, but M_max[i] is a more restrictive boundary so helps convergence
    constraints.append(v[i] &lt;= (M_max[i] / w[i]) * (1 - delta[i]) + U * delta[i])

# Enforce that all unsaturated products share the same water-level.
# For every pair (i,j), if both are unsaturated (delta[i] = delta[j] = 0) then v[i] must equal v[j].
for i in range(n):
    for j in range(i+1, n):
        constraints.append(v[i] - v[j] &lt;= U * (delta[i] + delta[j]))
        constraints.append(v[j] - v[i] &lt;= U * (delta[i] + delta[j]))

# Total allocation constraint: the sum of all allocations must equal the available capacity.
constraints.append(cp.sum(x) == T)

# Ensure each x[i] does not exceed its maximum. (These could be built in via the definition of x.)
for i in range(n):
    constraints.append(x[i] &gt;= 0)
    constraints.append(x[i] &lt;= M_max[i])

# Define the target allocation for each product (unconstrained ideals)
target = np.array([w_i * T for w_i in w])

# Objective: minimize squared deviation from the ideal allocation.
objective = cp.Minimize(cp.sum_squares(x - target))

# Define and solve the MIQP.
# Use a MIQP-capable solver such as GUROBI, CPLEX, ECOS_BB, etc.
prob = cp.Problem(objective, constraints)
result = prob.solve(solver=cp.ECOS_BB)


print(&quot;Status:&quot;, prob.status)
print(&quot;Optimal value:&quot;, result)
print(&quot;Optimal allocations:&quot;)
for i in range(n):
    print(f&quot;  x[{i+1}] = {x.value[i]:.4f}&quot;)
print(&quot;Water-level (v) values:&quot;)
for i in range(n):
    print(f&quot;  v[{i+1}] = {v.value[i]:.4f}&quot;)
print(&quot;Saturation indicators (delta) values:&quot;)
for i in range(n):
    print(f&quot;  delta[{i+1}] = {delta.value[i]:.4f}&quot;)

# Expected behavior for this example:
# - For product 1, the unconstrained target is 50 but M_max[0]=40, so we expect it to be saturated (delta[0]=1, x[0]=40).
# - For products 2 and 3 (unsaturated, delta = 0), they share the same water-level z.
#   The total allocation constraint becomes: 40 + (w[1] + w[2]) * z = 100  --&gt;  (0.3+0.2)*z = 60, so z = 120.
#   Hence, x[2] = 0.3 * 120 = 36 and x[3] = 0.2 * 120 = 24.
</code></pre>
<h2 id="summary">Summary</h2>
<p>In business applications, we often see allocations that should be &quot;preference based&quot; if unconstrained, but then with different constraints it becomes complicated to tease out the preferences in an optimal way. This QP application shows a way to have an allocation identical to the water filling, but with the flexibility of QP.</p>
<p>If you were simplifying the QP constraints to remove the equations (IV), the output would be (40, 35, 25) since that minimises the objective function.</p>
<p>This example here is taken out of an abstration of a practical problem of allocating ancillary service capacity to a series of contracts, with trader preferences. This chapter is part of my book &quot;Energy - From Asset to Cashflow&quot; (not yet published).</p>
						<hr>
                </div>       
                <div>
                    <a href="/posts/SimplePerf3">
                        <h2>In-memory software design 2025 ? - from 40GB/s to 55GB/s</h2>
                    </a>
                    <p>Posted on Sunday, 16 February 2025</p>
<h1 id="in-memory-software-design-in-2025-from-40gbs-to-55gbs">In-memory software design in 2025 - from 40GB/s to 55GB/s</h1>
<p>In the last blog, we looked at techniques for improving when we were doing partial sums and to reduce the scanned trades we used in-memory indexes using sets.</p>
<p>But what if we simply <em>want to go faster</em> for the complete aggregation?</p>
<p>Up to now, we've been programming <strong>for the programmer</strong> making the program easy to write and understand, but what if we make the program <strong>easy to run for the cpu</strong>?</p>
<p>How do we do that? By improving memory locality and structures.</p>
<h2 id="memory-locality">Memory locality</h2>
<p>The initial implementation uses a <code>std::vector&lt;Trade&gt; trades;</code> with each trade maintaining a <code>std::vector&lt;DailyDelivery&gt; dailyDeliveries;</code> that contains the two vectors of power and value <code>std::array&lt;int, 100&gt; power; std::array&lt;int, 100&gt; value;</code>.</p>
<p><img src="/posts/images/in-mem/TradeInMemory.png" class="img-fluid" width="80%" alt="TradeInMemory" /></p>
<p>While <code>vector</code> does a great job at trying to get the memory allocated to it to be continuous in cpp, when you nest vectors in vectors and you allocate the sub-vectors, you are creating fragmented memory. This means the CPU's memory controller has to jump around a lot to get the next bloc and there's a high probability that adjacent will not be in the cache for the other cores of the CPU to be able to use.</p>
<p>The best architecture for locality will be always dependent on the access patters to the data. In our case here, we are going to optimize for maximum speed when doing complete aggregations.</p>
<p><img src="/posts/images/in-mem/TradeCompressed.png" class="img-fluid" width="80%" alt="TradeCompressed" /></p>
<p>This is done by creating an specific area for the Power and Value data of each delivery day is allocated to. Trades point to the beginning and end of this data. If you need to move inside this area, because you know exactly the start delivery date of the trade and the size of the delivery day vector (100 ints), you can immediately index into the large array.</p>
<p>Now, if you are dealing with a small amount of trades (~50'000 with an average of 120 days of delivery ~~ 4.9 GB of RAM for 600 million quarter hours), you can get away with a single coherent bloc of RAM for your delivery vector. But see later for a more practical production discussion:</p>
<pre><code class="language-cpp">    // Each daily delivery contributes 100 ints for power and 100 ints for value.
    std::vector&lt;int&gt; flatPower(totalDailyDeliveries * 100);
    std::vector&lt;int&gt; flatValue(totalDailyDeliveries * 100);
</code></pre>
<p>Now with this information, we can directly index into the area based on the trades data:</p>
<pre><code class="language-cpp">#ifdef _OPENMP
#pragma omp parallel for reduction(+: all_totalPower, all_totalValue, \
                                     traderX_totalPower, traderX_totalValue, \
                                     traderX_area1_totalPower, traderX_area1_totalValue, \
                                     traderX_area2_totalPower, traderX_area2_totalValue)
#endif
    for (std::size_t i = 0; i &lt; flatTrades.size(); ++i)
    {
        const auto&amp; ft = flatTrades[i];
        // Each trade's deliveries are stored in a contiguous block.
        size_t startIndex = ft.deliveriesOffset * 100;
        size_t numInts = ft.numDeliveries * 100;
        long long sumPower = 0;
        long long sumValue = 0;
        for (size_t j = 0; j &lt; numInts; ++j)
        {
            sumPower += flatPower[startIndex + j];
            sumValue += flatValue[startIndex + j];
        }
        // (a) All trades
        all_totalPower += sumPower;
        all_totalValue += sumValue;
</code></pre>
<p>When running with this configuration, my time to aggregation on the laptop improves from <strong>241ms</strong> to <strong>178ms</strong> a 37% improvement in speed - which get us to <strong>55 GB/s</strong> on a commodity laptop (using OpenMP of course).</p>
<h2 id="limits">Limits</h2>
<p>But as you scale to larger and larger amounts of trades and delivery days, you will really find that the vector<int> allocator will not be able to handle that in a single bloc.</p>
<p>At that point, we'll start running our own custom allocation, keeping our own block memory via a custom allocator. By creating blocks of 64 to 256MB that we allocate as needed and indexing our trade deliveries into it, we can then scale to more of the the entire memory of your machine.</p>
<p>Two good references on that are <a href="https://johnfarrier.com/custom-allocators-in-c-high-performance-memory-management/">Custom Allocators in C++: High Performance Memory Management</a> and <a href="https://www.youtube.com/watch?v=kSWfushlvB8">CppCon 2017: Bob Steagall “How to Write a Custom Allocator”</a>.</p>
<h2 id="next-steps">Next steps?</h2>
<p>Going from a simple data structure to a memory adapted structure allowed us to go from <strong>40GB/s</strong> (42% of laptop bandwidth) to <strong>55GB/s</strong> (57% of laptop's 96 GB/s bandwidth).</p>
<p>If you still need more performance, you must further adapt your data structures and locality to the access patterns. Then start looking in depth at where the stalls are in the execution trace. There's other approaches such as looking at AVX instructions in more detail to find some perf, loop unrolling, and so on. Get a real cpp expert to consult on it.</p>
<p>Or just get a machine with more bandwidth! An example of that is the M2, M4 Max and Ultra family of chips from Apple, with memory bandwidth of over 800GB/s - over 8x what my laptop has. Or just run on a server, as noted in the first article, Azure has now a machine with 6'900 GB/s of bandwidth.</p>
						<hr>
                </div>       
                <div>
                    <a href="/posts/SimplePerf2">
                        <h2>In-memory software design 2025 - Applied to energy</h2>
                    </a>
                    <p>Posted on Saturday, 15 February 2025</p>
<h1 id="in-memory-software-design">In-memory software design</h1>
<p>In the last blog, <a href="https://ewinnington.github.io/posts/SimplePerf">In-memory performance in 2025</a> we looked at a simple design for an energy aggregation system to aggregate 9.8 GB of 100'000 trades rolled out and saw we got about 40 GB/s performance.</p>
<p>I upped the numbers of trades to 500'000 trades (rolled out over quarter hours, with average trade length of 120 days in quarter hours) and validated that on a ~50 GB dataset, we are running at ~1200 ms for the complete aggregation, confirming a linear scaling of the compute time.</p>
<p>But this brings us to a point where on commodity hardware, we are running our aggregations over 1 second in duration. So how can we improve this? This time, instead of trying to brute force, let's bring in some other techniques from databases: Indexes!</p>
<h2 id="simple-indexes-for-in-memory-aggregations">Simple indexes for in-memory aggregations</h2>
<p>For our use case, we are going to look at two meta-data properties on our trades, Trader and Delivery area, but the concept scales efficiently to large collections of meta-data, because we are dealing with such a small amounts of Trades (500k).</p>
<h3 id="building-the-index">Building the index</h3>
<p>We can generate at insertion a set for each meta-data, trader and delivery area.</p>
<pre><code class="language-cpp">    // =================
    // 2) Build Indexes
    // =================
    // We'll do single-attribute indexes: trader -&gt; set of tradeIDs, area -&gt; set of tradeIDs
    // For large data, consider using std::vector&lt;size_t&gt; sorted, or some other structure.

    std::unordered_map&lt;std::string, std::unordered_set&lt;size_t&gt;&gt; traderIndex;
    traderIndex.reserve(10);  // if you know approx how many traders you have to avoid resizing continuously

    std::unordered_map&lt;std::string, std::unordered_set&lt;size_t&gt;&gt; areaIndex;
    areaIndex.reserve(10);
</code></pre>
<p>I'm using an unordered set, but could also use an ordered set, this might be more efficient.</p>
<p>As keys, I'm using the string data of the meta-data on the trade - in production this would probably be integer keys, but for this use case is sufficient.</p>
<pre><code class="language-cpp">    for (size_t i = 0; i &lt; trades.size(); ++i)
    {
        const auto&amp; t = trades[i];
        traderIndex[t.trader].trades(i);
        areaIndex[t.deliveryArea].trades(i);
    }
</code></pre>
<p>When we insert a set of trades, we can add them to the set index.</p>
<p>As a linear pass, it's extremely efficient to build this index and cheap to keep it updated as we insert new trades.</p>
<h3 id="using-the-index">Using the index</h3>
<p>If we have a search query on a single attribute, we can now use the simple index and have directly the result.</p>
<p>But if we are doing a query on two (or more), we are going to take use the smallest index first and match with the largest index. The unordered_set gives an acceptable performance for <code>bigger.find(id)</code> but you can probably do even better with a set structure that is optimized for intersections. You can benchmark using <code>std::set_intersection</code> against my simple implementation if you are using sorted sets.</p>
<pre><code class="language-cpp">// ===================================
    // 3) Use the indexes for filtering
    // ===================================
    // For instance, let's do a query: TraderX, Area2
    // We'll find the intersection of (all trades for TraderX) and (all trades for Area2).

    std::string queryTrader = &quot;TraderX&quot;;
    std::string queryArea   = &quot;Area2&quot;;

    // Get sets from the index
    // (handle the case if the key doesn't exist -&gt; empty set)
    auto itT = traderIndex.find(queryTrader);
    auto itA = areaIndex.find(queryArea);

    if (itT == traderIndex.end() || itA == areaIndex.end()) {
        std::cout &lt;&lt; &quot;No trades found for &quot; &lt;&lt; queryTrader &lt;&lt; &quot; AND &quot; &lt;&lt; queryArea &lt;&lt; &quot;\n&quot;;
        return 0;
    }

    const auto&amp; traderSet = itT-&gt;second;
    const auto&amp; areaSet   = itA-&gt;second;

    // Intersection
    // We'll create a vector of trade IDs that are in both sets
    // For speed, we can iterate over the smaller set and check membership in the larger set.
    const auto&amp; smaller = (traderSet.size() &lt; areaSet.size()) ? traderSet : areaSet;
    const auto&amp; bigger  = (traderSet.size() &lt; areaSet.size()) ? areaSet   : traderSet;

    std::vector&lt;size_t&gt; intersection;
    intersection.reserve(smaller.size());  // a safe upper bound if all the smaller set is selected, to avoid resizing

    for (auto id : smaller)
    {
        if (bigger.find(id) != bigger.end())
        {
            intersection.push_back(id);
        }
    }
</code></pre>
<p>This gives us the intersection and we iterate it to do the summation - again, here we are summing up over the entire year into a single value, but we could just as easily be doing daily, weekly or monthly sums.</p>
<pre><code class="language-cpp">    // ====================================
    // 4) Aggregate deliveries for matches
    // ====================================
    // Let's sum up total power / total value for the intersection set.

    long long totalPower = 0;
    long long totalValue = 0;

    #ifdef _OPENMP
    #pragma omp parallel for num_threads(4) reduction(+:totalPower, totalValue)
    #endif
    for (auto id : intersection)
    {
        const Trade&amp; t = trades[id];
        // sum up all deliveries
        for (const auto&amp; dd : t.dailyDeliveries)
        {
            for (int slot = 0; slot &lt; 100; ++slot)
            {
                totalPower += dd.power[slot];
                totalValue += dd.value[slot];
            }
        }
    }
</code></pre>
<p>This addition here is single threaded, but you can also use OpenMP to accelerate it - this will require some tuning, you don't want to use too many threads for these smaller aggregations, the <code>omp parallel for num_threads(4)</code> can be added to example limit to 4 threads. (note: I added the openMP in the code above).</p>
<p>Generally you get a 10x or more acceleration in single core, depending on how selective the indexes you are using are- In parallel, I'm getting 40-50x acceleration in multi-core with a num_threads to 4.</p>
<h2 id="why-arrayint100">Why array&lt;int,100&gt; ?</h2>
<p>In my last post, I used an array of 100 points in a day. I'm using it because it's simplest to have all days have the same &quot;size&quot; for memory alignment, therefore if I am calculating days in local time I have a 25h and 23h hour day once a year. I would generally prefer to work in UTC and have constant 24h days -- but for some reason humans prefer local time so it aligns with expectations.</p>
<p>Just be clear with the developer if you use an internal UTC or Local time representation. If using local make sure to:</p>
<ul>
<li>Properly sanitize your inputs, check trades fill the 96 quarter hours only for all days apart from the short (92) and long day (100) and zero fill the remainder.</li>
<li>Keep summing on all 100 hours for summations, the 4% extra index length is not worth an if statement in the inner loop of the code - try to keep loops jump free.</li>
</ul>
<h2 id="how-to-deal-with-canceled-amended-recalled-trades">How to deal with Canceled / Amended / Recalled trades ?</h2>
<p>My first answer is don't! Let me explain: Normal trades should represent the 99.9% or 99.99% of your deals - unless there's something you haven't told me about the way you are trading!</p>
<p>We can design this by having a tradeStatus on the trade.</p>
<p>int TradeStatus:</p>
<ul>
<li>0 : trade is valid</li>
<li>1 : trade is canceled</li>
<li>2 : trade is amended (ie. replaced by a new one)</li>
<li>... : any other status necessary</li>
</ul>
<p>When a trade is canceled, we leave it in the trade vector, but simply set the tradeStatus to a non-zero value, and skip it with a test at the beginning of the aggregation.</p>
<pre><code class="language-cpp">    #ifdef _OPENMP
    #pragma omp parallel for num_threads(4) reduction(+:totalPower, totalValue)
    #endif
    for (auto id : intersection)
    {
        const Trade&amp; t = trades[id];
        if(t.tradeStatus != 0) continue;  // skip canceled/amended trades
</code></pre>
<p>If the trade is amended, same thing, we add a new trade to our list of trades and set the previous one to amended status. Generally, the this is not using up much memory. If it ever becomes a problem, we could:</p>
<ol type="a">
<li>have a &quot;trade compression&quot; which removes all non-zero trade status from the vector.</li>
<li>flush the entire trade vector and reload the whole set.</li>
</ol>
<p>Depending on your implementation, the flushing and reloading might be just as fast - not every program needs to stay resident in memory all the time.</p>
<h2 id="snapshot-state-to-disk-for-recovery-or-storage-fork-as-in-redis">Snapshot state to disk for recovery or storage - Fork() as in Redis</h2>
<p>If we want to take snapshots of the state,  we can get inspired from Redis' famous <a href="https://architecturenotes.co/i/143231289/forking">fork() snapshotting technique</a>.</p>
<p>Use this when needing to snapshot a large data structure in RAM to disk (serialize the entire state), without blocking our main process from accepting new trades for the entire duration of the write.</p>
<h3 id="how-fork-helps">How fork() helps</h3>
<p>On Linux, calling fork() creates a child process that initially shares the same physical memory pages as the parent.</p>
<p>Copy-on-write (CoW): If either the parent or the child writes to a page after the fork, the kernel duplicates that page so each process sees consistent data.</p>
<p>The child process can serialize the in-memory data (in a consistent state from the moment of forking) to disk, while the parent continues to run to accept new trades. New trades arriving in the parent process after the fork will not affect the child’s view of memory. The child effectively sees a snapshot as of the fork().</p>
<p>You want the data structure to be in a consistent state at the instant of fork().
A brief lock (or pause writes) just before the fork() is triggered, ensuring no partial updates. Immediately after fork() returns, you can unlock, letting the parent continue. Meanwhile, the child proceeds to write out the data.</p>
<p>We can store an atomic counter value in the program that represents the last tradeid inserted or a state version. This gives you a “version” or “stamp” number for the dataset.</p>
<p>I won't put the full code for that here, since the design is a little more involved, but the basics are:</p>
<pre><code>static std::atomic&lt;long&gt; g_version{0}; //snapshot version id 
static std::mutex g_tradeMutex;  // protect g_trades from concurrent modification - lock on write to 

// ---------------------------------------------------
// fork() to create a child that writes the snapshot
// ---------------------------------------------------
int snapshotNow(const char* filename) {
    // 1) Acquire short lock to ensure no partial updates in progress
    g_tradeMutex.lock();
    long snapVer = g_version.load(std::memory_order_relaxed);

    // 2) Fork
    pid_t pid = fork();
    if(pid &lt; 0) {
        // error
        std::cerr &lt;&lt; &quot;fork() failed\n&quot;;
        g_tradeMutex.unlock();
        return -1;
    }

    if(pid == 0) {
        // child
        // We have a consistent view of memory as of the fork.
        // release the lock in the child
        g_tradeMutex.unlock();

        // write the snapshot
        writeSnapshotToDisk(filename, snapVer);

        // exit child
        _exit(0);
    } else {
        // parent
        // release the lock and continue
        g_tradeMutex.unlock();
        std::cout &lt;&lt; &quot;[Parent] Snapshot child pid=&quot; &lt;&lt; pid 
                  &lt;&lt; &quot;, version=&quot; &lt;&lt; snapVer &lt;&lt; &quot;\n&quot;;
        return 0;
    }
}
</code></pre>
<p>In essence we have two processes continuing from the same command fork() return, each taking one branch.</p>
<h2 id="data-io">Data I/O</h2>
<p>To integrate your cpp aggregation software into the rest of your stack depends on the software running around it.</p>
<p>You can run the application as an on-demand aggregation, loading everything to memory, doing the aggregation and exiting - leaving the server to do something else - this can be worth it if you only do aggregations on-demand and can afford the load time of a second or two from your NVME storage.</p>
<p>You can keep the cpp program running either :</p>
<ul>
<li>exposing an http RestAPI (<a href="https://github.com/microsoft/cpprestsdk">https://github.com/microsoft/cpprestsdk</a> is a good library for that, I've used it before).</li>
<li>having a GRPC endpoint for performance</li>
<li>receiving data from a Kafka stream - I'm sure Confluent can give you a good example of that.</li>
</ul>
<h2 id="summary">Summary</h2>
<p>In-memory data aggregation using cpp is relatively easy to write and maintain. Cpp is no longer the terrible monster it was - auto pointers help and using std:: components makes everything simple. OpenMP is an easy win to add to compute or memory intensive sections.</p>
<h4 id="sidenotes">Sidenotes</h4>
<p>On Windows you can get everything you need to compile cpp by installing:</p>
<pre><code>winget install LLVM.LLVM
</code></pre>
<p>On Linux (or wsl), you need to install the following:</p>
<pre><code>sudo apt-get update
sudo apt-get install clang
sudo apt-get install libomp-dev 
</code></pre>
<p>Then in both os, you can usually run a compilation on your source file (inmem_agg_omp.cpp) using:</p>
<pre><code>clang++ -O3 -march=native -flto -ffast-math -fopenmp -o inmem_agg_omp inmem_agg_omp.cpp
</code></pre>
						<hr>
                </div>       
                <div>
                    <a href="/posts/SimplePerf">
                        <h2>Performance of in-memory in 2025</h2>
                    </a>
                    <p>Posted on Thursday, 13 February 2025</p>
<h1 id="performance-of-in-memory-software-in-2025">Performance of in-memory software in 2025</h1>
<p>Yesterday, at an Energy panel in Essen, I mentioned that some heavy calculations should be done in-memory. It is something that people have a tendency to dismiss because generally they are not aware of the capability and speed of modern CPUs and RAM. A 32GB dataset in memory can now be processed every second by a commodity cpu in your laptop.</p>
<p><a href="https://tailscale.com/blog/living-in-the-future">Living in the Future, by the numbers</a> is a great article on the progress we have had since 2004:</p>
<ul>
<li>CPU Compute is 1000x faster</li>
<li>Web servers are 100x faster</li>
<li>Ram is 16x to 750x larger</li>
<li>SSD can do 10'000x more transactions per second.</li>
</ul>
<p>You can also see this progression on <a href="https://techcommunity.microsoft.com/blog/azurehighperformancecomputingblog/announcing-azure-hbv5-virtual-machines-a-breakthrough-in-memory-bandwidth-for-hp/4303504">Azure with the high-compute servers</a>, Microsoft and AMD are packing so much more memory bandwidth in modern compute.</p>
<p><img src="https://github.com/user-attachments/assets/b7561026-767e-4dc4-97a5-e5306c3fa36a" class="img-fluid" width="60%" alt="image" /></p>
<p>We are going to have 7 TB/s of memory bandwidth!</p>
<p><img src="https://github.com/user-attachments/assets/f2c78f96-0eb0-4359-9f65-963b2b4b4f7b" class="img-fluid" width="60%" alt="image" /></p>
<p>You can now run super-computer level problems on Azure!</p>
<p>But what does that all mean? What can we do even on a commodity laptop? I have a Latitude 9440 laptop on my desk here, with a 13th gen i7-1365U with 32 GB of RAM.</p>
<h2 id="energy-trade-aggregation">Energy Trade aggregation</h2>
<p>Let's start with a small calculation from the world of Energy. I have 100'000 trades, these trades affect one or multiple quarter hours of one year (8'784 hours =&gt; 35'136 quarter hours).</p>
<p>Pulling out a little C++, completely unoptimized, how long does it take to aggregate them and how much RAM is used?</p>
<pre><code class="language-cpp">// A struct to hold the daily delivery arrays (power, value).
struct DailyDelivery
{
    int dayOfYear;  // 1..365
    std::array&lt;int, 100&gt; power; 
    std::array&lt;int, 100&gt; value; 
};

// A struct to hold the trade metadata.
struct Trade
{
    int tradeId;
    std::string trader;        // e.g. &quot;TraderX&quot;
    std::string deliveryArea;  // e.g. &quot;Area1&quot;, &quot;Area2&quot;
    // You could store time points, but we'll just store day indexes for simplicity.
    // Real code might store start_delivery, end_delivery as std::chrono::system_clock::time_point.
    int startDay; // 1..365
    int endDay;   // 1..365

    std::vector&lt;DailyDelivery&gt; dailyDeliveries;
};
</code></pre>
<p>The Daily Delivery structure represents a day of delivery, with 100 slots (for the 25h day =&gt; 100 quarter hours).</p>
<ul>
<li>I'm storing the delivery of power in kW as an int32, meaning in a single trade I can do –2'147'483'648 kW to 2'147'483'647 kW.</li>
<li>Same thing for the value, we store the individual value of the MW in milli values (decimal shift 3), so each MW could be priced at -2'147'483.648 € to 2'147'483.647 €.</li>
</ul>
<p>The Trade stores:</p>
<ul>
<li>metadata: Trader and DeliveryArea. We could add as many metadata elements as we need, but for simplicity in the demo, I only use this</li>
<li>a dailyDeliveries vector containing the array of all days affected by the trade.</li>
</ul>
<p>Now if we wanted to see what is the total sum of power of all trades, the sum of TraderX and the sum of TraderX's deals in Area1 and Area2, we can runn the aggregation over all the memory. This is completely straight forward code, no optimizations what so ever.</p>
<pre><code class="language-cpp">    // --------------------------------------
    // 2) Run the aggregations (measure time)
    // --------------------------------------

    // The aggregates we want:
    // (a) All Trader total (yearly - all zones)
    // (b) Trader X total
    // (c) Trader X / Area1
    // (d) Trader X / Area2
    //
    // We'll assume we only have TraderX, so &quot;All Trader&quot; == &quot;TraderX&quot; in this simple version.
    //
    // But let's keep it generic. If you had multiple traders, you'd do some checks:
    //
    // For Weighted Average Cost = total_value / total_power (where total_power != 0)

    // We'll measure the time for a single pass that gathers all these sums.

    using Clock = std::chrono::steady_clock;
    auto startTime = Clock::now();

    long long all_totalPower = 0;
    long long all_totalValue = 0;

    long long traderX_totalPower = 0;
    long long traderX_totalValue = 0;

    long long traderX_area1_totalPower = 0;
    long long traderX_area1_totalValue = 0;

    long long traderX_area2_totalPower = 0;
    long long traderX_area2_totalValue = 0;

    for(const auto&amp; trade : trades)
    {
        // (a) &quot;All Trader&quot; sums:
        //    Summation for all trades, all areas, all days
        //    Because this example is all TraderX, you might have to adapt if you had multiple traders
        for(const auto&amp; dd : trade.dailyDeliveries)
        {
            for(int slot = 0; slot &lt; 100; ++slot)
            {
                all_totalPower += dd.power[slot];
                all_totalValue += dd.value[slot];
            }
        }

        // (b) If trade.trader == &quot;TraderX&quot;
        if(trade.trader == &quot;TraderX&quot;)
        {
            for(const auto&amp; dd : trade.dailyDeliveries)
            {
                for(int slot = 0; slot &lt; 100; ++slot)
                {
                    traderX_totalPower += dd.power[slot];
                    traderX_totalValue += dd.value[slot];
                }
            }

            // (c) and (d) by area
            if(trade.deliveryArea == &quot;Area1&quot;)
            {
                for(const auto&amp; dd : trade.dailyDeliveries)
                {
                    for(int slot = 0; slot &lt; 100; ++slot)
                    {
                        traderX_area1_totalPower += dd.power[slot];
                        traderX_area1_totalValue += dd.value[slot];
                    }
                }
            }
            else if(trade.deliveryArea == &quot;Area2&quot;)
            {
                for(const auto&amp; dd : trade.dailyDeliveries)
                {
                    for(int slot = 0; slot &lt; 100; ++slot)
                    {
                        traderX_area2_totalPower += dd.power[slot];
                        traderX_area2_totalValue += dd.value[slot];
                    }
                }
            }
        }
    }

    auto endTime = Clock::now();
    auto durationMs = std::chrono::duration_cast&lt;std::chrono::milliseconds&gt;(endTime - startTime).count();
</code></pre>
<p>How long do you think that takes on a commodity laptop? It's 9.8 GB of RAM to scan and fully aggregate. This is also running inside a VM on my Windows WSL instance, with other software running at the same time.</p>
<pre><code>Time for in-memory aggregation: 907 ms
--- Memory usage statistics (approx) ---
Total bytes allocated (cumulative): 9822202136 bytes
Peak bytes allocated (concurrent):  9822202136 bytes
Current bytes allocated:            9822202136 bytes
</code></pre>
<h2 id="parallelization">Parallelization</h2>
<p>Since we are running on a multicore CPU, we can use more than one core to do the aggregation. With OpenMP, it's extremely simple to setup some parallelization for the compute. At the beginning of the loop, we can define a parallel aggregation for reduction, meaning a final sum.</p>
<pre><code class="language-cpp">    // Parallel over trades
    // The 'reduction(+: variableList)' tells OpenMP to create private copies of
    // these variables in each thread, accumulate them, and then combine them
    // at the end.
#ifdef _OPENMP
#pragma omp parallel for reduction(+ : all_totalPower, all_totalValue, \
                                       traderX_totalPower, traderX_totalValue, \
                                       traderX_area1_totalPower, traderX_area1_totalValue, \
                                       traderX_area2_totalPower, traderX_area2_totalValue)
#endif
    for (std::size_t i = 0; i &lt; trades.size(); ++i)
    {
        const auto&amp; trade = trades[i];
</code></pre>
<p>With this, we improve the time to aggregate on the laptop to: 241ms. This means we can now do the <strong>complete aggregation on a laptop 4x per second</strong> - even on a completely unoptimized, simplistic memory structure for trades.</p>
<pre><code>Time for in-memory aggregation: 241 ms
</code></pre>
<p>So when you are doing large numerical aggregations or calculations, ask yourself - can I do this in RAM? If so, you might be surprised at how quickly and efficiently you can do it with modern cpp.</p>
<h2 id="performance-vs-memory-bandwidth">Performance vs Memory bandwidth</h2>
<p>This completely unoptimized implementation is running at :</p>
<p>Data size (GB) / time (s) = bandwidth (GB/s) =&gt; 9.8 GB / 0.241 s ≈ <strong>40.7 GB/s</strong></p>
<p>My laptop has approx ~96 GB/s of memory bandwidth. I calculate it as follows: LPDDR5 at 6000 MT/s, 8 bytes, dual channel = 6000 * 8 * 2 =~ 96 GB/s. My laptop has less than half bandwidth of the HC family on Azure (AMD EPYC™ 7003-series CPU) using CPUs that were released in 2021. Still impressive for my laptop, but it shows you could do much better.</p>
<p>If I really needed to optimize, I would re-organise the data structures to improve the memory aligment as an initial step. With that, we should get closer to the theoretical bandwidth of the machine.</p>
						<hr>
                </div>       
        <ul class="pager">
            <li class="previous">                
            </li>
            <li class="next">
            </li>
        </ul>
    </div>
    <div class="4u 12u$(medium)">

            <h5>Tags</h5>
            <ul class="actions small">
                    <li><a role="button" href="/tags/Migrated" class="button small">Migrated (38)</a></li>
                    <li><a role="button" href="/tags/Thoughts" class="button small">Thoughts (12)</a></li>
                    <li><a role="button" href="/tags/Architecture" class="button small">Architecture (12)</a></li>
                    <li><a role="button" href="/tags/Database" class="button small">Database (11)</a></li>
                    <li><a role="button" href="/tags/CSharp" class="button small">CSharp (10)</a></li>
                    <li><a role="button" href="/tags/Dotnet-try" class="button small">Dotnet try (7)</a></li>
                    <li><a role="button" href="/tags/Oracle" class="button small">Oracle (7)</a></li>
                    <li><a role="button" href="/tags/Jupyter-notebook" class="button small">Jupyter notebook (7)</a></li>
                    <li><a role="button" href="/tags/CommandLine" class="button small">CommandLine (4)</a></li>
                    <li><a role="button" href="/tags/Rpi" class="button small">Rpi (4)</a></li>
            </ul>
            <ul class="actions small">
                <li><a href="/tags" class="button small">View All Tags &rarr;</a></li>
            </ul>    

            <h5>Older Posts</h5>
            <ul>
                    <li><a href="/posts/network-tailscale">Network with Tailscale</a></li>
                    <li><a href="/posts/2025-predictions">On the Horizon - 2025 - My predictions</a></li>
                    <li><a href="/posts/Disposable-Software">Disposable software</a></li>
                    <li><a href="/posts/Audit-Trail-Oracle">Using an audit trail table on Oracle</a></li>
            </ul>
                <ul class="actions small">
                    <li><a href="/posts" class="button small">Archive &rarr;</a></li>
                </ul>

        
    </div> 
</div>
					</div>
				</div>
			</div>

			<!-- Footer -->
			<footer id="footer">
				<div class="inner">
    <section>
        <h2>Feeds</h2>
        <ul class="actions small vertical">
            <li><a href="/feed.rss" class="button small"><i class="fa fa-rss"></i> RSS Feed</a></li>
                        <li><a href="/feed.atom" class="button small"><i class="fa fa-rss"></i> Atom Feed</a></li>
        </ul>
    </section>
    <section>
    </section>
    <ul class="copyright">
        <li>Copyright © 2025</li>
        <li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
        <li><a href="https://wyam.io">Generated by Wyam</a></li>
    </ul>
</div>

			</footer>

		</div>
		
		

		<!-- Scripts -->
		<script>hljs.initHighlightingOnLoad();</script>
		<script src="/assets/js/jquery.min.js"></script>
		<script src="/assets/js/skel.min.js"></script>
		<script src="/assets/js/util.js"></script>
		<!--[if lte IE 8]><script src="/assets/js/ie/respond.min.js"></script><![endif]-->
		<script src="/assets/js/main.js"></script>

	</body>

</html>
